[00:00:00] 大家好，那我們現在開始今天的課程。今天的課程是我們生成式 AI 課程裡面，終於要進入生成式 AI，但是是比較「古典」的生成式 AI。其實所謂古典也就是大概在 10 年內，但是因為 AI 的技術發展太快了，所以這個 10 年前的技術大概都已經變成古典技術了。我們就來看一下這個古典的技術叫做 GAN，就是生成對抗網路 (Generative Adversarial Network)。這就是我們今天介紹的主題。

[00:00:37] 在介紹這個主題之前，先跟大家討論一個問題：到底生成式 AI 有什麼困難點？我們今天希望 AI 有創作的能力，因為我們上一次已經說明了，為什麼要研究生成式 AI，其實一個很重要的原因就是希望 AI 有創作能力，它可以創作一些我們以前沒有看過的作品。但是我們也知道現在的 AI 模型（Model）只是一個「呆萌型」的 AI，所以今天要教它創作的時候，到底要怎麼教？這件事情其實困擾了人類很久，一直到 GAN 出現以後，大家以為 GAN 就是真正的解法。

[00:01:34] 我們先來看一下有什麼困難點。對於機器人，我需要知道輸入是什麼、輸出是什麼。我現在知道輸出就是要一個創作，比方說一個小插圖好了，那輸入到底要輸入什麼？我們可能會覺得說，輸入就是文字，例如「一個可愛的女孩」，然後它生成一張圖片，這就是我們這個學期會學到的文字生成 AI。

[00:02:16] 但是，根據我們這個呆萌機器人的運作方式，假設我們今天要做一個繪畫插圖的 AI，輸入是「一個可愛的女孩」，是不是就要找到一張可愛女孩的圖像去對應？這時候會發現有一點怪怪的地方，因為「一個可愛的女孩」不一定要長成第一張圖的樣子，也可以是第二張、第三張，甚至第 200 張，這些都可以叫做一個可愛的女孩。

[00:02:56] 所以這時候會發生一個非常關鍵的問題：現在是「一對多」的關係，這根本不是一個函數 (Function)。我們小時候學函數的時候，老師有講過函數不可以一對多。如果我們強迫它去收集資料，輸入是「一個可愛的女孩」，輸出是第一張圖；另一筆訓練資料輸入一樣是「一個可愛的女孩」，輸出是第二張圖；第三筆資料輸出是第三張圖。

[00:03:35] 當然大家會有一個疑問，如果我輸入的敘述比較長，比如針對不同的圖片用不同的敘述，那是不是就可以解決？其實有很多種方法，比方說我先決定要輸出的圖像類別，有可愛的女孩、飛機、火車等等，我把這些東西編成編號，可愛女孩是 1 號，飛機是 2 號，狗是 3 號，貓是 4 號。但現在比較大的問題是，我們可能同時都輸入 1 號（可愛女孩），但對應的訓練資料有第一張圖片、第二張圖片、第三張圖片。

[00:04:51] 如果我們硬是這樣子去訓練 AI 可不可以？可以，反正之前的模型我們怎麼訓練它，它就真的去學了。但它拼命學的結果會變成什麼樣子呢？它最後輸出的結果，會把所有同樣輸入的資料對應的圖片「平均」起來。因為它不能得罪 1 號圖片，也不能得罪 2 號、3 號，所以它就想盡辦法輸出的時候要越接近第一張、也要同時接近第二張、第三張，最後很榮易就是平均。

[00:05:35] 平均會有什麼問題？問題大了。第一個問題，平均之後可能根本就不是一張有意義的圖像，可能是一團模糊的東西。第二個問題，就算運氣好，平均起來真的也是一個可愛的女孩，但這也不是我們希望做的事情，因為它永遠只能輸出那一張圖而已。你每次輸入「可愛的女孩」，它永遠就是輸出那一張平均圖，但我們希望它是有創作能力的，每一次要創作出不同的東西。所以，簡單來說，剛剛那樣的想法是失敗的，我們不能這樣做。

[00:06:25] 為了打造一個創作型的 AI 機器人，大家就開始想那些比較形而上的東西。一般的創作型 AI 應該是這樣：我們要想辦法輸入一個天馬行空的想法，或者一個有意義的想法。我們把那個有意義的想法叫做 Latent Vector 或是 Latent Tensor（潛在向量/張量）。這個 Latent Tensor 代表我們想要創作出來的東西的特徵向量。我們先不要管那個向量怎麼來的，假設世界上就是有辦法跟電腦溝通，我知道怎麼告訴它說要畫一個可愛的女孩，我就會把「要畫可愛的女孩」這件事情變成一個 Latent Tensor 輸進去，然後它就應該出來一個創作的作品。

[00:07:43] 這個對人類一點幫助都沒有，因為剛剛說的都是形而上的東西，我怎麼知道電腦的語言長什麼樣子？不管怎麼樣，這個神秘的 Latent Tensor 主宰了生成式 AI 最重要的關鍵。大家想盡辦法找出一個方法來產生這個 Latent Tensor。第一個辦法是訓練出來，這個我們未來會看到；第二個就是「隨機產生」。隨機產生的意思就是我隨便給一個天馬行空的想法，但是隨機產生大家一定也會覺得怪怪的，有時候我希望它畫火車，不要隨機產生飛機或人。所以隨機產生到底要怎麼控制，這也是未來要解決的問題。

[00:09:12] 總之，一個神秘的東西叫做 Latent Tensor。接下來我們來看 GAN 到底是怎麼想的。GAN 的想法大概在 2014 年提出，當時 Yann LeCun (楊立昆) 說過，他覺得 GAN 是深度學習裡面最有潛力的一個想法。大家如果有關切 AI 的話，都知道我們現在想做的東西叫做 AGI (通用人工智慧)，大家一直覺得 GAN 可能是通往 AGI 的一個重要關鍵。

[00:10:30] 為什麼覺得創作跟 AGI、跟人類智慧有關？舉個生活例子：如果我們走在路上，眼角餘光看到一個人走過來，腦海就會浮現這個人的完整影像，覺得是好朋友，轉頭要去打招呼，結果發現根本不認識他。這意思是說，資訊科學家相信，如果我今天對一個東西很理解，我就要能夠把它創造出來。就像我對朋友很理解，腦海可以浮現完整的他。所以如果 AI 可以創造，代表它真的「懂」，具有比較接近人的智慧。

[00:12:59] 我們還沒介紹 Yann LeCun 是誰，他是 Meta (Facebook) AI 部門非常重要的人物，被稱為深度學習三巨頭之一。同時他是 CNN (卷積神經網路) 的重要推手。雖然 CNN 之前也有人有類似想法，但今天大家知道 CNN 大部分是因為 Yann LeCun 的關係。所以 Yann LeCun 跟 Hinton 跟 Bengio 三個人就被稱為深度學習的三巨頭。

[00:14:12] 回到 GAN，事實上是一個叫 Ian Goodfellow 的人想出來的。他在博士時期跟著兩位非常有名的老師，其中一位就是 Bengio。他們早期寫了一本書叫《Deep Learning》，那本書那時候真的是神經網路的「聖經」。Goodfellow 這篇文章發表在一個非常重要的 AI Conference (NIPS) 裡面，當時震驚了全世界。

[00:15:05] 大家看一下那個時候的作品（投影片展示），其實真的看不懂它到底在畫什麼，有一些像飛機，有些像狗。大家會覺得畫得這麼糟糕，為什麼那時候很震驚？還有一張是 AI 生成的民宿房間，大概是 56x56 像素，其實很小。但這是 10 多年前的事，那時候世界還蠻容易被震驚的。

[00:16:04] 如果你要去看 Ian Goodfellow 教你 GAN 的話，可以去聽他 2016 年的演講 (NIPS 2016 Tutorial)。其中有一個亮點，有一位知名的模型作者在中間起來發問。他喜歡說這種話：「你這個東西做得也算是可以啦，但是我跟你講喔，我有一個學生十年前就做了一個東西，基本上就是你現在做的這個東西。」這位很有名，在 AI 界也算是蠻有爭議的 (Jürgen Schmidhuber)，大家想看八卦可以去看一下。

[00:17:47] GAN 曾經非常非常紅，紅到如果你做 AI 研究沒有做任何一個 GAN 的話，人家會覺得你好樣不是在做 AI。所以大家都會做一個 GAN 出來。因為太多了，取名字變成困擾，GAN, BGAN, CGAN, DGAN 早就被取完了，一定要取酷炫一點的名字。那時候有一個網頁專門收集所有的 GAN。

[00:18:49] 等一下我們會說到 WGAN (Wasserstein GAN) 的作者，是我們政大的學長。GAN 其實很難訓練，WGAN 最重要的一件事情就是讓 GAN 比較好訓練。這位作者曾經來到台灣，那時候我們學長姐參加 GAN 的比賽得到佳作，就跟 WGAN 的作者一起合照。

[00:19:46] 我們那時候做的一個模型是這樣：比如說今天有一些日文的字型很漂亮，但是缺了很多字。我們能不能用 AI 把沒有的字型補完？這個問題很早以前就有人想做。你可能會想說幹嘛那麼複雜？我今天就訓練一個呆萌機器人，準備兩套字型，A 是完整的中文字型，B 是缺字的日文字型。我把兩邊都有的字拿去當訓練資料，輸入 A，輸出 B。理論上輸入沒看過的 A 字型，應該就會產生對應的 B 字型。但不幸的是，出來的字型解析度會有一點模糊，邊角不會銳利。所以必須要用 GAN 去做。那時候我們做的字型大小是 512x512，大家不要小看這件事情，那時候 512x512 已經是大圖輸出了。

[00:22:05] 我們現在來看 GAN 到底是怎麼想的。GAN 的想法也很簡單：你今天不要在創作者上動太多的手腳，不要做太多的限制，不要教它情歌要怎麼做，你就讓它自己去創作。但是自己去創作它當然是會亂做，所以我們需要找一個老師來監督它有沒有亂做。因此，GAN 最核心、最有趣的想法，就是它用了兩個模型：一個模型叫做 Generator (生成器)，也就是創作者；另外一個模型叫做 Discriminator (鑑別器/判別器)，就是老師。

[00:23:02] 這是生成器跟鑑別器的大對抗。生成器開始天馬行空地去做創作。這聽起來很神奇，其實一點都不神奇。比方說我要生出一張 512x512 的圖，輸出的結果就是 512x512x3 (RGB) 這麼多個數字就好了。它可能會拼出一張亂七八糟的圖，但它就是會拼出一張圖來。只要格式正確，神經網路就會吐出東西。

[00:24:14] 鑑別器就是老師，老師要評斷它到底做得好還是不好，像還是不像。如果今天要生照片，老師就要覺得這像是一張照片；如果是音樂，就要決定這像不像一段音樂。通過老師之後，才可以決定它是不是真的。

[00:25:02] 整個流程是這樣：生成器隨便輸入剛剛說的那個神秘的 Latent Tensor (通常寫成 z)。在 GAN 裡面，這個 Latent Tensor 基本上是隨機產生的 Noise，通常符合標準常態分佈。我們把它輸進去生成器 (G)，生出一張圖，我們叫做 G(z)。

[00:26:55] 再來鑑別器 (D) 要辨識它是真的像真實世界的照片，還是假的。我們為了簡化，假設做照片，比如台灣黑熊的照片。我們要怎麼訓練鑑別器？要多少訓練資料就有多少。我去外面找到 1 萬張真的台灣黑熊照片，然後讓生成器也亂生出 1 萬張它自以為是台灣黑熊的照片。這就有 2 萬筆訓練資料。對鑑別器來說，真正的照片輸入進去，正確答案應該是 1 (Real)；生成器生出來的，正確答案應該是 0 (Fake)。這就是訓練鑑別器。

[00:28:45] 鑑別器有點品味之後，我們再去訓練生成器。生成器的目標是什麼？它生出一張照片 G(z)，經過鑑別器之後，它希望老師可以肯定它，所以它的正確答案永遠是 1。這個時候鑑別器是不動的 (參數固定)，生成器要想辦法調自己的參數，使得它生出來的圖在鑑別器那邊得到的分數越接近 1 越好。

[00:29:28] 它們就這樣子交替訓練。為什麼要交替？理由很簡單：如果一開始把鑑別器訓練得太厲害，生成器每次都被打 0 分，它就沒興趣學下去了，梯度會消失，所以不能一下弄那麼厲害。第二個理由，因為生成器也會進步，所以訓練資料要交互替換。一開始生成器亂生，很容易鑑別；等生成器生得比較像的時候，再去訓練鑑別器怎麼分辨。

[00:30:37] 訓練到最後，發現生成器真的可以生出不錯的作品的時候，就結束了。這就是生成對抗網路。為什麼叫對抗？因為生成器希望鑑別器給它 1 分；但鑑別器希望把生成器生出來的判別為 0 分。它們的目標基本上是相反的，所以叫做生成器跟鑑別器的大對抗。

[00:31:58] 聽起來原理講完了，但這其實是一個非常殘忍的訓練方式。我用畫圖的方法跟大家說：今天我想要學畫圖，去一個很有名的老師那邊上課。第一天老師什麼也沒教，叫我畫。我畫完交上去，老師看得很不高興，打我一巴掌。我就回去繼續畫。他也沒告訴我哪裡畫不好，透視不對還是顏色不對，完全沒教，就是打我一巴掌。我回去再畫，交第二張，老師又不爽，又打我一巴掌。經過打了一萬次左右，我終於會畫圖了。這真的是非常殘忍的教育方式。當然你稍微有點進步的時候，老師打得比較輕一點，但還是打你。很神奇的是，這個 AI 非常耐打，它就真的學會了。

[00:33:26] 我們利用一點時間說一下，現在這個時代如果你對 AI 有興趣，有個好消息是幾乎所有的文章、論文都會登出來。因為現在大家為了佔山頭，很怕等到投稿登出來的時候已經有人先發表了，所以大家都會放在公開網頁 (如 arXiv) 上。壞處是，現在的期刊或研討會，如果你的文章裡面沒有數學符號，大家會覺得你的文章很低級，所以一定要用各種數學符號嚇你。

[00:34:43] 我現在就要跟這家講這個數學符號。
第一個，Latent Vector 的分佈 ($P_z$)。這比較不重要，通常就是標準常態分佈。
第二個，真實世界的分佈 ($P_{data}$ 或 $P_x$)。老實說我們完全不知道什麼是真實世界的分佈，這只是一個概念性的東西。我們拍照是拍一張照片，不是去算機率分佈然後抽樣。但在概念上我們會說這張照片是從真實世界的機率分佈抽樣出來的。
第三個，我們呆萌型 AI 機器人 (Model) 生成的分佈 ($P_g$)。

[00:37:48] 現在要插播一個奇怪的事情：介紹 Log 這個函數。為什麼要介紹 Log？Log 有一個非常特別的地方，它是遞增函數。在 0 到 1 中間，接近 0 的時候是負無窮大，接近 1 的時候是 0。Log 還有一個可愛的地方，它會把乘法變成加法，除法變成減法。在很多時候取 Log 比原本的機率分佈有道理多了。

[00:39:56] 我們回到鑑別器。鑑別器的目標是：

看到真實世界的資料 ($x$)，要給高分 (接近 1)。

看到生成器生出來的作品 ($G(z)$)，要給低分 (接近 0)。

如果我們取 Log：
對於真實資料 $D(x)$ 接近 1，$\log(D(x))$ 就會接近 0 (最大值)。如果鑑別器很沒水準，真實資料給很低分 (例如 0.001)，$\log(0.001)$ 就會是負很大的數字 (扣很多分)。
我們在算 Loss Function 的時候，大家喜歡寫成期望值 ($E$)。其實期望值就是平均值。在真實算的時候，就是把這些分數平均起來而已。

[00:43:25] 對於生成器生出來的作品 $G(z)$，鑑別器應該要給越低分越好，也就是 $D(G(z))$ 越接近 0 越好。
那我們看 $1 - D(G(z))$，如果 $D(G(z))$ 接近 0，那 $1 - D(G(z))$ 就會接近 1。
所以鑑別器的目標函數 (Objective Function) 通常寫成這樣：


$$\max_D V(D, G) = E_{x \sim P_{data}} [\log D(x)] + E_{z \sim P_z} [\log(1 - D(G(z)))]$$


兩項都是越大越好 (越接近 0 越好，因為 Log 最大是 0)。

[00:44:19] 那生成器 (G) 呢？目標完全相反。它希望 $D(G(z))$ 越接近 1 越好。所以它希望上面那個式子越小越好。
這就是生成對抗網路可怕的數學式子，但如果你知道它在幹嘛，其實沒這麼可怕。它只是在算平均而已。

[00:44:56] 好了，我們說完了。生成對抗網路的原理說完了。
[00:45:14] 好，那我們現在要解釋另外一張看起來有點可怕的圖，就是這張。這個是原版 GAN 論文裡的原始圖片。大家要學一下，當你在外面表現一個概念的時候，本來你可以簡簡單單地說：我們 GAN 訓練的目標，就是希望兩個機率分佈越接近越好。

[00:45:40] 我再說一次，我們真的不知道、沒辦法看到那個機率分佈到底長什麼樣子，但反正它是一個機率分佈，所以概念上我們可以畫出來。我們就概念上亂畫，比方說我們說真實世界的機率分佈（$P_{data}$），也就是真實世界大部分的點應該是長在這個黑色虛線的附近。

[00:46:05] 然後綠色實線 ($P_g$) 就是我們的神經網路（生成器）的機率分佈。一開始的時候，你會發現，雖然這是概念上的圖，但很清楚地表示：一開始鑑別器（Discriminator，藍色虛線）應該很容易把它們區分開來。因為它們真的長得不像，一個在左邊，一個在右邊。所以鑑別器在真實資料這邊（$x$）都會給高分，在生成資料這邊都會給低分，很輕易地就分出來。

[00:46:42] 這張圖是概念圖，但你如果看懂這張概念圖，出去外面講話，人家都會覺得你超有水準、超有學問的感覺。

[00:46:58] 然後一直訓練、一直訓練，因為我們的神經網路（生成器）在繼續訓練的過程中，那個被打很多次的生成器會越來越進步。沒錯，生成器會越來越接近那個真正的圖的樣子。想要在外面唬人的說法就是：它的分佈會越來越接近真實世界的機率分佈。

[00:47:35] 當越來越接近的時候，你就會發現這個鑑別器有點困擾了。因為在交集的地方，它就不知道到底應該要給 1 還是給 0，它就不太會分了。直到最後，最好的結果就是它完全放棄，給它一半的分數，就是 0.5（二分之一）。因為它真的分不出來到底是真的還是生成出來的。這個時候我們就說學完了，學得最好的時候就是我們的鑑別器辨別出來大概是 0.5 左右，它分不出來是真的還是假的。這個時候我們就說學成了。

[00:48:18] 那個 WGAN (Wasserstein GAN) 我們剛剛有稍微說過，它其實是因為原來的 GAN 有點難訓練。最主要的原因就是因為它們兩個（生成器與鑑別器）的訓練目標完全不一樣，所以在那個地方很難訓練。所以 WGAN 想了一些方法讓它變得比較容易訓練。這在以前是很重要的事情。

[00:48:53] 其中有一件很有趣的事情是關於「崩壞」（Mode Collapse）。崩壞這件事情很有趣，大家有沒有聽清楚剛剛我們 GAN 到底怎麼訓練？生成器要生出一張圖，然後它希望這張圖能夠得到老師（鑑別器）的認可。但是那個聰明的生成器，如果你夠聰明，今天生出了一張圖，發現老師只要看到這張圖就不會打你了，你就知道成功了。

[00:49:20] 既然這張圖老師會接受、不打你，那你從此就永遠都交那同一張圖。這當然不是我們的目標，這種情況就叫做「崩壞」，就是模型的崩壞（Mode Collapse），它真的永遠都只生出這張圖。WGAN 解決的事情有兩件：第一件就是讓它比較穩定地可以訓練；第二件事情就是預防那個模型會崩壞。

[00:50:08] （休息後繼續）大家好，我們繼續。那剛剛大家已經知道 GAN 的原理就是這樣。我們現在要說一些快樂的事，這也不是真的那麼快樂啦，因為我們要介紹的是 GAN 的「高光時刻」。我覺得「高光時刻」這個詞用得還蠻貼切的。

[00:51:08] GAN 在以前曾經真的是電腦創作的王牌，尤其是 StyleGAN。我們先稍微介紹一下。StyleGAN 開始的時候是 NVIDIA 那邊做的。那個時候不像現在這麼變態，發表這篇文章的時候，他們宣稱他們只用單 GPU 而已。他們做了一個叫做 Progressive GAN，這個想法其實是很棒的想法。

[00:52:07] 他們的目標是要產生高解析度的圖，1024 x 1024。大家還記得我們當年 512 x 512 就已經宣稱大圖輸出了，那 1024 x 1024 是那個時候精度非常高的。但他們也知道要一下就到 1024 很難，所以想到了一個很聰明的事情：訓練 GAN 的時候，先訓練它生出 4 x 4 的圖，也就是解析度超低的圖。

[00:52:45] 你想，要生成那種 4 x 4 的圖應該就容易生了嘛。所以它第一層先訓練生出 4 x 4 的圖，第二層再把這 4 x 4 的圖去升級精細一點，比方說 8 x 8，然後再升級，一直升到 1024 x 1024。

[00:53:12] 所以今天輸入了一個 Latent Vector 之後，我先讓它訓練一個 GAN 生出 4 x 4 的圖，大家看著當然沒有人會滿意。但是它很會生了以後，再讓它升成 8 x 8，一直升到 1024 x 1024。理論上如果要繼續往上升也可以，但是運算資源會越耗越多。

[00:53:42] 那個時候做出來以後，大家真的非常震驚，這是第一次大家看到 AI 生出這麼像人的一些圖。這是第一代的 Progressive GAN，還不是最後大家覺得真的厲害的。就是你真的要人家告訴我們說這個是 AI 生的，那我們才認真地去看，認真看完以後可能才能勉強挑出一些缺點。大家就覺得，喔，這個真的有厲害了。

[00:54:19] 那我們要介紹一個網站，叫做 This Person Does Not Exist。這個是在 StyleGAN 那個時候生出來的。你可以看到 StyleGAN 的時代，不管是 Progressive GAN 還是後來的第二代 StyleGAN，生出來的例如人的照片、貓的照片，甚至還有假的租屋廣告照片。

[00:55:04] 現在越來越可怕，那個租屋廣告的照片，我們現在最多只是說這個照片跟實景不符，可能是很新的時候照的。但以後越來越可怕，那個房間的照片完全沒有那個房間，你很高興去了以後才發現沒有。這個也可以生出二次元老婆，什麼都可以。

[00:55:38] 我們現在示範一個給大家看一下。你看，生出來真的很像一個人的照片，真的相當厲害，以現在的角度還是覺得很厲害。你再按一次，它就會再生出一張照片來。

[00:56:04] 你可能會覺得說這個不是很厲害嗎？那為什麼現在不用呢？其中一個重要的原因就是它最多就生出這樣，它比較沒有辦法再做更多的控制，讓我們用文字去表達更多細節。比方說我要畫這個人坐在咖啡館幹嘛幹嘛，它比較單純訓練那種大頭照的時候可以表現得很好，但比較複雜的意涵的時候，就沒有辦法做得這麼好。

[00:56:48] 但是有一件事情是跟後面有關係的。就是我到底怎麼樣把這個意思帶進我們要生出來的東西？因為標準的 Latent Tensor 基本上就是從一個常態分佈裡面隨機生出來的一串數字。後來他們發現，其實我們可以訓練某一種 Tensor 或向量，叫做它的 Style（風格）。

[00:58:02] 所以我今天可以把這兩個向量加起來。一個是標準的、隨機抽出來的數字（代表天馬行空的想法），另一邊把我的「風格」加上去。然後就產生一個新的 Latent Vector，這堆數字就可以去產生我的圖。如果我們訓練得很好的話，它就會照著我們的意思把我們要的圖給生出來。

[00:59:07] 其實這個「加」不一定是真的加，概念上你可以想成它就是融合。如果你有興趣可以去研究一下它怎麼混，總之它在生圖的過程中，一直告訴它我要的風格是什麼。這真的很像我們後來的 Prompt。我們下了文字 Prompt 之後，其實也是轉成一個向量交給 AI 去處理。

[01:01:13] 接下來我們舉幾個比較重要的例子，標準的 GAN 還做了一些什麼有趣的事情。這在今天來說其實都不是太先進的技術，大家現在應該也不會這麼做了，但為什麼要回顧？除了讓大家看 GAN 的高光時刻，另一個原因是想讓大家知道，在用任何神經網路的時候，其實我們的「想法」很重要。

[01:01:57] 那時候單純有一個這樣的想法：比方說我今天派無人機上去拍了一張學校或街道的空拍圖，我是不是可以直接轉成 Google 地圖？這是很多工廠、廠區很希望做的事情。

[01:02:46] 大家可能會想說，生成器就是輸入一張空拍圖，輸出就是一張 Google 地圖。但是大家會發現這樣子做不行。為什麼？因為鑑別器完全在鑑別的事情只是「這像不像一張地圖」。所以生成器就很聰明地發現，它只要畫出地圖，即使跟那個空拍圖一點關係都沒有，我們的鑑別器就會接受。

[01:03:29] 如果是標準的生成對抗網路，它就會亂畫，畫得跟輸入一點關係都沒有。所以要怎麼辦呢？其實答案非常簡單。生成器輸進去還是一樣（輸入空拍圖，輸出地圖）。但是，送給鑑別器的不是單純只把地圖送進去，而是把原來的空拍圖一起送進去。

[01:04:11] 意思是說，只有「右邊那張地圖」是「左邊那張空拍圖」相對應的地圖的話，我們才通過，才說它對了（1）。如果它即使畫地圖畫得很好，但並不是左邊那個空拍圖應該有的樣子的話，那我們還是給它 0 分。所以鑑別器就是做這件事情，它變成需要「配對」（Paired），成功配對的才算成功。這就是 Pix2Pix。

[01:04:46] 所以可以做很多有趣的應用。比方說，它可以畫出比較像真實世界的街景。原理是一樣的，就是把左邊那張標註圖（Segmentation Map）放進去，訓練好之後，生成器就會生出很像真實世界的街景。

[01:05:28] 你可能會說為什麼要做這件事情？因為左邊那種標註圖，特別是有設計過電動玩具的話，其實很容易生出來。要動也可以，大小要改變也可以。不用任何 AI 你就可以做出這種事情。但右邊要真的很像真實世界的街景就難了。

[01:06:16] 這最大的應用是訓練自駕車。你可能會覺得 Google 地圖車明明就在大街小巷拍攝，可以把真實情境錄下來。可是問題是，因為你不知道那台可愛的自駕車會有什麼樣子的反應。自駕車還沒有訓練好，它在應該順著往前走的情境突然往右轉，那 Google 地圖完全沒有它往右轉的時候會發生的情境，因為街景車是正常走在路上。所以你需要有一個很快就可以生出來這種改變的真實世界情境，去訓練自駕車。

[01:07:32] 它用的一個技術叫做強化學習（Reinforcement Learning）。強化學習就像訓練狗貓一樣，成功給獎賞，失敗給處罰。如果不把它放到模擬情境，直接放到路上亂開，撞死 1000 個人之後終於學會開車，那當然不行嘛。所以一定要用模擬的情境去訓練它。

[01:08:27] Pix2Pix 也可以去玩耍一下。作者自稱是貓奴，所以做了一個貓的生成器（Edges2Cat）。你在左邊畫一隻很像貓的輪廓，按一下它就會生出右邊那個很像真實世界貓的照片出來。我以前都會帶著大家在課堂上看，但後來發現每次畫出來都不太像，所以決定算了，大家可以回去自己玩玩看。

[01:09:03] 接下來，又有了一個非常有趣的法，叫做 CycleGAN。因為 Pix2Pix 還有一個缺點，就是我們需要這兩個東西有很多的「配對」。比方說生成貓，需要有貓的照片還有它的手繪輪廓版本。還好這件事情不是很難，但有一些配對真的很難。

[01:09:52] 我舉一個例子。曾經有一段時間很流行一件事情：把男生自己的照片放進去，它就生出一張照片說「如果我是女生的話」就長那個樣子；反過來也是。

[01:10:43] 如果用 Pix2Pix 可不可以訓練？當然可以。但訓練的時候很遺憾的是要做一件事情：比方說我今天是一筆訓練資料，我需要全世界去搜尋一個長得很像我的女生，來做成配對。這聽起來好悲傷喔。總而言之，你需要去找到很多這樣的配對，可能要上萬筆，然後才有辦法去訓練。大家發現這實在太困難了。

[01:11:32] 因此 CycleGAN 的做法就想到說：可不可以根本不要配對就去做這件事情？做法是這樣：它訓練了兩套 GAN，也就是生成器有兩個。一號生成器叫做 G，二號生成器叫做 F。

[01:11:57] G 這個生成器是幹嘛的？就是我放一個 Domain A（例如男生的照片）進去，就要產生一張 Domain B（女生的照片）。如果完全沒有控制，它會亂生。所以我們需要做另外一件事情，就是找一個相反的生成器 F。這個生成器是輸入一張女生的照片之後，它要生出一張男生的照片。

[01:12:41] 鑑別器 A 唯一要辨別的事情，只是這張照片是不是男生的照片；鑑別器 B 就是鑑別是不是一張女生的照片。所以你會發現大家做的事情基本上跟以前的 GAN 是一樣的，只是做了兩套。

[01:13:10] 那有什麼不一樣呢？不一樣的地方是：今天男生的照片進來以後，G 生出一張女生的照片。然後因為有一張女生的照片了，把這張女生的照片輸進去 F 之後，要回到我原來的這個人的照片，才叫做成功。也就是必須輸出一張照片，然後 AI 要可以辨識這兩個是同一個人。這叫做 Cycle Consistency。

[01:13:41] 用這樣的訓練方式，就不需要配對就可以訓練了。那個時候的自動翻譯（AI 翻譯）的一個高點就是利用類似 CycleGAN 這樣的想法，完全不做配對。因為有些翻譯做配對有困難，或者是古老的語言文字真的不知道意思，沒有辦法配對，就可以用 CycleGAN 這種方式訓練。

[01:16:07] 那個時候他們做了一個另外非常厲害的示範，就是 GAN 的高光時刻的一個重要作品：放一隻馬進去，它就會變成斑馬的樣子。如果有大家知道的話，斑馬的尾巴跟馬的尾巴是不一樣的，他們做得很厲害的事情是並沒有把尾巴換掉。所以你會看出來那不是真的一隻斑馬，那隻斑馬是生成的，而且可以即時（Real-time）生出來。

[01:17:13] 那個現在在陽明交大的魏澤人老師，他那個時候做了這個。他把館長變成「館長一」。裡面是館長的聲音，但是你會看到新垣結衣的臉。聽起來也是蠻悲傷的。但那時候你會發現技術還是會有一些瑕疵，就是所謂的「土石流」，臉部可能會崩掉。

[01:18:08] 不管怎麼樣，大家發現 GAN 好像真的做到我們心目中想的生成式 AI 要做的事。GAN 真的是生成式 AI 的希望，連翻譯、換臉什麼都做了。所以大家覺得 GAN 真的是電腦創作的巔峰時期。

[01:18:36] 直到後來出現的，我們這個學期會介紹給大家的 Diffusion Models。這其實非常的有趣。如果在一年多前開課的話，我們絕對跟大家講 Diffusion Models 就是生圖的第一把交椅。但是在現在又開始有一點點動搖。反正總而言之，這些技術是我們會介紹給大家的。到底哪邊又開始動搖了，我們會再介紹給大家。

[01:19:05] 好，那今天最後的時光...我先來說一下作業好了。
[01:19:05] 好，那今天最後的時光...我先來說一下作業好了。我怕到時候萬一沒有機會說到作業，來不及告訴大家說今天這個作業要做什麼。

[01:19:29] 今天作業聽起來是這一次最「技術」而且最「數學」的一次。這對那個痛恨數學的真的不知道怎麼辦好。但是事實上沒有這麼複雜，我現在要解釋給大家聽。

[01:19:44] 我們要解釋的是一個很重要的、我們之前介紹過的一個概念，叫做 Softmax。還記得嗎？就是強迫把三個數字或是五個數字或多少個數字，強迫把它加起來變成 1 了。那個技術真的非常非常重要，那個是一個很重要的關鍵點，特別是我們未來的生成式 AI 大型模型，那個是超級重要關鍵點。

[01:20:08] 所以呢，我們要做一件事情。這等一下會說出比較可怕的話來，大家先聽一下。我們作業最後呢，就是你寫一個程式出來，然後它要可以算 Softmax。算完了以後呢，你去觀察你不同的數字丟進去，讓它做 Softmax 加起來變成 1 了之後，它到底有什麼特點？也就是說你有觀察到什麼事情？因為再說一次，Softmax 這件事情真的特別是對我們那個文字模型非常非常的重要。

[01:20:55] 這個聽起來是不是很可怕？很可怕，因為這對不會寫程式的同學覺得這個超可怕。雖然函式沒有很難寫，真的沒有很難寫，其實算是相當的簡單，但是畢竟我們有一些同學是從頭到尾都沒有寫過程式的。所以我們要發揮生成式 AI 的特色，而且我已經把 Prompt 已經幫大家寫好。

[01:21:20] 也就是那個 Prompt 基本上的內容就是說：我現在想要學 Softmax（我當然是以老師的身分寫的，你可以改成說你想要學 Softmax），所以呢，今天請用 Colab 幫我寫一個程式。但是呢，因為我主要的是要了解 Softmax，所以呢，你不要一段就給我把所有的程式寫完，你要分段寫，像我們平常在 Colab 裡面比方說畫一個函數圖形的時候做的事情，分段寫。而且你要給我那個 Markdown 做的那個很好的解說。就是這樣，讓我放在 Colab 上。

[01:21:56] 剛剛那一段話大家不用記，因為我們的作業的題目裡面有，這個是我的示範的 Prompt 已經放下去了。你基本上直接的如果你都不要改，你就直接的貼到某一個你最信任的大型模型，像是 ChatGPT、像是 Grok、像是 Gemini、像是 Claude 等等等等。反正你信任哪一個大型語言模型，或是說哪一個大型模型那個時候理你了，你就用那一個就對了。然後他就會給你一個程式出，然後你就想辦法把它剪貼到你的那個可愛的 Colab 裡面去，結束了，就是這樣。

[01:22:30] 但是呢，請大家還是要試一下，因為你還是要執行。然後你發現有問題的地方或是更不清楚的地方，你再多跟那可愛的生成式 AI 多溝通一下，或是換成你自己的想法。所以我們的作業需要交出來的第一個就是你到底有沒有改那個 Prompt？沒有改也沒關係，你就照著附是沒有問題的，但是我建議你自己跑的時候，你就會發現說這個還是改一下 Prompt 可能會更清楚。好，你就試著改那個 Prompt，然後你就貼出你到底你最後的 Prompt 到底是怎麼下的。這是第一件事情。

[01:23:07] 第二件事情是，你下那個 Prompt 之後，幫你寫好的 Code，就是你把它貼到 Colab，然後是真的可以執行的 Colab 的作業。然後那個 Colab 最好你還是改成你自己的作業，意思就是說有一些解釋的方式是你自己的解釋方式，或是有一些地方你覺得有一點點問題，特別是有寫程式的同學，這你發現說有點點問題，你再想辦法看一下說它是不是真的寫錯了，或者寫太複雜，把簡單的東西寫得超級複雜之類之類的。

[01:23:40] 然後我們中間其實還有說它要有一個互動的表示方式。也就是說你最後是可以自己輸入幾個數字，然後它就告訴你說這幾個做 Softmax 的結果。那甚至它可能會做成那個數值滑桿的方式，然後你就可以拉數值滑桿，然後去比較一下，你去觀察一下到底發生什麼事情。

[01:24:08] 所以第一個是你改的 Prompt；第二個是你最後要跟用那個生成式 AI 協助，然後生出一個 Colab 的檔案。那這個檔案最好你把它改成完全全是你自己的作業，也就是改成你自己的風格、你自己的解釋。然後第三個一定需要有的東西就是說你到底發現了什麼？就是你發現的 Softmax 到底有什麼特性？不要告訴我加起來等於一，加起來等於一是我們都知道的特性。然後你去觀察一下 Softmax 到底有什麼特，比方說本來兩個分數很近的是不是真的會被拉遠這樣子（例如啦）。你去看一下、觀察一下 Softmax 到底有什麼特性。我再強調一次，這件事情非常非常的重要。

[01:24:52] 然後這一次的作業真的聽起來很技術，但實際上沒有那麼技術。然後也會讓沒有太多寫程式經驗的同學，其實在寫完這個作業的時候應該信心會大增。因為我們這一次是完全沒有範例的，好像是唯一一次不會有範例的程式作業。完全沒有範例，但是其實有那個 Prompt 範例，所以 Prompt 下去了以後，基本上它已經完成了至少 80% 左右了，你再稍微的修改就可以了。那當然你可以去試驗一下下，你就會發現你也很容易做出比老師還要好的 Prompt。大家可以試試看。

[01:25:34] 好，那我們再接著下來呢，最後的一點時間呢，要為大家解釋的是一個這個在學 AI 裡面呢，你一定要懂的，然後看起來又有點可怕的，如果你之前沒有弄懂的話，看起來一直會有點可怕，叫做 Cross Entropy。

[01:25:51] 我們來看一下下。前情提要就是說我們在神經網絡的時候，我們舉了一個例子叫做「巴哥」就是鑑別器（辨識器）。就是今天輸入一張巴哥的照片，我就要輸出三個分數。那如果說它成功的話，就是說它把正確的那一個給了最高分。那我們可以做的更好，就是用 Softmax 把它加起來變成 1。好，就是這樣子，就經過了 Softmax 之後加起來變成 1，然後因為正確答案是 100%（One-hot Encoding 的概念，雖然老師口語說是 100），所以這個的確是正確的。

[01:26:29] 然後我今天呢要跟大家說的事情是說，那我們這個 [0.6, 0.3, 0.1] 呢和正確答案 [1, 0, 0] 的差是多少？你就會發現是 0.26（如果用MSE算）。那另外一個其實照說應該是差不多的表現，其實照 [0.6, 0.2, 0.2] 因為我們只關心第一名嘛，第一名都是 0.6，所以照說第二個答案我們也會說他答案是正確的。

[01:27:00] 可是呢，照理說那個誤差應該是基本上是一樣對吧？就是因為他都是說 0.6 嘛，所以他基本上我們應該是說這樣應該沒問題啊。假設說有第一筆資料，第一個模型告訴我們是 [0.6, 0.3, 0.1]，第二個模型告訴我們是 [0.6, 0.2, 0.2]。哪一個模型比較好？基本上就是一樣好，因為他們都說第一名是它。但算出來的誤差，如果我們用那個相減平方加起來（均方差 MSE）加起來的話，做出來的話，以後它的誤差，兩個誤差是不一樣的。這第一件事。

[01:27:42] 第二件事情其實更糟糕的是，如果我真的很懂這件事情的話，因為白尾八哥跟那個土巴哥（一號跟二號）其實是比較接近的。所以老實說如果真的要說的話，其實第一個答案比第二個答案還要好。因為第二個答案居然跟我們講說那個兩個根本不像的那個機會是一樣大的（0.2 和 0.2），一點都不好。

[01:28:07] 所以種種的原因我們會發現說，這樣算好像不是這麼好。所以呢，我們有沒有辦法想一個辦法，至少讓他們說這個 60% 的時候，兩個的 Loss 是一樣的。因為他們答題的對我們來說其實我們會覺得他們的答題狀況是一樣好的。

[01:28:33] 好，那我們就來說一個很有趣的一個數字。這個有趣的數字呢，就機率一個機率出來了以後（P），然後有一個很有意思的概念，雖然這個有意思的概念應該叫做 Entropy，但是我們先不要管它，因為用 Entropy 用英文，然後或是「熵」用中文，兩個說出來大家都覺得沒什麼意義，如果你不知道這個本來的意思是什麼的話。

[01:29:06] 今天我們發現一件事情，就是那個發生機率。機率都是 0 到 1 中間，那如果發生機率很低很低的話，我們會覺得比較驚訝。所以人家告訴我們這件事情的時候，我們會覺得喔，這個驚訝，這個資訊量很大。那如果你告訴我們的是一個廢話，那個就資訊量很小。

[01:29:34] 我舉一個例子。我以前在念博士的時候，在南加州。南加州一個特點基本上就是不下雨的。然後那邊像我們的天使隊這個大概 10 年才一次因雨延賽。我們沒有什麼大巨蛋，我們沒有喜歡大巨蛋喔，不下雨的地方你蓋什麼大巨蛋。反正就是因雨延賽的機會非常小，因為基本上就不下雨。

[01:30:03] 所以呢，你今天告訴我說這個明天是不下雨的，神經病這告訴我這個幹嘛？沒有資訊量。但是呢，如果你告訴我說明天會下雨，這個資訊量大了。因為這機率很難發生的事情，明天居然要發生了。我順便說一下這個資訊量大到什麼程度呢？就是如果你在那個加州、南加州那邊呢，有一天下雨了以後，你就會看到那個記者很像我們在播颱風的樣子，穿著雨衣在路上（下雨而已），又說哪個地方又有車禍了，因為那個天雨路滑那個又車禍了，然後等等等等。

[01:30:47] 所以如果告訴我們說有一件事情很少、機率很低的事情告訴我們說會發生，那個量很大。所以我們就想要用一個數字來表示資訊量的大小。我再說一次，如果這個數字越大的話（機率越高），資訊量越小；機率越小的話，資訊量越大。

[01:31:12] 然後我們又想到這個可愛的 Log 啦。可愛的 Log 快要對了。就是機率很接近 1 的時候，資訊量是 0 啊，很好，這個數字太棒了。取 Log 之後那這些很小的時候呢，那個是負的很大的值。唯一不好的地方就是它是負很大的值，它不是正的。那把它變成正的沒有什麼問題，就加一個負號就好了。所以就變 $-\log(p)$。這個看起來乍看之下有點可怕的，其實就可以代表是資訊量的大小。

[01:31:41] 那資訊量大小意思就是我們驚訝指數的大小。就是我們聽到的會比較驚訝還是比較不驚訝。所以就是聽到了會議很驚訝，就是資訊量大的意思就是說這個事情其實很低的機率會發生，它不是常態，所以我們會覺得很驚訝，資訊量就大。那根本就是一直都會發生的事情，我們就會覺得這個資訊量很小。所以表示的方法就是 $-\log(p)$。

[01:32:17] 好，這剛剛我們說的資訊量大跟小的情境。所以它的名字叫資訊量，我們可能用 $I(x)$ 去表示這個資訊量。好了，說完了。然後 Entropy 呢，其實基本上就是資訊量的期望值。結束了，就沒了，就是這樣子。

[01:32:34] 那跟我們的 Loss Function 什麼關係呢？就是回到我們的八哥。如果今天是在正確答案的話呢，我們就希望呢他給正確答案是越高分越好。就是我們要越不驚訝越好。就你的答案不要那麼讓我們那麼驚訝，今天明明是土巴哥，讓你告訴我加巴哥嚇死人的，你在亂說什麼東西。

[01:33:02] 所以說呢，我們希望說呢，那個正確答案的那一邊那一個數字呢，是資訊越小越好，就是越接近正確答案越好，就它的機率越高越好。意思就是說如果我今天明明是正確答案，你還是跟我講機率很低，我就要給你大扣分，就是要給你很大的數值，告訴你說很不對，就是要扣的分數要很多。就是這樣子來的。

[01:33:35] 所以呢，你就會發現說我們只關心那個正確答案那一個數字。所以呢，剛剛的 [0.6, 0.3, 0.1] 和 [0.6, 0.2, 0.2] 就會用一樣的、一樣的那個過來。就是這樣子。

[01:33:49] 所以呢，再說一次，在我們的世界裡面都會跟人家講說，我們就是希望跟真實世界的那個真實的那個機率分佈是越接近越好。但是我再一次強調，我們算出來的時候是大家回家的作業 Softmax 算出來、硬算出來的。你可以去品味一下它會不會跟真實世界是不是真...其實基本上就是不會啦。

[01:34:21] 但是我們在跟人家說的時候，特別是我們不太喜歡的人都可以跟他講說：我們的那個 AI 呢其實很簡單，就是呢我今天決定輸入跟輸入，它其實只是一個函數。我就要想辦法達到一個函數學習機，把這個函數學起來。這個函數的定義域呢，就是一個 Tensor；輸出的 y 也是要一個 Tensor，要固定大小 Tensor。我們希望呢，我們的這個訓練的目標就是希望我們的這個訓練出來的機率分佈呢，是越接近真實世界機率分佈也好。那人要完全聽不懂你在說什麼，也不知道怎麼做，但事實上其實真的是這個樣。

[01:35:04] 我們會發現 Cross Entropy 其實是比那個我們本來用的那個均方差其實是更好的 Loss Function。好，那大家到底怎麼算 Cross Entropy？就是你就知道記得說我們的目標就是希望越接近真實世界的數據其實是越好的，越接近真實世界的機率越好。你剛剛會說老師您剛剛明明告訴我說這個我們沒有辦法知道真實世界機率，但是在我們的訓練資料裡面，我們就假設真實世界機率就是機率分佈就長這樣，比方說土巴哥就是 100（1），加巴哥就是 0，0 之類之類的。那個就是真實世界的，我們希望說它我們訓練出來神經網絡越接近這種答案是越好。

[01:35:59] 那不管怎麼樣呢，就是真實裡面有一個機率分佈，就是 $P_1$ 到 $P_n$。然後呢，那個 Entropy 就是資訊量的期望值。所以我們常常會說 Entropy 就是「亂度」。就資訊量都很小的話，就表示說這個事情的變化其實真的很少、很少，所以我們可以預期基本上就是長那個樣子，所以就是亂度很低。那反之呢，就是今天如果大家發生的機率，比方說你到一個地方在加州的時候，天氣是下雨，天氣這個資訊這個 Entropy 大概就很少，因為基本上都晴天。但是呢，你去一個地方是一半會下雨、一半會放晴的地方呢，你就會發現喔，這個 Entropy 就亂度、就是複雜度會就變成比較高。所以你算出來 Entropy 就是資訊量的那個期望值就會比較高。

[01:37:14] 那 Cross Entropy 就是記得我們的 Cross Entropy 想要做的事情呢，基本上呢就是希望能夠看到這兩個機率分佈（真實世界的機率分佈 $P$ 跟正確答案的機率分佈 $Q$）到底是接近還是遠的。那所以我們一樣去算一個好像是期望值的東西，只是呢這個後面的這個資訊量呢其實是用那個我們的神經網絡的答案（$-\log q$），那前面的機率分佈呢就是用真實世界的機率分佈（$p$）。那你可以想見的事情是呢，這個東西最小最小的時候，就是我們的機率分佈跟真實世界的機率分佈是一樣的時候，Cross Entropy 是最小的。就這樣子，說完了。

[01:38:32] 那當然還有一個更可怕的詞啊，就是 KL Divergence (KL 散度)。那個 KL 散度其實很簡單，就是 Cross Entropy 算出來的時候其實不是 0（最小值不是 0）。就是兩個機率分佈一模一樣的時候 Cross Entropy 不是 0。那我們當然當然你已經跟正確答一模一樣，你居然不是說扣零分，這個真的有點過分。那我要怎麼樣做到呢？很簡單，就減掉真實世界的機率分佈（Entropy）。你就會發現兩個機率分佈是一樣的時候，它相減就是 0。就這樣子，大家這個不是很難的，所以大家可以看一下下。

[01:39:21] 到呢，有一件事情是跟我們生成式 AI 有關係的事情。就是到底什麼時候要用 KL 散度？其實有一個動作叫做 蒸餾 (Distillation) 了。

[01:39:49] 我花一點點時間來講一個很重要的，特別是 DeepSeek 出來以後大家覺得很重要，就是大家一直會說到「蒸餾」這件事情。大家在外面形容的相信絕對不知道他真的發生什麼事情，大家都說我們有一個大的老師模型，然後去訓練一個小的學生模型，就是老師教學生的概念嘛。你聽完了，但是你不知道發生了什麼事。我現在跟大家講蒸餾到底發生什麼事情。

[01:40:21] 蒸餾是這樣子。就是呢，今天我先訓練一個大模型，訓練得非常非常的好。假設訓練得很好。然後呢，我訓練完了以後呢，本來照理說呢，我看到一隻八哥，如果我要訓練這個學生模型的話，我訓練它的話，本來因為我們有訓練資料，本來應該這樣子訓練它，說這個是 100（1），就是這個正確答案應該是 100，我們的目標應該是 100。這叫做 Hard Target，就正確答案是這樣子。

[01:40:52] 但是這個正確答案裡面呢，有一點點看不出...就是其實這個白尾八哥跟土巴哥（一號跟二號）是比較接近的，三號比較遠。其實看不太出來。但是一個好的老師他可能領會到了，所以他真的有學到說喔，他覺得說這個應該是 [0.7, 0.25, 0.05]。也就是說我們訓練學生的時候的答案，不是用真實世界的、不是用我們的訓練資料的那個答案去訓練他，而是用「老師的答案」去訓練他。這在概念上就有點像是說老師經過消化吸收了以後再去教你，所以你會比較容易懂。

[01:41:38] 意思就是說比方說課本來有這個白底黑字，他說的當然都是對的，假設那個課本真的都是寫的都對。但是呢，這個不是經過老師消化吸收的告訴你，所以你要自己重新再去學習比較困難。那蒸餾其實簡單的說就是去學老師到底就老師的答案啊（Soft Target），就去學老師的答案。那意思就是說那有機會，因為老師已經經過他的努力的消化吸收了，所以你有機會用比較小的模型就去把這個比較大的學會。這個就是跟大家介紹的這個特別是蒸餾，其實跟我們的生成式 AI 後面又有一點點又很強烈的關係。

[01:42:24] 這個就是今天為大家介紹的。不要忘了我們的作業，今天的作業。那有問題再一次，就是有問題就記得我們對於所有的同學都有線上的 Office Hour 的時間，請大家如果有問題可以多來跟老師或是助教討論。好，這就是這樣子。我們先休息十分鐘。

[01:42:51] （助教 James 連線分享）
可以聽到我的聲音嗎？有聽得到。這邊，好的好的。老師祝各位同學大家好。我是因為個人的因素，所以我現在人其實在神奈川，所以不在台北，所以可能連線會比較緩慢一點，再請大家見諒。

[01:43:07] 那我要其實要分享的，我本身並不是...我台電系的碩士班，那本身並不是做 AI 相關的，但是還是想要跟大家分享一下一些 經濟學人 (The Economist) 最近有關於 AI 的報導。

[01:43:21] 那其實最近大家都在中美競爭。中美競爭其實兩個 approach。像美國他們就有很大的公司，基本上壟斷的各包括壟斷的高級晶片市場，那他們也導致中國在最近的發展上面都受限。那中國呢，比如說要像 DeepSeek，他們要求的並不是說 state of the art，並不是說一定要最好的模型，它反而是能更廣泛的而應用。

[01:43:53] 那這其實也導致一件事情，如果大家對於比較好的會議有興趣的話，有個會議叫做 ACL，它是計算語言學。這是今年 ACL 應該是自然語言處理相關的最好的會議。大家可以看到美國在今年的第一作者裡面，他只佔 14%。所以那個大會作者還特別在一開頭把美國標示很紅色的。大家到第一座只是中國的 51.3%。所以表示說其實在中國在美國晶片管制之下，他其實在應用的語音，包括這些論文的發展還是雖然受到限制，但是其實反而就是能激起他的鬥志。這第一個新聞在這邊。

[01:44:56] 第二個新聞呢是在 Google 他們有搞 Antitrust，就是大家常聽到反壟斷法。基本上就是希望把美國政府覺得 Google 太大了，希望把它分散。那其實後來因為有 ChatGPT 出現以後導致一件很大很大的一個進展是什麼呢？就是說大家以前要去搜尋一個，比如我今天要找一個解答，我會到 Google 打關鍵字，然後再連到那個網站上去找。那現在大部分的網站，包括 Bing 或是 Google 他們基本都是 AI assisted，表示說你其實不需要點到網頁，它就可以直接在第一頁首頁就讓你回答。

[01:45:33] 但是造成一個很大的問題就是說因為大家知道資訊就是錢，包括 YouTube 一樣，流量數就是錢。那導致說後台這些網站他們，他們的流量都被 Google 這樣大的網站放出（吸走）了。所以在今年 9 月的時候呢，美國的聯邦裁定說 Google 其實不需要做拆分。就是原本大家都期盼說 Google 會拆分，但是因為隨著 AI 的技術的一些就所謂的突破式的進展，然後導致說現階段以前運作的一些廣告收益模式有很大的一個 breakthrough，不能講 breakthrough，應該是一個很大的 interruption。所以大家裁判在他的法官在他的判決的時候，他覺得說在這個新的科技面，其實過去的網路的這些 Monopoly 其實已經不存在了，AI 會帶新的市場、新的運作機制。

[01:46:25] 第三個新聞呢就是 Princeton 大學的教授，他其實說 AI 就像以前的電力等等一樣，大家就一開始就覺得它很新很潮，然後大家都瘋狂學。但其實大家壞可能過五年十年以後大家都會了，所以它就變成說越來越多的人只是你要去運用這個工具，它就像電力一樣。這個跟那個 吳恩達 (Andrew Ng) 講的一樣，就 AI 就是現代的電力。

[01:46:55] 所以那這也會造成一件事情，就是說最頂尖的包括 DeepMind 或者 Open AI 或是 Anthropic 這些 researcher 他們有很大的、非常大的 allocate 很旁觀的收入。那其實一般比如說像初階的工程師或軟體工程師這種可以被取代的人，其實他們的薪水都會變得很廉價，所以這會也是兩極的發展。

[01:47:22] 最後一個，這應該是 27 號的 The Economist 的 topic，他們來講說他們在期盼說如果接下來 AGI 就是要叫做要怎麼講...GAI... sorry 我忘記全名了。他說以後 AI 他會自己發監督自己的發展。那他們可以期盼說其實從以前農業時代可能經濟 1700 年每年可能只每個 Century 只增長 8%；在工業進展以後大概每年會增長 35%。那 AI potential 就是說 AI 開始建自己的發展的時候，可能會每年每年（不是一個 Centuries）每年增長 20%。

[01:48:05] 那跟剛剛講的一樣就是到最能被 AI 取代會變越來越廉價，反而是那些 Cost Disease，這裡有講一個 Cost Disease，就比如說一些看護之類的，它的價格反會提高。所以你如果不想被 AI 取代，其實不一定要賺到這個領域，這是實在太競爭了，那其實可以考慮比如去種田之類，這也是一個辦法。

[01:48:28] 好，那 Take away 大家可以稍微看看就好。那其實 The Economist 是在最近其實他有一一直在發 AI 相關文章，然後他有些免費的 Podcast 然後也是蠻有趣的。大家如果有興趣的話，包括最新一期也是在講說 AI 的投資，如果像之前那網絡時代泡沫化會怎麼樣。那其實不會，然後現在市場也可以吸收。那大家有興趣的話其實都可以稍微看一下相關的文章。大概是我要今天分享的內容，如果大家有問題話也隨時歡迎提出。謝謝。

[01:49:10] （助教宣佈事項）
好，謝謝台大的 James 同學。那我先公佈一下，下禮拜確定會上閃電秀的同學是中央大學的呂佩真同學。然後下禮拜同一時間要記得要上 Butter 哦。好，那我先關掉。好，謝謝，可以離開。拜拜。

[01:49:56] 好那在進行助教課之，我先宣布一下課務的事情好了。那昨天早上有些有些人會收到 NTU 的通知就會被加進擴嘛，但他派是以旁聽生的身分被加入。但我後來有去跟平台去做反應的動作，那他們有去做修正，他們給你你們一封新的信件，他們再重新幫你們用學生的身分加入這門課。那之後還有遇到什麼樣的問題的話，只要指正大聲的部分都再來寄信來詢問我。OK 好。

[01:50:53] 那有鑑於的情況，所以有些人沒辦法繳交作業一嘛，那作業一的部分我們會延期到下禮拜一 9 月 22 號的時候再收。就要在那時間前繳就可以了。OK 嗎？那也就是說作業一跟作業二會是同一天收，OK，同一天要完成。好，那接下來就開始上助教課。

[01:51:19] 好，那我今天要介紹兩個應用。第一個應用是用在 Google AI Studio 上面。第二個應用會用到這個平台，但這只是一個過度而已，就是大家可以進行期待。好，所以這就是我們今天的助教課的三個部分。第一個部分先稍微介紹一下 Google AI Studio。

[01:51:48] 那 Google AI Studio 呢是 Google 所推出一個生成式 AI 的平台。它整合很多不一樣的模型，你都可以用，你都可以在這上面去做你的專案的開發。那它的優勢呢，就是在於是說它是一個人機互動的概念，它是一個你看到就點進去，就是一個很直接式的操作。OK，它技術門檻非常的低，它不用你學會打很多 Code。

[01:52:21] 那它的主要功能有幾個：一個是聊天機器人的設計，那一個是圖片的生成（我們今天介紹的應用會跟圖片生成有關）。然後第三個是簡報的製作，就是你選用不一樣的模型的時候，它就可以幫你塗一些簡報。那第四個就是跟 ChatGPT 一樣可以做你的文案之類。

[01:52:42] 好，那第一個應用就是之前陣子很紅很多人的在使用的就是客製的公仔圖片。OK，那我自己也有生成一個，然後這是這個圖片我放我自己在打羽球的樣子。OK，我自己也有生成很多個啊，那這是其中一個我自己比較喜歡的。那我們來一起玩玩看好不好。

[01:53:08] 那首先介紹一下這個流程就是：第一個就是進入這個平台 Google AI Studio；第二個部分就是下 Prompt，就像我們在打那個就 ChatGPT 一樣，就是給他一些 Prompt，給他一些文字的輸入，那它是會吐出根據那文字吐出我們想要的結果。那第三部分就是 AI 自己去生成；最後一步就是下載這張圖片。OK，好，那來打開一下我的網頁。

[01:53:47] Google AI Studio 的介面長這個樣子，你們只要有 Google 帳號就可以直接登入。OK，那今天我要介紹這個公仔的圖片生成呢，它是用 Studio 裡面的一個叫做 Imagen 的這個模型（編按：影片中口誤聽似 Nano Banana，依據上下文應為 Imagen 系列模型）。OK，它這個圖形，這個模型主要生成就是圖片。可以嗎？就是吃進去文字，然後吐出圖片。

[01:54:17] 那我先...我就隨便來打一個。先增加我背一些準備一些圖片。我們來生成看這個...生成看這個公仔我看一下哦...嗯...那來生個貓咪好了。好，比個贊。好，我們首先 Prompt 那要下什麼什麼樣的指令呢？下真實風格好了。「真實風格 貓咪公仔 在電腦桌前」。

[01:55:10] 然後桌上要有什麼東西？我們看一下我現在桌上好公仔嘛，公仔也要有包裝。「包裝盒」。請問各位助教喜歡什麼樣的包裝盒啊？Pop Mart 喔？你這個嗎？拉布布 (Labubu) 的？

[01:56:03] 然後大家還有想到什麼嗎？大家快點，現場人那麼多一人給一些意見有想到什麼樣的東西你們電腦桌上村的公仔，那你們那背景還要有什麼嗎？發揮大家想像，各位發揮一下現在大家應該都會坐在電腦桌前面吧？沒有嗎？... 就這樣嗎？

[01:56:57] 好，那我們先生成看看好不好，只有這麼一點點的文字，看他會生出什麼樣的東西。好，給他一點時間。欸，他不知道不是...他只根據這張照片生出相對應的。我覺得這公仔很像真實，但它沒有到公仔的樣子。

[01:57:36] 好，我們要不然再重新升一根據照片。呃我想要有有一隻吉伊卡哇在（抱著這隻貓）。好，來試試看。「坐在這電腦桌上」，「電腦椅子上」。好。

[01:59:00] 看一下看一下。好像哪裡怪怪，好像哪裡...哦，那隻手，那隻手穿過的那隻貓咪，我就覺得哦，原可怕。那我們試試看，如果有兩張我是兩個圖，兩張照片中的兩個人物跑出來，然後放在一起會發生什麼事，他能不能做出來。一個是這隻貓，那我再選這隻貓好了。「請將這兩隻貓咪互相勾肩搭背」，「互相勾肩搭背的方式做成一個公仔」，「馬路上」會很應該還行吧，「放在馬路上」。然後旁邊的包裝，旁邊還要有包裝。

[02:01:27] OK 有沒有達到我想要的東西？看一下，來看一下。欸，他這次又跑比較久哦剛才我們生成一張的時候，這跑 13.6 秒，這個他跑了 20 幾秒，他都還沒有... 哦，我覺得非常可愛，我以為他會做得超巨大的，然後站在一個十字路口中間。但久沒有。

[02:02:11] 好，這只是一個小平台介紹給大家，然後之後這平台大家做期末專案的時候也可以搭配使用。那這樣課的最後我會用 Colab 去呈現，那大家如果有帶到最後的話可以稍微看一下。

[02:02:38] 好，這是第一個，第一個應用。那我可以給大家稍微看一下說我之前升過什麼樣的圖片。天啊...我看一下哦...哦，不是這個。這個是我前幾天在生成的東西。這是右邊應該大家...我不知道大家有沒有在聽 K-Pop 那些的，反正右邊就是 aespa 的一個成員，那左邊是 IVE 的其中一個成員。然後我是輸入兩張照片進去，然後叫他吐出公仔的圖片，然後下一些 Prompt 之後，這是他最後的結果，就讓這兩個人同時站在一起，然後放在電腦桌上。

[02:04:36] 好，要不然這個 Prompt 我之後再複連接給大家好了。我之後會放在我的 Colab 裡頭，然後大家有興趣可以直接使用。

[02:04:42] 那我先進第二個應用。
[02:04:42] 那我先進第二個應用。第二個我要介紹的是 You.com 這個 AI 的輔助平台。那這個平台呢，基本上是以個人的搜尋引擎，就是有點像是 Google Chrome 一樣，只是它是有 AI 的功能。它會幫你收得更仔細，然後會幫你挑一些資訊出來，哪些比較重要、哪些可能沒那麼相關之類，它可能會幫你篩選掉。

[02:05:26] 但我不會使用到這個平台，我是利用這個平台的 API 來做下一個應用。我下一個要講的東西呢，是因為我自己本身是研究生，那我自己需要看很多的論文。我可能要在論文的文獻收集上面可能要收集比較久，我需要有人來幫我去做收集的動作。

[02:05:49] 所以呢，我去找到一個叫 Stanford 大學（史丹佛大學）幫忙開發的一個 AI 的平台，我就把它找到了。然後這個東西叫做 Storm AI 的風暴平台。那它可以幫忙的事情就是搜尋這個學術的論文，然後把多個你搜尋到論文的連接可以把它匯融起來。然後它還可以幫你做分析、做摘要，然後最後生成一個類似 Wikipedia 的那種文章出來。

[02:06:22] 我自己看完之後、我自己用完之後，我發現蠻學術的，但它不能當做一個正式的論文。OK，那為什麼會提到這個 You.com 平台呢？因為這個 Storm 的這個平台，這個 Stanford 的團隊啊，他在製作這個 Storm 平台的時候，它是需要做文獻的收集嘛。那文獻的收集的時候，它是利用 You.com 這個平台去做文獻的收集的，然後再利用 AI 去做分析。那它利用的 AI 可以是 OpenAI，可以用 Gemini 或者是用 Claude 之類隨便，然後最後再生成這個報告。

[02:07:04] 那今天只是做一些講解，大家可以上去玩看。然後 Colab 的連接就是今天有一些程式底子的時候可以再聽我等一下的介紹。那大家聽不懂也沒關係，之後也會再學，就是學怎麼串那些 API 之類的東西。好，那接下來我先打開 Storm AI 的那個網站給大家。

[02:07:38] 首先是 You.com，這個東西只是一個過度過客而已，就我平常也還好不會拿他來查東西，但會使用它的 API，等下再教大家怎麼用。好，這是 Storm 網站的網頁。OK，然後我自己通常會習慣用中文，就是查資料、看東西。那只是這個平台的缺點，也不是說缺點啊，就是只能用英文去查詢的動作。

[02:08:13] 好，首先我們來試試看創建一個文章。最近大家有在研究什麼主題嗎？各位各位夥伴們，大家有對什麼樣的主題有興趣嗎？我們現場來做一下好不好？試玩看看這平台。各位助教們，大家都在研究什麼東西？（助教：相機）。

[02:08:44] 好，有關相機的一個主題。比較學術內涵一點的有嗎？跟 Camera 相關的學術內涵有所...（同學提問）。OK 好，英文我不太會啊。好，我先請 ChatGPT 找一個。跟計算要如何均勻的預計算...好，讓他去翻譯一下。複製，然後回到我們的這個 Storm AI。

[02:11:40] 這是他們所開發的一個平台，然後就可以直接做使用。所以我只是想說就使用這平台給大家看。但我在看發生什麼事...（操作介面）。好，因為它是主要就是做學術論文的搜尋，然後生成一篇相關的文章。就是他是一個史丹佛大學團隊所做的一個網頁。

[02:13:02] 好，有人懶得看了。好，我就拿我之前稍微有做過了。好，這個是我之前隨便丟的一個問題，就是這個 Generative AI 如何應用在這保險上。然後就給他去做分析。然後在使用的過程中，他還會再多問兩個問題。一個是問說你研究這個主題的動機是什麼，那第二個問題是你你要達到什麼樣的目的、你想解決什麼樣的問題。那這兩個問題回答完之後，它才會生出以下的資訊。

[02:13:43] 比如我們打開翻譯 Summer 有打開它會生成他所生成的東西的一個簡介，然後再去生出說這個簡介之後會介紹說它會有什麼樣的...它會生成出什麼樣的主題。就是它會去搜尋那些論文之後去他做比對，然後看說最近都在應用在什麼樣的事情上面，最近比較多人在研究什麼樣的主題，那熱門主題有哪些，他就會寫出來給大家去做交叉比對。然後再生成出他自己認為的一個主題出來，再去做文章的描述，再去寫文章。OK，然後之後他會寫個未來是說你做完這件事情之後，你未來還可以再做什麼？

[02:14:46] 好，這就是我稍微要簡介的一個應用。那下一個我要介紹的事情是，我把兩個應用我用 API 的方式，我用 Colab 上面自己去製作。那第一個是公仔生成。那這指令哦我剛才我忘記了...這邊可可以直接複製貼上到剛才那個 Box，這 Google AI Studio 的 Box 裡頭直接做使用。

[02:15:39] 那這邊如果看不懂人就算了，沒關係，之後我們還會教。那這邊我唯一出現的問題是因為你使用 API 的話你需要...因為生成圖片那件事情呢，它需要大量的 API，它需要很多的 Token。但是身為一個秉持能免費就免費的人，我不會想要去課金，然後我沒辦法生成出來，就是因為我沒花錢。對，所以第一個公仔生成的如果要用 API 去做公仔生成的話呢，你需要做課金的動作。

[02:16:23] 那如何去申請這個 API 呢？我這邊有放一個連接，就你只要點進去後你就會進到這個 AI Studio 的 API Key 裡頭。然後你就點說你要 Get，你就 Create 一下 API Key。生成之後它就會可以叫你幫這個鑰匙做取名，就是說 Google API Key 之二好了，隨便先幫它取名。基本上它會讓你先幫鑰匙做命名的動作，然後它生成完之後呢，你就要 Copy 這一串。

[02:17:53] Copy 完之後再回到這個 Google Colab 上面，然後打開左邊有個鑰匙的東西。然後先任一個密鑰的部分把它打開。這個名字的部分之後會再會再講，我只是稍微做個簡介而已。這「值」的部分呢，就把剛才複製的東西貼上，然後「名稱」的部分基本上我們之後會有一個固定的名字，但這邊因為我們是隨便，我自己喜歡取名譬如說 Google...好，老師覺得叫 Gemini 就在好。

[02:19:35] 因為我們這課程之後會再做，先偷跑先講這一塊。那你幫這個鑰匙取完名之後呢，這邊就不能、就不會再做更動的動作。就你之後不管再怎麼做，這邊都不會再做更動。然後記得之後要跑這程式碼的時候要把這個東西打開。你不打開也可以，之後它會想辦法打開給他。那你之後就可以，如果你真的要跑的話，你有課金的話，你就可以直接跑我以下的內容的程式，這是可以直接跑的。這個遇到 Error 主要是因為我沒有課金。

[02:20:47] 那我第二個的要分享的是 Storm 的這個研究助理。這個是我有成功過的一個程式碼。首先因為他要先下載一些東西，所以先把它 Install 起來。那我前面看到你們看到這東西是我原本在嘗試的內容，這我等一下都不會跑，我直接跑第三個結果。因為它那時候原本設定的東西是用 You.com 這個平台的 API 以及 ChatGPT 的兩個 API 的功能，把它串在一起做使用。

[02:21:30] 然後這時候先按一下重新啟動，再跑一次。但是我個人是沒有 GPT 的那個...我自己是沒有在 GPT 那邊 OpenAI 那邊去做課金的動作，所以我並沒有使用 GPT 的原本它給的方式，我自己是用其他的 API 去的，然後成功的做出來原本要的結果。

[02:22:08] 那跑完上面之後呢，我到時候給大家的程序碼會把這兩中間這兩段去刪掉。就中間在做 Input 的，Input 這些你需要的、你所需要用的一些工具包裡頭把它 Import 進來，然後再把一些 API Key 去做設定的動作。然後在 Import 這裡它所需要的一些套件把它 Import 進來。

[02:22:43] 那我這邊去有去做一些設定，就是因為我是用免費的 API Key 去做生成的，所以呢在請求 API 的時候會有一些限制，有最大的 Token。我授予存取權限，就是我有一些 Key 沒開，所以我現在都給他授予存取權限。

[02:23:22] 就是我先有稍微檢查一下說我的 API 的金鑰設定是不是都已經設定完成。然後就如同我剛才所說的，就是有一些東西，因為我是用免費的帳號、我是用免費的 API Key，所以我在做一些設定的時候，我要去做限制。比如説一分鐘之內我只能請求多少次，那我最大的 Token 它可能說免費的免費版頂多一次就是頂多就 2 萬個，那我就要設定 2 萬個，不能有 4 萬個 Token 跑進去。對，那我這邊都有去做設定，然後是做協同的處理。

[02:23:59] 那這邊都會先讓它跑一下。這邊有些就是你請求太多次，我都會讓它去做擋掉的動作，就是說我不會讓它一次丟給他那麼大量的東西，讓它太平凡去做，因為我要保有我要用我免費的帳號。

[02:24:28] 然後中間如果你要放你要生成的主題的話，我是有寫說你的 Topic，在 Topic 這邊會寫說你會想要生成什麼樣的主題。

[02:24:44] 哎，他還在跑嗎？還在哦，這東西...這我之前跑是稍微花差不多三五分鐘內就會把它跑完。對，然後給大家稍微看一下結果。如果這些東西都執行完之後呢，你的右邊的 Output 裡頭會出現一個檔案，像這邊就會出現。我這邊有設定好是說就說這是我有設定的主題的東西就是 Generated AI 用在這個保險上面。然後底下呢這邊還沒跑完，它裡面會出現檔案，不一樣的檔案，然後再給大家看。

[02:25:29] 然後最後呢，你要怎麼把這個檔案把它載進來載到你的電腦裡頭？我有寫一個、寫一個程式嘛，就是說你把這個檔案壓縮，然後從這邊取、從這個路徑上取出來，然後你的檔案名稱叫什麼，然後它就會載進你的電腦裡頭。那最後載出來的結果是用 JSON，我是用 JSON 啊，然後載出來成果會是長這個樣子。

[02:26:00] 它會有會有很多不一樣的東西。比如說第一個就是你在發送請求的時候，你所詢問的對話，它就會把它記錄起來。他並沒有顏色的顯做區隔，所以我已看不太清。

[02:26:25] 然後第二個呢，他會講說他在生成這篇文章的時候，他用了哪些的...他生成了哪些的 Title、哪些的主題，它也都會寫出來。比如說像一開始 Introduction，然後它的 Background 是什麼、它的歷史是什麼，那它使用的 Application 是什麼，就它都列出來。

[02:26:43] 然後你在問的任何的 Prompt 它也都會再顯示出來，它也都會把它記錄起來。像它這邊就記錄非常的對、非常完整。然後最後也會顯示出再開一個 JSON 檔是說你的結果是長怎麼樣。只是它是因為是用 JSON 的檔案，然後它不是大家熟悉的在 Word 裡頭所出現的文字，所以看得有點痛苦。

[02:27:18] 然後這邊就裡面就是我自己的對...我我自己的一些設定就為一些 API Key 之類的。然後這個是一些簡短的整理，就是它所生成的一個文章寫給大家看。就是這是有整理的版本，但是原本剛才是 JSON 檔，然後這個是 TXT 檔。

[02:27:55] 再來最後一個是，然後這是他所參考的所有那個連接。然後這些連接我接的行也是可以打開的，對，他都有說他參考哪些連文章的連接所去生成的文章。

[02:28:16] OK，那今天也快到 7 點了，那再提醒一下禮拜中央大學學會有一個同學會做閃電秀的分享，佩真同學。然後大家如果要報名閃電秀的話，可以在 NTU COOL 上面可以報名，對不對？上面有連接，大家可以去報名。然後助教會再提醒大家什麼時候報、什麼時候上來分享。OK，那今天的課就到這邊，謝謝大家。
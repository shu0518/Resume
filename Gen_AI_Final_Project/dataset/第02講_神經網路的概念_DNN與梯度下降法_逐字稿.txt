[00:00:00] 大家好，非常感謝我們現場的同學，還是有很多同學到現場。那歡迎大家如果可以的話，就到現場來參加。我們今天就是生成式 AI 課程的第二節課，準備要為大家先複習或是預習，針對以前有沒有學過神經網絡的概念。神經網絡就是現在的 AI 最重要、最核心的一種技術，所以這是我們今天要學的。不管以前有沒有寫過程式，今天你就會手工打造一個你自己的神經網路。好，這個就是我們今天最重大的目標。

神經網路的核心概念：呆萌型 AI 機器人

[00:00:43] 所以我們就開始。那神經網絡呢，其實很簡單，也很容易，也不是什麼可怕的東西。它其實只是一個「呆萌型的 AI 機器人」（註：原音似 IG，依語境修正），這我們上次已經稍微說過了。雖然最正確的說法是它是一個「函數學習機」啦，那如果你覺得最正確的說法聽起來有點可怕，那你也可以想它就是一個 AI 模型。你要很清楚的知道輸入的東西是什麼，輸出的東西是什麼。你只要知道這件事情就好了。而且要知道這件事情，其實只有領域的專家可以知道，不是 AI 的專家，是領域專家會知道你到底要打造什麼樣的呆萌型 AI 機器人。

[00:01:23] 所以第三個、最後一個重點就是「呆萌型 AI 機器人」。那我們已經說過了，那呆萌之前基本上只有呆，他們有「萌」是可愛的說法，那只是它真的一實質上它只有「呆」。那就是他不知道他自己要做什麼，所以你要做什麼，一定要你告訴他。這個到我們生成式 AI 的時候，ChatGPT 的時候也是這個樣子。好，所以你一定要告訴他要做什麼，那沒有非常的困難，沒有非常的可怕。然後你要知道輸入是什麼，輸出是什麼。

數據化：電腦只認得數字

[00:01:55] 另外一個小重點，事實上是一個很重要的事情，輸入跟輸出都需要是「數據」，就是一堆的數字。電腦只認數字，他沒有辦法——比方說那個要八哥辨識，我們人是會辨識：這個是土八哥、這白尾八哥、這是家八哥。我們會跟別人講說這隻是什麼八哥，但是電腦不會。電腦不會，他只認數字。所以我們要讓他辨識這些，不管是辨識什麼，就給他一個數字代號。比方說土八哥是一號，白尾八哥是二號，家八哥三號等等的。就是這樣子，它只會處理數字。在大世界裡面只有數字。

資料的輸入形式：向量、矩陣與張量 (Tensor)

[00:02:42] 所以呢，輸入的形式跟輸出的形式，我們稍微簡單的說明一下。輸入一定要是一個數字或是一堆數字，輸出也是一個數字或是一堆數字。反正就是數字。那數字的格式有可能是像這個樣子，向量的形式。這有一個很有名的數據集叫做「鳶尾花 (Iris)」的辨識，就是要辨識三種的亞種。

[00:03:09] 但是在以前的機器學習的方法，你要輸入這麼多個我們叫做 Feature (特徵)。如果是深度學習的話，你想都不用想，輸入就是一張鳶尾花的照片，輸出再試看它是哪一個亞種的鳶尾花。但是以前機器學習沒有辦法輸入那麼多的 Feature、那麼多的特徵。好，就是像這邊就是有四個特徵，它比較能夠接受。那這四個特徵怎麼做出來？就是我們人想說，那鳶尾花到底每一個不同亞種有什麼樣特徵？可能花萼的長度跟寬度，還有花瓣的長度跟寬度，可以代表一個鳶尾花的亞種的特性。所以我們就輸入這四個數字。所以這四個數字是我們人為選出來的，那我們就會說這時候有四個特徵。那大家也知道這個就是向量的形式的數據。

[00:04:05] 那也有可能是矩陣的形式。比方說這個很多同學很有興趣，就是我現在想要做股價的預測。很多同學都很有興趣，我想要做股價的預測，那我可能就覺得說，我需要過去 20 天的數據全部輸給他，那讓 AI 判斷說下一天、下一次股市開市的時候——明天沒錯，下一天沒錯。好，因為有時候我在星期五的時候說下一天，就同學說老師明天沒有開市。好，那就是下一次會開市的時候。我們輸入前 20 天交易的情況，然後在下一天、下一次要開市的時候，我們預測它會漲還是會跌。

[00:04:50] 所以輸入的時候，它就有可能是像這樣矩陣的形式。我們把第一天的這個開盤價、收盤價、最高點、最低點、成交量等等的把它放進去，然後第二天的也放進去，第三天也放進去。就這樣子，所以它就會是一個矩陣的形式。所以那個資料有可能是矩陣的形式。

[00:05:11] 那有可能是立體的矩陣。那就是像這樣子，就是像一般的照片。照片進去的時候，比方說 256 乘 256 的照片放進去的時候，那個對一個很標準的做法就是把它變成三張矩陣。好，因為再一次哦，就是輸入只能是一堆數字，輸出也只能是一堆數字。所以你要把所有的東西可以換成數字的，你才可以讓 AI 去學。所以一定要換成數字的樣子。

[00:05:46] 那照片可以放成數字也是很簡單的事情。那一個標準的做法就換成 RGB 三種三原色的矩陣。第一張矩陣就是例如是 256 乘 256 的大小的話，就 256 乘 256 大小的矩陣，然後每一個點的紅色的強度。那第二張矩陣是綠色的強度，第三張矩陣是藍色的強度。所以這個時候它的資料形態又比矩陣複雜一點點，我們姑且叫它 3D 矩陣好了。

[00:06:15] 你就會發現從前面到後面的時候，第一種資料格式是向量。再進一階的時候，矩陣的時候，你會發現每一列其實都是一組向量。對吧？就每一列它都是一組向量。這個是第一列它的一個向量，第二列又一個向量。所以就是一堆向量的集合。然後又下一種的時候就是三張矩陣，三張矩陣就三個矩陣的集合。就是這樣，所以你就會發現每進一階就是前一種的那那個集合，就一串一串前一種那種元素。

[00:06:55] 然後，所以我們當然可以在下一種的就是 3D 矩陣，每一個都是 3D 矩陣的那種東西，就一串的 3D 矩陣。我們應該叫什麼？我們就會發現沒詞，沒有詞。那沒有詞怎麼辦？那數學家很聰明，他都是會想出一堆漂亮的詞給大家。所以說沒有關係，這個以後我們就給他一般化的詞。就是呢，如果說輸入只是一個數字或輸出只一個數字，這種東西我們只是單純的數字，我們叫它「純量 (Scalar)」。這樣可以吧？這聽起來比較有學問嘛，就是最白痴的數字。那這個說出來這個人家不相信我們在學術的殿堂研究高級的東西，所以我們就告訴人家是純量。

[00:07:59] 然後再來下一種的就是一堆的純量，這種的我們已經以前就學過就是向量。然後再接下來就是一堆的向量合起來我們就叫做矩陣。然後一堆的矩陣我們現在已經學會了，我們要給它一個一般型的名字。所以我們純量就把它叫做最低階的，叫零階的 Tensor，就「張量」。但是你不要想物理學的張量跟這個張量有什麼關係，不用去有關聯性，不用想有什麼關係，就想沒有關係就好了。這個你問 ChatGPT 哎喲有關係，不要管它。好，就是零階的 Tensor。那向量的話我們就可以叫一階的 Tensor。你就會發現這個很簡單，因為再來再複雜一點點，就矩陣就變成二階的 Tensor，然後再變成三階 Tensor、四階 Tensor。好，就這樣子結束。

[00:08:50] 所以未來你會看到的數據機有沒有可能變得更複雜？當然有可能。但是它就是某一階的 Tensor，就這樣好就結束了。所以如果你在外面很討厭一個人，你要跟他說解釋什麼是 AI，那最簡單的、不是最簡單，解釋最精確、但是最噁心的解釋方法就是跟他這樣說：「所謂 AI 呢，就是我們觀察真實世界，我們想要能夠有預測的東西。然後呢，我們就把這個想要預測的東西把它畫成一個函數的形式。然後我們就打造一個函數學習機。那這個函數要注意哦，它的輸入一定就它定義域一定是一堆的 Tensor，就某一個維度的 Tensor，那值域也是某一個維度的 Tensor，就這樣。然後我們就打造一個函數學習機，用過去的訓練資料把這一個函數給學起來。」就是這樣，完畢。然後他完全不知道你在講什麼。

[00:09:51] 但是希望說我們上過課的同學都知道是剛剛在講什麼。剛剛說的話白話文就是：我們就打造一個 AI 人、呆萌 AI 機器人。然後你要很清楚輸入是什麼、輸出是什麼。唯一的限制就是輸入跟輸出都是一個數字或是一堆的數字。可以是向量也好，矩陣也好，都可以，反正就是一堆數字。輸出也是一堆數字。然後我們就用過去的訓練資料去訓練它，然後學會了，AI 機器人就可以幫我們預測他沒有看過的東西。比方說這張照片、這張八哥的照片他沒有看過，但他可以正確預測這是什麼八哥。所以這個就是 AI 的全部的要說的話。

領域專家的重要性

[00:10:32] 剛剛那件事情其實真的很簡單啊。所以我本來是覺得我們出的那個問題應該讓大家做發想的。就是你去想說你在你的生命當中觀察什麼是你覺得這個是真的可以打造一個呆萌的 AI 機器人去訓練他，讓他學會的，然後對我是有幫助。就是比方說我要預測這個天氣也可以，我要預測這個 iPhone 的價格也可以，我要預測股價也可以。反正先不要想說這個到底可不可能做得出來。但是呢就是我們想成說，我如果把我的問題畫成一個呆萌型，我很清楚輸入是什麼，輸出是什麼，我可以找到很多很多的訓練資料去練它。在這樣的情境之下，我是不是可以就訓練出一個不錯的模型去做這樣的事情。

[00:11:31] 然後你再更深入的就會發現說，要知道要打造什麼樣模型機器人，只有領域專家才可以。也也就是未來的 AI 世界，其實領域專家就是大家自己學的不管哪一個專業會越來越重要，不會越來越不重要。但是它的深度可能會需要加深。因為在未來你會知道，你要第一個你要建造什麼樣的模型，你一定要是領域專家才知道建什麼樣模型對我們的實務是有幫助的。這第一個。第二個呢，未來你會發現這個 ChatGPT 非常會吹。那你要知道 ChatGPT 什麼時候在吹牛，什麼時候沒有吹牛、他說的是對的。那你需要有一個比確率更高的品味，那也就是你要了解的深度需要更深，你才知道它什麼時候在吹牛，什麼時候不是。

[00:12:19] 好，那我們現在再回來。所以我們今天要打造的 AI 機器人就是一定要把所有的東西都要可以換成數字，我才有辦法當成它的輸入。然後還有它的輸出。那剛剛有發現說這個股價啦，本來就是數字的那些東西，當然就是數字。那但不就之後大家也會發現，那不是沒有不久，就是其實我們不會認真的講，但是相信大家可以理解，那個聲音也可以把它轉成數字的形式。最簡單的想法就是它就是一個波形嘛，然後那個你就可以用數字把它記下來，說它每一個時間點它的波的那個高度是多少。然後它就可以把那個畫出來，所以它也可以把它變成數字的形式。那文字也是可以變成數字的形式。

獨熱編碼 (One-hot Encoding) 與機率分佈

[00:13:06] 好了，那我們就回來我們的八哥辨識的例子齁。所以我今天呢，就是在野外看到，我今天想要知道說，我今天看到說發現說台灣常常見到八哥有三種。那我們可能會認真的想，就是說那我今天要怎麼樣打造模型機器人，幫我可以辨識台灣的三種八哥。那那個所以這個時候呢，我們會知道輸入的時候一定是一張八哥的照片，這樣可以吧？輸出就是三種八哥。

[00:13:36] 但是我要再度的強調，他不是人類，他沒有辦法這個知道說學一學就說：「哦，這一隻是土八哥，這家八哥，這白尾八哥」。沒有辦法，沒辦法。他能夠做的事情就是我們給三種八哥都一個編號，叫他一號八哥、二號八哥、三號八哥。就給他一個編號了。所以一號就是他的名字，二號就是他名字，三號就是他名字。那一號是誰？隨便你高興。就是隨便高興，就是一號。比方說一號是土八哥，二號白尾八哥，三號家八哥。這個是隨你高興的，這隨你要怎麼樣編都可以。

[00:14:12] 那所以第一，我們就非常公平的給它每一個、每一個輸出的時候要辨識三種的時候，每一個都會給他一個分數。就輸出就是三個。因為了種種的原因啦，那如果大家真的很有興趣，那為什麼不能只輸出一個數字？那他輸出土八哥就輸出一號 1，那白尾八哥輸出 2，家八哥輸出 3？這個很有興趣知道，我們可以有機會可以討論一下。但是在總而言之，在這個 AI 的世界裡面，深度學習裡面，我們要辨識三種，我們就會給他三個分數。

[00:14:48] 然後呢，如果學成了之後呢，我們就要告訴他正確答案嘛。因為他沒有辦法憑空去學習，所以我們需要讓他知道正確答案。那比方說這一隻是土八哥，這張照片的土八哥。那我們怎麼樣告訴他是土八哥呢？那我們就給土八哥最高分，那一分。那白尾八哥，不是白尾八哥，所以我們就給他零分；家八哥給他零分。就是這樣。那那個白尾八哥的時候呢，就是 0 分、一分、零分，所以就 0, 1, 0。然後家八哥就是 0, 0, 1。就是這樣，這個應該可以理解。那這種、這種編碼的技術，我們把它叫做 One-hot Encoding，只有一個地方是 1，其他都是 0，叫 One-hot Encoding。

[00:15:29] 那你今天可能會有一個疑問，這個疑問就是說：老師幹嘛要那麼小氣呢？他是土八哥，我們就給他最高分嘛。那為什麼給他 100 分呢？呃，他因為這張照片是土八哥的話，我們是正確答案就告訴他對土八哥得 100 分，白尾八哥零分，家八哥零分，這樣也很——為什麼不是 100 分呢？沒有其他特別的理由，因為我們特別特別喜歡三個數字加起來等於 1。因為三個數字加起來等於 1，我們對外面說話的時候就說它是一個「機率分佈」。你看是不是很高級的感覺？就是機率啊，機率啊。那機率加起來要變是 1 嘛。那就這樣子。

[00:16:08] 那所以我們特別喜歡那個最高分給它一分。好，就這樣子沒有差哦。你如果堅持說你的模型一定要給正確答案就是最高分是 100 分，可不可以？可以啊，可以啊。那可不可以就這樣做？可以啊。可以。所以說這個完全是因為我們高興，我們快樂，所以把它叫做 1, 0, 0；0, 1, 0；0, 0, 1。

訓練與測試：避免「背答案」 (Overfitting)

[00:16:29] 好，所以我們就會收集很多的數據。他沒有辦法自己學，他不會那麼厲害，說今天請你自己上 Google 上網搜尋看看家八哥、土八哥、白尾八哥長什麼樣子，然後你自己去學一學，會認了以後告訴我，然後我們就可以開始測試你。沒有沒有。你有準備好資料給——記得，呆萌型機器哪有那麼聰明還自己上網去搜尋。沒有，他就是你要準備好給他。那你就準備很多八哥的照片。

[00:17:01] 好，第一張這個通常我們輸入都叫做 X，輸出都叫 Y。這跟我們以前小時候的函數寫的習慣是一樣的。那第一筆資料我們就叫做 X1，就是一張八哥照片。這一張是那個土八哥，然後所以輸出的時候我要告訴他這個正確答案就是 1, 0, 0 這樣子。好，然後那一此類推，白尾八哥就是 0, 1, 0，家八哥就是 0, 0, 1。好就這樣子結束了。

[00:17:28] 好，然後呢，我們就可以準備要去訓練它了。那事實上準備這些訓練資料就是第一個是一張八哥的照片，輸出就是 100、010、001 中間的一種。可是呢，電腦這個很神奇的一件事情就是電腦他跟我們人有時候真的還蠻像的。比方說大家如果你念到不太會的東西的時候，你都怎麼處理？我怎麼好像聽到有人說問 ChatGPT，對，現在好像可以做這種事情。那以前不行的話怎麼辦？那有時候我們就把它直接把它背下來。就這樣。

[00:18:05] 那如果今天是這個測驗的題目，那你就是都把它背下來。所以這個老師考的時候，因為其實你發現某一位老師都是某一本參考書的，所以你就覺得哦這個沒問題，你就把所有的正確答案全部背下來了。所以考試在平常考試都是 100 分。然後但是很不幸的呢，在期中考就是段考的時候呢，他要考考你的時候呢，就完全換了題目就換了。但是因為你只會背那個背以前看過的那些題目，所以你那個真正考的時候，你因為不是真的懂，你只是把答案背下來，所以那個在真正考試的時候就會變得很差。

[00:18:46] 好，那這個 AI 也是這個樣子，有可能發生這種事情。他用各種卑劣的手段，我們不知道的手段，他把答案背下來了。他比方說土八哥，他發現說土八哥在這個腳角邊角都會出現一棵樹枝，例如剛剛好。那所以他就記得了，所以他以為邊角角出現一棵樹就是土八哥這樣。還有所以所以說他都會亂來。

[00:19:13] 所以在這樣的情況下，我們怎麼知道他有沒有在背答案？其實方法很簡單，就是我們所有準備好的資料裡面，我們就準備一些暗崁（台語：私藏）下來當做測試資料，然後其他的才去訓練它。就這樣子，就有一些留下來當測試就要考它的。好，所以說呢，就是我們訓練完了以後，用訓練資料訓練完，那測試條是他從來沒有看過的題目，那他也做得不錯，那我們就相信說他真的可以了。就這樣。

[00:19:41] 那這種背答案的其實就叫做「過度擬合 (Overfitting)」。就太為了這個答案，然後那個 Overfitting 就是過度擬合的情況。好，這背答案情況。好，我一定要說一件事，一句話這個一般的同學沒有什麼問題，但是有一些高級的同學，就是從小受到的養好的 AI 教育的同學，這個呢常常會以為只要訓練的時候正確率太高就叫過度擬合。不對。正確率太高有人他說不定真的資質很好，考試能力很強啊，你怎麼可以這樣子否定那個模型機器人呢？所以那個不是正確率高就是過度擬合。要「測試正確率高，但是測試資料很低」，這種才叫做過度擬合。這樣子可以？就他真的背答案，因為他沒有辦法應用在那個真實的狀況。那如果訓練資料也很高，測試資料也很高，這是我們最喜歡的情境，所以那種情境不會叫過度擬合。好，這順便提醒的一下事情。

監督式學習 (Supervised Learning)

[00:20:46] 好了，然後呢，我們剛剛做的方法就是——不是剛剛做，我們其實沒有真的做，我們今天是要真的做啦——但是我們剛剛說的方法呢，叫做「監督式的學習」。監督式學習就是我們準備好教材給他然後去做監督式學習。然後你一定會覺得那當然一定是這樣子做的，因為因為他是呆萌型機器，他其實不知道、他根本沒有辦法自己去學習，那所以說當然是監督式學習。

[00:21:15] 有可能非監督式學習，這個其實是很有意思的主題，這個我們在後面的課程其實會碰到非監督式的學習。現在想起來有點懸疑，因為我們自己都不知道答案，所以我們才要去讓他自己看，完完全就讓他自己去學。這種情境真的可能嗎？這個其實是要設計。好，這個我們未來會說到。總而言之，在開始的時候大部分的情境我們看的都是監督式的學習，就是我們準備好資料，然後去把它就是去訓練它。好，那訓練成功了以後大概就長這個樣：我輸入一張那個他沒有看過的八哥的照片輸進去，然後訓練的好的話就是他會給正確答案最高分。像類似像這樣，這只是一隻土八哥的照片，然後進去了以後呢，就給他 1.9, 1.1, 0.2。然後我們就會說這個準，因為他真的給了最高分是給土八哥這樣。這樣可以嗎？其實就這樣子，沒有了。這個 AI 就是做這樣的事情。

Softmax 函數：將分數轉為機率

[00:22:15] 好，那當然大家應該還要有問題啊。這個沒有沒有問題，這個其實其實這個就是這樣用的。然後但是唯一的小缺點，這看起來沒有什麼那個高級的地方。因為 1.9, 1.1, 0.2 你也沒有辦法跟人家解釋 AI 到底在想什麼。什麼叫做 1.9, 1.1, 0.2？他為什麼人家就會問，不小心他看到那個 AI 本來吐出來的答案，他會問那白尾八哥為什麼給 1.1 分呢？家八哥給 0.2 分到什麼意思呢？是到底什麼意思呢？這個然後這樣子，所以我們說不清楚。

[00:22:45] 所以我們大家就會做一個很奸詐的事情，就讓三個數字加起來變成——這樣可以嗎？——加起來變成 1。但是還有條件，有一個條件呢，就是這三個數字加起來變成 1 之外，大小關係要維持。就是不能分數高的變分數低的，分數低的變分數高的。就這好。那大家有沒有想得出來三個數字要怎麼讓它加起來變成 1？這個這個真的太幼稚園的題目了。1.9, 1.1, 0.2 請問怎麼樣三個數字加起來變成 1？答案就是非常的簡單，就看 1.9 佔總和了多少，然後 1.1 佔總和了多少，0.2 佔總和多少。

[00:23:21] 現在不是麥克風有問題，是我真的講的很小聲，因為實在太衰敗（簡單）了，我不太好意思從我的嘴巴裡說出來。然後所以我們看 1.9 總和的比例，1.1 佔總和的比例我們就知道說這個這個然後就它佔總和多少嘛，那三個數字當然加起來就是 1 嘛。這樣可以吧？

[00:23:43] 那加起來變成 1 有什麼好處呢？我們來看下面的。所以呢，我們就發現說假設比方說經過轉換之後呢，三個數字加起來變成 1，就是 0.61, 0.28, 0.11，三個數字加起來是 1。那我們在外面說話的時候就很小聲——很大聲說：「哦，我們的神經網絡判斷這一隻八哥，這一隻呢，61% 的機會是土八哥，28% 的機會是白尾八哥，11% 的機會是家八哥。」

[00:24:13] 你有沒有覺得你在外面說出這樣的話，人家都會對你投以崇拜的眼神？因為就覺得說：「哇，你的 AI 怎麼這麼厲害，還算得出機率來！」就這樣。那我要非常鄭重地說一件事情，這個剛就是我們硬是把它加起來變成 1 的嘛。所以那個加起來變成 1 的動作完全是我們做的，我們的 AI 去做這種事情，它是不是根據真實的狀況做這種事？某種程度也是因為他根據真實的狀況去學習，但是那個那個比例不一定是真實的比例。就這樣。

[00:24:43] 所以所以說呢，但是人家都覺得很高級。然後你就說對沒錯，那個 AI 就是學出一個機率分佈來。你看聽了以後人家什麼話也不會問了，就完全滿意了。說老闆如果是人家委託你做了一個 AI 的模型，人家就完全滿意了說好很好這樣。而且打得要夠正確了，不能不能不能正確率很低。正確率也不錯，那人家可能就覺得這個好厲害，這不知道怎麼做出來的，這個我們一輩子可能也弄不出來這樣。沒有，它就是我們人為加起來等於一的。

[00:25:11] 這是非常重要的概念，非常重要。加起來等於 1 這件事情是非常重要的事情，但是很少很少地方告訴你說這個加起來等於 1 是很重要的事情。好，現在大家知道了，這個最大的秘密就加起來等於 1，這個是我們人自己算出來的。就這樣。

[00:25:27] 然後但是如果剛剛有同學在這邊認真的算了以後，你會發現老師你又騙——這個其實我們在學生程式 AI 的時候，你要對每一句話都懷疑，這是非常好的習慣。特別是跟 ChatGPT 溝通的時候，一定要懷疑他說的話。那那個在上課你也可以懷疑老師說的話。因為這邊的確是不是這剛剛有一點點欺騙你，因為你就會發現 1.9 根本不是佔他們總和的 61%。這樣佔多少？佔多少助教算 1.9 佔三個數字的好 0.59 好，0.59。那 1.1 佔總和多少？這個後悔回答了說這個這個我剛剛只是只是好心回答了一個 0.334。好，你後大家有沒有發現說這個答案不是這個，就這樣，答案不是這個。不是正確答案不是這個。

[00:26:31] 那為什麼呢？那是因為因為呢這個出現的數字有可能是小於零的數字。就是大於零的時候，我們剛剛那個加起來就是直接看佔比例，那是很合理的。但是有時候那個我們不能控制 AI，那有時候你設計的模型就真的有可能吐出小於零的數字。他就覺得太不可能了，他就突出一個負 200 的數字出來，也有可能就是就不可能嘛，就不可能是這個類別，所以它就負分。有可能是負分，所以在這樣的情境之下呢，就會發現說這個加起來根本不是 1。好，那注意這個時候怎麼辦？好，來請那個送黑板。

[00:27:11] 好，然後加起來變成 1 的時候呢，它就是等一下哦。好，所以我們現在本來有三個數字，A, B, C。好像回到幼稚園時代的感覺，幼稚園好像還沒有 ABC。好吧，那 ABC 呢，那今天有三個數字，我取另外的數字哦，因為剛剛說有負的話不知道怎麼辦嘛。然後等於那個 0.6, 0.8 好了，然後這個等於負 3 好了。這個完全不可能啊，他覺得這個你不要騙我，不可能。然後就負 3 這樣子。

[00:28:00] 你就發現第一件事情我們需要讓三個數字變成都是大於零的數字，我才可以繼續做下去。這樣可以吧？大於零的數字。然後但是呢，但是呢，這個要把它變成大於零的這個動作需要是對於任何的數字都是做一樣的動作。因為我知道有同學很聰明會想到平移就好。不可以。因為平移有時候要平移，這邊可能要平移 3.1 啦，3.1 比較保險，讓大家都大於零。那但是換一組數字的話，平移的程度是不一樣的。好，這樣子大家不一致，有點不公平的感覺。所以我們還要找一致的方法。

[00:28:42] 那請問怎麼辦？就如果你怎麼樣做一個動作，然後讓它這個數字都是大於零，然後而且大的還是大的，小的還是小的。就這樣。怎麼樣？可以加絕對值嗎？不可以的原因是因為加絕對值 -3 變最高分，本來最低分變最高分。他不能加絕對值。這樣來三個數字變成大於零的數字，其實沒有我們想像中間的困難。

[00:29:09] 來這個順便說一下，這個對數學非常擔心，就從小最痛恨的就是數學，看到數學老師猶如見到仇人的感覺的同學們不要擔心。因為我們是應用數學系開的課程，所以偶爾還是要負責任的告訴你後面的公式的由來是什麼。然後但是不用太擔心的是你最後只要有個感覺，知道說我們到底做了什麼事情為什麼要做那件事情就結了。好，就這樣。

[00:29:36] 好，然後那三個數字要變成大於零的數字，其實在所有的數學裡面大家學到一個小技巧，就是呢就會說我們要做一個轉換。那在數學裡面做一個轉換意思就是把一個函數，找一個函數把它帶進去。這樣可以。那所以我們就開始換——不是幻想——回想我們從小到大學過的函數。那從小到學過函數，比方說多項式函數可不可以？就帶進去了以後就是大家都大於 0。然後即使是懸吊的多項——比方說拋物線，我們小時候最喜歡的多項式函數拋物線。我拋物線好的拋的話可以都大於 0 沒錯，但是但是有一個地方沒有符合。什麼地方沒有符合？就大的還是要大的，小的還是要小的。然後你認真的聽的話就是大的還是要小的還是要小的就還是要小的（口誤：意指遞增）。意思就是說大的數字帶進去會就會比較大，小的數字帶進去會比較小。意思就是說它是遞增可以嗎？所以我們就慢慢的去掉了以後，就會發現這個多項式是不可以的，對數函數也是不可以的，那個三角函數更不可以，在上下震盪的，不可以。

[00:30:51] 然後只留下了一個活口，就是誰？就指數函數啊。指數函數帶進去全部都大於零，自然的。然後那個然後那個因為它是遞增函數就是一直往上衝。順便說一下，那個是我們我們本是我們最喜歡的，我們最喜歡的那個就是如果指數函數是——如果我們的複利是指數函數的話——我們是不是複利，複利。所以是指數函數的話，就是我們的利息是指數函數計算的話，我們很喜歡，因為它越往上升越快。或是我們薪水是指數函數成長，我們蠻喜歡。所以我本來要說是我們最喜歡的指數函數，但是後來又想到有一些悲傷的事情也是指數函數話，我們就不太喜歡。比方說我們借借借貸款的時候，它就是用指數函數算我們的那個貸款的利率，我們就不是那麼喜歡。好等，好，所以就指數函數啊。總而言之總就算指數函數，所以就經過指數函數的轉換。

[00:31:47] 那指數函數在大學裡面最喜歡的指數函數就是 $e$ 啊。對 $e$ 這種指數函數就是 $e$ 帶度 A 然後帶 B 帶 C。然後所以我們就換了三個數字：$A'$, $B'$, $C'$。這個寫的好醜 $A'$，那都大於 0 了。所以我所以我們就開始可以做原來的那種件事情，就是先把它總和算起來。就是 $A'$ 加 $B'$ 加 $C'$ 三個大於零的數字加起來。然後呢，所以我們的第一個、第一個輸出的 $P_1$ 呢，就是就是 $A'$ 除以 $S$。對，$A'$ 除以 $S$。好然後 $P_2$ 呢就是 $B'$ 除以 $S$...... 這就是剛剛的三個都大於零的時候，我們就這樣做的。$P_3$ 呢就是 $C'$ 除以 $S$。

[00:32:39] 好，大家不要小看這件事情，這件事情是非常重要的。這個重要的點在哪裡？重要的點就是是不是一定要這樣做？答案是不一定。不一定可以切換成那個投影片，不一定要這樣做。因為這個是我們覺得一個自然的做法，就這樣子。然後它是不是真實世界就是這樣子算？當然不是，當然不是。這個是我們強迫要讓它把它加起來變成一的動作。那這個動作就叫做 Softmax，就是 Softmax。就是先加先把它經過指數轉速轉換，全部都大於零，然後讓它加起來變成 1。就這樣。

[00:33:18] 然後這個動作 Softmax 這件事情跟 AI 的有一些細節，特別是生成式 AI 有一些細節非常有關係。在八哥沒有沒有沒有沒有什麼關係，因為因為我們剛剛說過第一名還是第一名，第二名還是第二名，第三名還是第二——第三名。所以對於這種我們只是要選第一名的那種，因為這個辨識出來的話，它不可能是同時是三種鳥嘛，三種八哥，所以我們只要選第一名就好了，只有一種是有可能的，我們就選第一名，最高分的那個就可以。那但是有一些情況像生成式未來我們會學到不一定是這個樣子。

[00:33:56] 好，為什麼團變成黑暗的世界哦，來了。好，好了。所以呢，所以呢，剛剛就說到我們經過了這種殘忍的加法了之後，我們就會發現...（處理黑屏）...好，那反正我們繼續說。就是剛剛大家看到的就是它會經過了 Softmax 之後它就會變成加起來等於一。然後所以那個我們就會用自己的手段讓它加起來變成一。我一定要一直強調這件事情，它是我們自己讓他加起來變成一的。這人為的加讓它加起來變成一，只是很合理的維持了大小的關係，但是他還是人工的動作。這件事情在我們生成式裡面會變成一個非常大的主題。在這個在這個八哥辨識的時候完完全全只是嚇人的主題。

[00:34:55] 因為我們會在外面的時候聽到，特別是很多的專家——然後順便跟大家修課的同學們說一下，你出去外面的時候，特別是碰你討厭的人也是這樣說的。就說那個 AI 是做什麼，AI 就是學出一個「機率分佈」出來。所以你今天學好了以後，訓練好了以後，那個輸入了一張就是他們有看過的，例如說在八哥辨識的時候，八哥辨識，然後輸出它就會給你一個機率分佈出來。所以在很多人在呆萌型的 AI 機器人就很喜歡寫成 $P$ 的，就 $P$ 就是學一個學機率的感覺嘛。所以大家就會嚇死了，就不知道在說什麼。好就是這樣子。好，那所以雖然這個機率這件事情在這個生成 AI 的時候會變得非常重要，但是我們還是要記得這個機率分佈是我們自己把它弄出來的。就這樣子。

[00:35:44] 好好啦好啦，這個這個再更嚇人的做法我們就不要做了。好，然後呢，我們來解釋一下下為什麼呆萌機器人，我們叫他呆萌心機器人。第一第一件事情就是他沒有辦法自己去學習，所以你沒有辦法告訴他說麻煩你就是去學八哥辨識，我要最後要希望打造一個模型可以學出三種八哥，就是土八哥、家八哥、白尾八哥，你就自己去 Google 搜尋一下，自己學一下這樣子。好，不行，沒有辦法這樣做。你一定要很清楚的告訴他輸入是什麼，輸出是什麼。

[00:36:14] 然後呢，這個件是還沒有結束。因為呢，我們在訓練他的時候，完完全全只是輸入一張土八哥的照片，就是告訴他 1, 0, 0；就告訴他這是土八哥啦，而且是告訴他代號 1, 0, 0。然後白尾八哥的照片輸進去，他就告訴他 0, 1, 0。就是這樣子。沒有要教他的意思。這個是重點，我們沒有要教他意思，要完全讓他自己看著辦。所以我們需要給他很多很多的訓練資料，不能只給他一張。因為我們不是那種手把手的教學，告訴他說在這個土八哥就看起來像一幅八哥一樣的鳥，這個在這在說什麼，然後就是看他的嘴的顏色哦，是象牙白就白色的，這種就是土八哥；黃色的話就是白尾八哥；家八哥更特別，它就是這種就家八哥，看起來像戴眼罩的樣子。沒有，我們沒有要手把手的教他，要讓他自己看著辦。自己看著辦他就需要需要很多很多的訓練資料。

[00:37:10] 那這個要多少的訓練資料呢？根據大家的經驗，差不多是每一個類別大概要給他 1000 張的照片，1000 張以上大概才足夠。我就這樣子。可能我們沒有什麼感覺，我就用一個比喻。今天呢，你到了比方說上了大學，大二的同學。好，大二的同學大二的同學呢，那個即使有一個課，就是你每一次每一天你上的某一個課，每一天都是有一位同學，每一天就是每某一位同學你每天都會碰到他。好就這樣子啊。然後呢，因為因為你才大二，還沒有見這位同學還沒有見過 1000 次，所以你沒有辦法記得他的名——有這麼蠢的事情嗎？所以我們的智慧如果是跟 AI 一樣的話，就是這副德性。就你見了同學一千次，你還沒有機會記得他的名字。然後可能要到大三大四的之後，終於看了一千次，總算記得了：原來你就叫這個名字。這實在太蠢了。

[00:38:12] 這是他為什麼現在 AI 其實只是一個呆萌型 AI 機器人第一個原因。第二個第二個原因呢，就是他真的不知道他在做什麼。那不要看生成式 AI 好像沒有，他很厲害，他會翻譯又會幹嘛又會什麼。他不知道。他不知道。他只是覺得前面出入這個的話，他後面比方說的文字的話就是後面要接幾號字。就我們也是把所有的文字都給他一個編號，就後面要接幾號字。如比如比如他覺得這個時候應該要接 87 號字，他不要順，他就給 87 號最高分。所以他就可能會說要接 87 號字。這個時候要接 54 號字比較順，他就他就說這個給 54 號字最高分，他就說要接 54 號字。就是這樣子。但他不知道他在做什麼，他甚至不知道他在生成文字，他也不知道每一個每一個代號到底是什麼意思。他不知道。他就只知道根據訓練資料來，他這個時候應該要輸出什麼。就是這樣。好，所以這個這一個就是為什麼它是呆萌型 AI 人的重要的原因。

神經網路的架構原理

[00:39:10] 好了，然後我解釋完了之後，我們要進入一個我們今天的重頭戲。就要跟說明一下下這個神經網路它到底是怎麼樣子的、原理是什麼。然後它是怎麼樣計算的。然後知道了以後大家就不會害怕，因為它基本上就是一個很簡單、某種程度它是一個很簡單的架構。

[00:39:33] 好，我們來看一下。好，在現在的 AI 模型呢，就是就是大紅。現在 AI 大紅的一個原因，重要的原因就是有一個叫做「深度學習」的技術。那深度學習的核心就神經網絡。這句話等一下我們就會推翻這句話。好，等一下再推翻了。那不管怎麼樣，我們要做的事情就是先把我想好我們自己要的呆萌型 AI 機器人是什麼樣的呆萌機器人。也就是輸入非常非常的清楚，輸出也非常非常的清楚。清楚這種型 AI 機器人。

[00:40:03] 然後我們要做的事情就是把這個「隱藏層」一個一個隱藏層給建構出來。在一個神經網絡裡面，呆萌型人輸入的地方那邊叫做「輸入層」，輸出的地方叫做「輸出層」。我覺得我在講廢話的感覺。那中間還沒有看到，就是我們要打造的地方叫做「隱藏層」。它是一層一層打造出來。

[00:40:27] 那大家可能會覺得說這個是不是要經過一個深度學習的課程，大概要經過一年之後我才能夠做這件事情。沒有，我們今天就可以做這件事情了。因為一層一層的隱藏層，雖然雖然那個你可以用各種不同的設計方式，但是基本的設計方式只有三種。

三種神經網路架構：DNN, CNN, RNN

[00:40:46] 好，就基本上只有三種設計方式。

第一種叫做「全連接 (Fully Connected)」，就是我們今天會介紹的，就標準的神經網絡，全連接神經網絡 (DNN)。在上個世紀的時候大概流行的就是這種神經網絡。

然後那個第二種神經網絡 CNN，就圖形辨識很強的。比方說我們做八哥辨識，還有今天我們要做的主題。老師說應該要用 CNN 做比較合理啦，但是我們管他的。為了讓大家知道神經網路真的很簡單，那個沒有什麼特別了不起的地方就是這個樣子，所以我們今天會用全連接來做這件事情。好，就圖形辨識很強，就圖形辨識一般要做圖形辨識，像八哥辨識那種情境，我們可能就要用 CNN 這種神經網路。

第三種是 RNN，這個有記憶的神經網絡。這個跟我們的文字生成 AI 有關係。不久之後我們會知道文字生成的 AI 其實很簡單，他每一次都只看一個字，他每次都只他是一個字一個字讀的。那我們不希望他讀到第五個字的時候，前四個字全忘了；讀到第 20 個字，前面 19 個字全忘了。我們希望他有記憶，他記得前面發生了什麼事情。所以他讀了一個字，一個字讀，他每一個字都會記得這樣子。就在這樣的情形就是有記憶型的神經網是 RNN。

[00:41:57] 所以基本上只有三種，雖然我們到目前為止一種都沒有介紹，但是大家應該都有一點點概念了。反正就三種。但是我們要設計一個隱藏層，就有三種的設計方式。一個是 DNN，一個 CNN，一個 RNN。事實上還有一個叫做 Transformer，可以當成第四種。它跟 RNN 比較有關係。這個我們在介紹生成式 AI 的時候為大家介紹。特別是 Transformer 我們會詳細的介紹它是它的原理、它的運算方式到底是怎麼做的。

[00:42:27] 好，所以就基本上就三種。所以你每次來設計一個隱藏層的時候，你就從這三種去選擇過來。今天我們為了簡化，只有唯一的需要選擇的就是全連接、全連接。
[00:42:42] 那這三種（DNN, CNN, RNN）呢，每一種不管設計哪一種，我們就會放上若干個神經元。那我們叫神經元當然是完全是擬人化的叫法，就是為了讓大家覺得這個神經網絡就是很像人類的神經網路。其實在電腦的世界裡面，它就只是一個 AI 神經網路的「計算單元」而已，就是一個基本的計算單元。那每一個神經元的計算方式是一樣的。所以呢，我們在做的時候其實沒有那麼在意它到底是怎麼算的。應該是說不用那麼在意啦，反正就是好像有一個小小的計算器。我們只要決定我要放幾個計算器上去給它就好了，就這麼簡單。等一下我們會知道神經網路的那個運算，因為它每一個神經元的運算方式基本上是一樣的運算方式。

[00:43:40] 我們來看，比方說呢，這一個可愛的神經元呢，就每一個神經元基本上長這樣。就是它會接若干個輸入，像這個神經元接了三個輸入。但是它最後只要決定一個數字吐出去就好了。可以吧？只要一個數字吐出去。

[00:43:57] 所以呢，它像這邊有三個輸入，這個第一個是 $X_1$，第二個是 $X_2$，第三個是 $X_3$，三個輸入。那每一個輸入呢，跟人類的神經元一樣，就是我們希望說那個比較重要的，我給他比較高的權重，就是比較看重他；比較不重要的就不要那麼聽他的話。在電腦裡面很容易呈現這件事情，就是 $X_1$ 這一個輸入呢，它就會連到這個連接上面會有個權重叫 $W_1$。所以 $X_1$ 如果是比較重要的，這個第一個輸入是比較重要的輸入，我就給它比較高的權重。第二個輸入呢，如果比較不重要就給它比較低的權重。第三個輸入看它的重要性，我就給它某一個權重叫 $W_3$。那這 $W_1, W_2, W_3$ 都是學來的，這個就是神經網絡的參數。可以嗎？這個是學來的，所以我們也不用擔心它到底應該是多少，這是我們要學來的。

[00:44:51] 然後所以呢，很自然的「總刺激」，就是總共我們要吐出來的數字，很自然的我們就可以想說，很合理的應該就是 $W_1$ 乘上 $X_1$ 加 $W_2$ 乘上 $X_2$ 加 $W_3$ 乘上 $X_3$。

線性與非線性問題

[00:45:04] 現在我要很壞心的問一個問題，請問這是「線性」的還是「非線性」的？那如果呢覺得線性的話，就請大家打一個圈，在網路上的同學，等一下這個你要在這邊打圈也可以。好，那那個如果覺得非線性的時候，你就打一個叉。這樣可以吧？好，那我們現在因為反正網路上的同學看不到大家，所以大家不用害怕。我們把答案洩露給大家。所以在現場的同學，請問你覺得是線性的請舉手？線性這個式子，這個式子上面這個式子 $W_1, W_2, W_3$ 已經學好了，就決定了。這個時候是線性還是非線性？

[00:45:47] 好，謝謝。那不是，非線性的請舉手？有有人覺得這個式子可愛，應該是非線性。為什麼有同學是沒有舉手的？還是那邊其實是沒有同學？只這個大家一定要選一個。那請問網路上大家說的是什麼？圈還是叉？圈比較多還是有叉的嗎？好，很好。大家還是有支持它是非線性的。

[00:46:11] 答案就是它是「線性」的。為什麼它是線性的？就它沒有平方，沒有開根號，沒有沒有那種噁心的動作。那那個比較視覺化的情境，就是比較圖幾何的解釋方法，就是它畫出來的圖形就是平的那種東西。如果在二維的空間它就畫出來就直線，三維空間畫出來平面，四維空間畫出來...反正就是平的那些東西。就是這樣。好，所以它是線性的。

[00:46:39] 它是線性的，這我們的小時候學的線性回歸的標準的式子就長這樣。然後你就說老師既然說到小時候的線性回歸，好像還可以加上一個常數？沒有錯，為了滿足大家的需求，就是我們也會讓每一個神經元都有他自己的「偏值」，這個名字取得真好，偏值 (Bias)。每一個神經元都有自己的 Bias，所以他都會加上自己的偏值，就是再加上 $B$。然後但很不幸的事情是，很不幸的事情是，這個全部的這個式子就 $W_1, W_2, W_3$ 跟 $B$ 是我們要學的參數。學好了以後這個式子還是一個線性的，還是線性的。好，那線性有什麼不好的地方，等一下我們先休息十分鐘以後再跟大家說明一下。

(休息與課程行政說明)

[00:47:26] 好，大家好，我們繼續。那個那個就是在這邊講的比較...大家可能沒有聽過同學會覺得快，但其實很慢。但不要太擔心啊，反正這個大家如果沒有聽懂的回去也可以看。然後那個大家如果順利的經過了一陣子的陣痛，然後終於順利的收到了 NTU COOL 的通知了之後，會發現很多的資訊也都在上面。那所以大家就可以看到，其實我們也可以在 FB 先發啦，就是可以看到說這個我們的助教的 Office Hours 的時間。

[00:48:07] 好，就大家不用太擔心，可以有問題就盡量的來詢問。那很多問題不是太困難，像上次剛剛有同學覺得說，因為要寫程式他怕了，他就決定退了。這個超級可惜，因為這個程式，這個今天我們要寫程式，你就會發現這個你聽懂的話，程式就超級簡單。那你即使聽不懂，有那麼多的助教準備在那邊幫忙你，麻煩給他們一些工作，免得這個下次大家檢討我們的時候說這個我們不需要這麼多助教，沒人會有問題，那就慘了。好，所以請大盡盡量讓支持一下我們助教。

[00:48:45] 好，那我們會在這個...因為一開始的時候有一些同學可能找不到，就還沒有辦法進入到 NTU COOL 的一些資訊，所以我們會在 FB 這邊也會為大家說明一下說現在這個課程就是目前的境況。然後我們的助教的時間也會先在這邊公佈，有什麼問題可以去聯繫助教。好，好，大概這樣子了。

激發函數 (Activation Function)

[00:49:15] 然後所以我們就發現說呢，它基本上就是一個線性的函數。那線性函數有什麼不好？當然沒有什麼不好，線性函數簡單可愛啊。可是呢，問題是說這樣子建造的神經網路呢，整個都會是一個線性的。全部都建造好，全部都是一個線性的函數。所以呢，那我們就回想在高中的時候就有學過線性回歸。線性回歸那些參數全部都是公式就可以算出來的。但缺點是我們只有那個一副長得像線性的那個資訊數據我才可以學。那大部分的情境，我們相信大家也可以理解，大部分的世界上的應該...好。

[00:50:11] 所以在這樣的情境之下，我們就希望有沒有辦法讓它不是線性的，就吐出來的數字不是線性的。那很簡單就是再做一個轉換。那我們剛剛上一節課已經說過了，在數學裡面說要做一個轉換，其實就是找一個可愛的函數來啊，帶進去就對了。那這個可愛的函數一定要是什麼樣的函數才可以？它就一定不可以是什麼？我本來期望在我們的課現場有同學會告訴我，就不可以是什麼函數？這不是廢話嗎？我們不希望它不是線性的（口誤：我們希望它是非線性的），所以帶進去那個函數不可以是什麼？線性函數啊。就是一定要「非線性函數」。就這樣子。

[00:50:54] 因為我知道大家猶豫的原因，因為因為這個答案聽起來太白痴了。這麼簡單的問題，老師怎麼可能會問我們這麼簡單問題？不好意思，我們這課程只有這麼白痴的問題。所以這個請大家不要害怕就回答，即使錯其實也沒不會怎樣。就這樣子。

[00:51:10] 好，然後...然後所以就帶進這個是非線性的函數。那這個非線性函數我們把它叫做「激發函數 (Activation Function)」。然後就是一定要經過一個非線性函數的轉換。這個非線性函數轉換其實也沒有什麼特別了不起，因為等一下我們就會看到這個非線性函數有什麼樣的選取的方法。大部分的情境還有我們今天的作業，其實就是選一個很有名的非線性激發函數。什麼叫非激發函數？非線性的激發函數。就激發函數要非線性，所以我們就是選擇一個激發函數，你就看整個神經網路基本上都用同一個激發函數就結束了。就是這樣，所以也沒有什麼好擔心的。

[00:51:49] 還有我們所有技術的話快說完了，那還有一點點件小事情。這件小事情就是說呢，我們今天的剛剛這個神經元呢，需要調整的參數就是 $W_1, W_2, W_3$ 跟 $B$。相信大家如果有在外面看到 AI 的就什麼 AI 的模型的話，這個參數...這個模型很大，就是我們都是用參數量來形容那個模型的大小。像這個模型呢...不是這個模型，這個神經元呢，它要調整的參數只有四個，就 $W_1, W_2, W_3$ 跟 $B$，四個參數要調。

[00:52:21] 那我們在一個 AI 模型裡面都會把所有的參數，就是所有要調整的參數，我們會把它收集起來，叫做 $\theta$ (Theta)。就是神經網絡裡面或是所有的 AI 模型裡面，它需要調整的參數。那像這一個神經元就是 $W_1, W_2, W_3$ 跟 $B$ 就是要調整的參數。好，那我們還會有一個小問題，為什麼一定要用 $\theta$ 呢？沒有任何的理由，就是因為在資訊科學家跟數學家都很喜歡用希臘字母來表示。不然的話 Parameter 嘛就參數，為什麼不要用 $P$ 呢？那沒有其他理由，就是因為我們覺得用希臘字母寫比較高級的感覺。好，就是用 $\theta$ 表示。

[00:53:05] 所以你會發現在外面寫一個神經網絡的模型，通常是 $f$ 這個函數，然後下標就是寫 $\theta$。就是這樣。好，所以 $f_\theta$ 就是基本上就是一個模型，還需要有調整參數。那參數就是...對不起，$\theta$ 裡面。

[00:53:26] 好，那我們很快的介紹幾個有名的激發函數。其實開始的時候我們只要學一個就好了啦，那但是幾個有名的給大家看一下下。

[00:53:32] 那最右邊的大家會說這個好熟悉的感覺，沒有錯，它又常態分佈，高斯函數 (Gaussian function)，就常態分佈畫出來的樣子。那這個要跟大家講悲傷的事情，高斯函數曾經在上世紀是最受歡迎的激發函數，最標準的激發函數。但是這個世紀大家發生的...不是發生，是發現它的種種的缺陷，所以大家已經沒有什麼想要理他了。最悲傷的事情是很多這個深度學習的重要套件，像今天我們會學 Tensor Flow，另外一個是 PyTorch，那這個深度學習重要的框架裡面呢，基本上已經禁用這個函數了。就是你要用的話，你自己寫，他沒有要提供給你用的意思。這個所以我們可以忘掉它，寫在這邊完全是為了懷舊啊。

[00:54:20] 然後中間的那個叫 Sigmoid。你們作業如果你堅持想要懷舊的感覺，你也可以自己寫一個高斯，比較會寫程式的同學的話。那中間那個 Sigmoid 就是我們覺得好像比較接近人的那個神經元的動作。就是刺激很小的時候不要就有反應，不然會神經衰弱。所以刺激很小的時候就是負很大的時候就是輸出就是 0，就接近零啊。那很大很大的時候也不要爆衝，就是很接近一，最多是接近一。就是這樣。這中間的那個現在也常常在用的 Sigmoid。

[00:55:03] 比較神奇的是比較神奇的是這一個，這個 ReLU (Rectified Linear Unit) 這個激發函數。ReLU 這個激發函數呢，它這它是這個...我們看它左邊，左手邊線性的，而且常數函數就是 $f(x)=0$，完全線性函數。那右邊呢就是 $f(x)=x$ 這線性函數。中間最簡單的兩個，一個常數函數，一個就是 $f(x)=x$。好，這個怎麼看都線性的，它哪裡我們非線性？有，在這裡，原點這邊是非線性的，因為轉了一下，所以非線性的。

[00:55:39] 好，是真的可以嗎？答案是真的可以。那上個世紀大家有用嗎？沒有。上個世紀我們才沒有那麼厚臉皮，像看到這個誰敢說他是非線性的函數。雖然你覺得他好像不是線性，但你也不好意思說它是線性的函數（口誤：意指不好意思說它是非線性，因為太簡單）。但這個世紀呢，包括 Hinton——Hinton 就是去年的諾貝爾物理學獎的，我們稱為深度學習的三巨頭之一的 Hinton——他也帶頭使用的其中的一個在本世紀。然後所以大家就覺得這個真的好用。好用的原因是它很好算啊，這個我們人用人腦就可以算，所以很簡單。

神經網路設計實戰：隨便設計

[00:56:18] 好，所以這個以上就是神經網絡的最基本的介紹。就是不管我們是用 DNN, CNN, RNN，其實我們只是想要配置每一個隱藏層。在配置的時候，我們看要配置幾個神經元上去，然後我們決定它的連接方式是要用 CNN 型還是 RNN 型，還是用全連接型就 DNN 型。就這樣。

[00:56:38] 那我們今天要介紹的是 DNN 型。那為了讓大家更清楚這個作業的進行，所以我們就來一起練習一下下。假設我今天已經想好了，我今天的神經網路輸入就是兩個神經元，輸出層就是一個神經元的輸出。很少這麼簡單的題目，在真實世界很少這麼簡單的。在我們小時候寫算數學的時候都是這種題目，但真實世界很少這種這麼簡單題目。那不管了，反正我們就是兩個輸入一個輸出。那我們就開始設計神經網路。

[00:57:09] 然後你就會說等一下老師你剛剛從來沒有教我我們怎麼設計一個神經網路。現在就開始教我們設計神經網路。好，我現在再開始教你，設計神經網路。

[00:57:19] 好，大家要聽好。就是呢，我們今天因為規定只用隱藏...不對不起，全連接的神經網絡就 DNN。那個那個型的神經網絡 DNN，所以我們今天只能要求大家只能用 DNN，就 DNN 這種。那所以我們要設計 DNN 的神經網路。那我們來一層一層的設計，就所有的神經網都一層一層設計。

[00:57:44] 第一件事情呢，我們要決定第一層的時候，我們要放幾個神經元上去。 就這樣，就這樣。好，就這樣子。來，今天這因為這頁，大家聽好這所有的設計的技巧就第一層，我們先決定第一層要放幾個神經元上去。好，然後那要怎麼決定呢？答案就是你愛怎麼決定就怎麼決定。如果你不能決定，你就問你自己大概還五歲左右的弟弟或妹妹，然後就問他：「你覺得你最喜歡哪個數字？」他說 10，你就放十個神經元上去。他說 20 你就放 20 個神經元。他說 200 就 200 個。可以嗎？就這樣沒了。那你回去自己怎麼設計？就是這樣設計。

[00:58:28] 好，那這個叫什麼設計法？這個沒有沒有沒有設計法。好，那你可不可以問 ChatGPT 有沒有什麼設計原則？可以，但他會找不到答案，因為就是沒有設計法。就是隨便...我本來說隨便設計，但聽起來太隨便了。但事實上就隨便設計，隨便隨便設計。你回去自己勇敢的試看就知道了。

[00:58:47] 好，第一層假設我們就設計第一層是三個神經元。平常我們比較少設計這麼少個，我們通常可能會多一點。就不要害怕，你回去就可以勇敢的嘗試，這是因為這就是我們今天的作業。就三個神經元。好，第一層就三個神經元，然後那個就放上去就對。

[00:59:08] 那為什麼放上去就好？因為連接方式是固定的。全連接顧名思義，就是 $X_1$ 呢，一定要傳給它的每一個下一層的每一個神經元。就這樣子，全連接。$X_2$ 也要傳給每一個神經元。所以它這邊的連接數就是 2 乘 3。就是就每一個每一個每一個一層到另外一層要完全連接。這就全連接的神經網路。所以我們只要決定有放幾個神經元就好了。所以一層就設計完了。多麼簡單輕鬆快樂。而且剛剛已經說過了，這這一層怎麼設計出來的？不用管它，就反正你心中浮現的第一個數字就把它放下去就對了。就這樣好。

[00:59:47] 還有第二層怎麼設計呢？第二層就是一樣你心中浮現的開心的數字，你就把它放上去了。需不需要比第一層多？可不應該是這樣問。比第一層多可不可以？可以。比第一層少可不可以？可以。跟第一層一樣可不可以？你看是不是又是隨便的意思？但隨便真的說出來太難聽了，而是就經過你的判斷，覺得最適合的數字把它放上去。其實就隨便給的。好，然後所以我們就比較假設有給三個。我再再說強調一次，不一定要一樣。好，然後因為全連接，所以馬上只要決定放了三個，它就連接方式就固定了。

[01:00:26] 好，再下一層呢？下一層怎麼辦？就隨便。然後你現在會又浮現另外一個問題了。這個另外一個問題就是說：老師這樣子有點個小問題，我們到底要做幾層才夠呢？那我要做個三層嗎？還是五層？還是八層？還是十層？還是兩層呢？要做幾層才夠呢？答案就是你高興做幾層就做幾層。

[01:00:49] 這樣好，我們神經網路的設計已經說完了。

第一個隱藏層，我們要決定每一個隱藏層要有幾個神經元。

第二個我們要決定要做幾層。
就這樣，這就是我們的設計原則。沒了。沒有加原則，那個什麼你應該要放多少格，你應該要放幾層，沒有沒有原則，沒有原則。就大家勇敢的去嘗試。你現在會有點不相信老師說的話，這個也是自然的，事實上應該要懷疑才對啊。然後但是你回去做作業就會發現真的我隨便放幾乎都可以。會不會出現不可以的？就是訓練出來好像沒有那麼那麼好，也有可能，也有可能。但是反正那個你就不要當作業交就好了。就這樣。然後你就再改，然後改出來這個很容易改到，很容易改到你很滿意的成果。就這樣子。

[01:01:39] 好，我說完了。然後...然後因為我只是說累了，所以我決定要說尾了。所以所以決定就這那個就最後一層了，然後就是最下一層就接輸出層。所以就是就是沒有在設計了。所以我這邊就是兩個隱藏層的神經網路。

神經網路簡史與深度學習的由來

[01:01:52] 我這邊很快的為大家介紹一下下，神經網路的歷史。在歷史的演進過程中間呢，開始的時候大家就知道它是 Universal Approximator (通用函數逼近器)。也就是說所有的函數，你想學的那個東西，不管是人臉辨識、蘋果辨識...什麼叫蘋果辨識？八哥辨識。蘋果跟香蕉辨識——我為什麼每次要舉蘋果跟香蕉？下次請大家舉一個比較漂亮的例子。到底要辨識什麼？八哥辨識比較好。八哥辨識或是什麼反正任何的或是股票預測，反正任何的任何的函數，你想學的那個函數，那個剛剛的 Universal Approximation Theorem 就告訴我們說，你一定只要一個隱藏層，你就可以學會。

[01:02:38] 所以那個世紀、那個時代就上個世紀、上個世紀末的事情。那個上個世紀末的時候呢，大家就覺得神經網絡真的太厲害了，所有的問題都可以用神經網絡來解決。就這樣。那真是萬能型的一個一個 AI 模型。好，就這樣子。然後但很不信的是大家發現了他非常大的弊病。它的非常大弊病有很多啦，其中最大的就是電腦的運算能力真的不行啦。因為我們剛剛說過他要非常多的運算，它要非常非常多的大量的數據。但以前電腦根本沒有機會處理大量的數據。

[01:03:14] 例如說我要做八哥辨識，以前有沒有人想過我們可以做八哥辨識？有。還有那個好蘋果跟爛蘋果。就是我如果是一個水果公司，我不是說不是說那個電腦公司，我是說那個真的水果公司。他要判斷這個是好的蘋果還是爛的蘋果，他可不可以圖影像辨識一下，說這個好的，這是爛的？可以，以前就有這樣的想法，以前是真的有人這樣想的。但是問題是以前沒有辦法做。

[01:03:39] 為什麼沒有辦法做？一個原因是以前在最早的時候最早的時候呢，那個某一某一種電腦電腦就可以用的個人電腦，他一張圖都放不進去。很可憐啊，一張圖都放不進去，那那就更不要說這個我要我要讓他做八哥辨識，那不可能。蘋果好蘋果辨識不可能。這幾乎什麼什麼東西都做不出來。就這樣子。所以非常的悲傷，大家就覺得哦，這個神經網路真的很沒用，真的沒有用。

[01:04:11] 然後直到這個世紀，其實不是這個世紀，其實是直到最近，最近大概 10 年左右，大家才發現神經網路好像還有厲害的地方啊。就做出了很多個模型，包括說那個...包括說那個打敗世界棋王李世石那個。那大家就覺得這真的非常非常厲害，原來這麼厲。那等得又很多新的應用出來，大家就發現神經網真的可以，真的可以。好，然後所以所以那個神經網又再度的紅了回來，就是這樣子。

[01:04:44] 那為在紅回來的時候呢，那個大家就想說不要想起這個過去，悲傷的過去。那過去有一陣子大家都對神經網都沒有信心，因為為什麼都做不出來？就理論很好，但什麼都做不出來。那大家就沒有信心了。那為了要讓大家忘記這個過去我們稱為「神經網路寒冬」的時間，所以大家就說：「哦，不如我們就把它改一個名字叫做『深度學習 (Deep Learning)』」。

[01:05:12] 那為什麼叫深度學習？因為以前的那個數學定理告訴我們說，一個隱藏層就夠。但現在發現做多層是有好處的。做多層你通常可以讓他不要那麼就是你同樣用...就是你可以不用那麼多個神經元，就可以做到一樣好的效果。那一層的話你常常要用非常多個神經元才做得做得好。這樣在在在這個我們事後馬後砲的解釋就是說，如果我用比較多層次的這樣子去訓練它，訓練它訓練它的時候，那我就會發現說我應該就可以我的邏輯的層次比較深。我就可變變成第一層的邏輯，在第二層的邏輯，第三層的邏輯。那在這樣的情境之下，我就不用耗費那麼多的運算資源，我就可以把它給算出來這樣子。

[01:05:58] 好，好，但大致上的情境是這樣。就是如果邏輯的深度需要比較複雜的，我們就適合用比較深的神經網路。那那因為以前做的大家都是一層的神經網路或是兩層，兩層很給面子，一層就夠了，還做了兩層。那但是很少做三層的，幾乎沒有人做三層。所以呢大家就想了，那我們就來定——因為既然做比較多層、比較深的神經網路是有好處的——那我們就來定比較多層，三層或三層以上神經網路就叫深度學習。這樣子大家就想哦，深度學習好很好，就不會想到神經網路了。

[01:06:33] 好，又過了一陣子發現矯枉過正了。大家都接受神經網路很厲害了，但是呢，大家都想要做三層或三層以——你們回家作業不一定哦，回家作業不一定，你也可以試兩層可不可以哦，就是三層或三層以上的。然後那個發現矯枉過正，所以現在呢，現在大家又開始改了，說你只要用神經網路做的，我們就叫深度學習。深度學習就是用神經網路打造那個呆萌機器人的方法。大概就這樣。

[01:07:01] 好，這個就神經網路的整個的過程就這樣子。所以我們就發現這個做打造神經網路真的很簡單。我題目決定了我要打造他的時候，我只要決定說要放幾個——就決定要做幾個隱藏層，然後每一個隱藏層要幾個神經元。大概就這樣結束，就這麼簡單。

神經網路是如何學習的？(Loss Function & Gradient Descent)

[01:07:20] 好，那因為時間的關係，我們用很快的說法來說一下下。這個這個是我們在介紹神經網路過去的故事，還有悲傷的消息。那因為時間的關係呢，我們來說一下這個學習的時候...因為時間的關係，我們因為我們等一下要帶著大家做我們今天的範例，所以學習的地方呢，如果有興趣的同學可以看一下下，其實相當相當的簡單。

[01:07:47] 我決定不管怎麼樣，我還是要為大家說明一下下。就是神經網路呢，我們怎麼知道他學得好還是學得不好呢？它其實是非常非常的簡單，就是一個神經網路學得好還是...就是一個神經網路，我們都會都會有一個評判的方法。就是看說我們的學習跟那個正確答案到底差多少。就這樣子。

[01:08:11] 我們就會設計一個函數，就算分的函數。這個函數叫 Loss Function (損失函數)。因為我們都是算扣分，就是我的答案出來了——就是神經網路只要權重決定了以後，那個他這個神經網路就會吐出一個答案來。比方說我們八哥辨識還沒有訓練，他就可以辨識了，只是他辨識都亂七八糟。他就是就是那個今天只要只要我們會對那個會對我們的參數做初始話，就是會先給他一組數字，然後所以他就會開始給他一組之後呢，他就會他就會開...他就會開始辨識八哥字亂辨識，他都會說出答案。

[01:08:46] 跟我們現在的 ChatGPT 真的有好幾分相似，就他不管你說什麼話，他都一定接著下去說。然後但是有時候會他會胡說八道。那八哥辨識也是你還沒有訓練他就可以說了。因為我們要求他就是一定要輸出一個數字，一定要輸入一個答案出來。所以他就會告訴我們這是什麼八哥，只是錯誤答案可能是錯的。

[01:09:05] 所以我們就要把我們的答案跟正確答案比較它的距離差距。那那把所有的差距平均起來，那個就叫做、那個就是我們的 Loss Function。所以我們是算扣分的，越不像是的話，Loss Function 就越大。就是我們的答案跟正確答案差越遠，就 Loss Function 越大；我們答案跟正確答案越近，那個 Loss Function 就越小。就是這樣。所以我們就想盡辦法用用某種辦法，其實就是微積分，然後讓 Loss Function 讓它變小。就這樣。

[01:09:38] 好，我為什麼說的這麼快，就是只是因為我們的時間的關係。那但是這個其實過程其實還蠻簡單的，那有興趣的同學可以看一下，欣賞一下。好，就是它就是看 Loss Function 它的看 Loss Function 的方向。這其實是會算出在我們微積分裡面很喜歡的，就是 Gradient (梯度)。其實在某種程度它就是那個那個那個...話不會說了，我會太緊張了一點。就就是 Gradient 就是它的在那個平面上面的那個 Gradient 就是斜率、切線斜率。

[01:10:11] 那切線斜率我們在微積分學就是它變化率。那你在微積分的時候，微積分的美好時光裡面會學到說那個切線學率就代表變化率。那那個所以呢變化率是正的時候它就是往上跑，變化率是負的，它就往下跑。就是這樣。

[01:10:29] 那我們神經網路要做的事情...我後悔了，還是要給大家看那一頁圖比較神經網絡要做的事情呢，就是我們已經在某一個位置了。這個是 Loss Function，這個是 Loss Function 就是我們隨便在隨便有一個初始畫的值出來了以後，那我神經網路要做的事情呢，就是想辦法找那個切線斜率的相反方向。因為切線斜率是小於零，它就是往下。切線學率小於零的小於零的時候，它就是它就是往下降。好，那切線學率大於零的是就是往上跑。那我們就會發現切線斜率小於零的時候，我們要跑的方向是往右邊跑，它才會走到那個比較小的位置去。就這樣切小率小於零的時候。那切線斜率大於零的時候，我們就會要往相反方向走。所以我們就會發現切線學率給我們了很多的資訊，就是只要跟切線就唱反調就好了。

[01:11:28] 那在高維度的空間裡面，就高維度的函數，那個切線斜率相對應的就是所謂的梯度 (Gradient)。就這樣，就是立體的切線斜率了。但是指的方向是一樣的，往最大值的方向走。那個切線斜率指的方向就是往最大值的方向。好，切線學理或是 Gradient 就指往最大值的方向，這是在我們美好的微積分時光學的。那我們要往最小值，就是當然就要跟他（梯度）的反方向走。就是這樣子。

[01:11:57] 好，沒了。好，我說完了。因為我們今天用的最不能說草算，我們用最快速的方法說完了。之總而言之，我們需要知道的就只是我們想要希望能往那個切線斜率相反的方向走。就是我們希望、我們希望想辦法、想辦法讓這個想辦法讓這個這個那個我們的神經...就是我們的我們的這個呆萌型機器人他說的輸出來的答案越接近這個正確答案越好。這個廢話當然就這樣子。所以我們就希望我們的 Loss Function 越來越小、越來越小、越來越小。這就是我們想要做的事情。

實作：打造手寫辨識神經網路 (MNIST)

[01:12:42] 好，好，所以我們就準備要打造第一個神經網絡了。好，我們現在要做了，坐下來了。那這個打造這個神經網路呢是這個樣...（調整設備）...我們很快的說一下這個場景，說完了以後我們就要開始打造這個神經網絡。

[01:13:15] 那這個作業的範例就是在 AI01，AI 大寫然 1。那前面是我的標準的縮址網，就是 bit.ly 然後斜線 AI01 (編按：講者口述網址為縮寫，實際應為 bit.ly 或其個人縮址服務)。好那等一下大家就會知道這個些這個程式感覺很高級很難，但是超簡單。而且要順便預告一下，那做完了以後大家就可以向自己的親朋好友爸爸媽媽炫耀一下，說啊我真的覺得我是學習 AI 的天才，你看我才正式的學了 AI 第一節課，然後我就可自己打造出一個神經網絡來，而且是做這認真的事。等一下我們會講要做什麼。

[01:13:54] 就大家記得這個網址，因為作業等一下就是改這個網址。那我們要用的是一個叫做 TensorFlow 的框架。我們先先不要太管它。然後用的是一個非常有名的，可以很適合用深度學習的數據，叫做 MNIST。好，那這個數據集裡面呢，基本上就是手寫辨識的資料。就他想要做一個手寫辨識，就是那這個手寫辨識其實不是所有的英文字母，甚至當然不是中文字的那個手寫辨識，它就只是阿拉伯數字的手寫辨識。也就是說它全部的分類只有十種，就是 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 十個類別。

[01:14:32] 那我現在就變成一個分類的問題，跟八哥辨識一樣。所以我輸入就是一張那個 28 乘 28 很小的灰階的灰階的圖。那輸出的時候就是一個數字，就是看它是六的話就是 6。當然我們也知道我們我們就像八哥辨識一樣，我們輸出的時候事實上是輸出 10 個數字。輸入的時候是一個 28 乘 28 的矩陣。那全連接的神經網絡，我們需要把這個 28 乘 28 一共有 784 個數字，把它拉平。拉平了以後把它輸進去。就這樣子。那就是這些灰階的，就是灰階的把它拉平，就是這樣了。這樣那所以真正的神經網路應該長這樣，就輸入是 784 維的向量，然後輸出就是十維的向量，就 10 個數字。

[01:15:22] 這就是我們要打造這個我們的呆萌型機器人要做的事。輸入層已經決定了，就 784 個數字輸進去，就是一張手寫辨識的影像，把它拉成長的一個向量。然後輸出就是十維的向量。這樣可以嗎？所以我們題目就這樣子了。

Colab 操作說明

[01:15:42] 完那我們現在準備要上線，準備要開始做了。那等一下我會非常清楚的告訴大家，你需要注意的地方是什麼，你不需要注意...不是什麼叫不需要注意的地方，就是你需要注意的地方是什麼，然後你要改的地方是什麼。然後你只要聽懂，你就會發現非常的簡單。

[01:16:04] 那剛剛我們的那個縮址 bit.ly 斜線 AI 大寫 0 大寫的 AI 然後 01，就會出現這個網頁。這個是在 GitHub 上面有一個打造你的神經網路這一個。然後我會有各種的機密跟大家說，好，這樣可以嗎？好，那我們就這邊會發現有一個叫做 Open in Colab。就這樣打開。

[01:16:37] 然後呢，所有的作業都是這樣子。其實我們這個程式作業都會有一個示範，你就只要改那個示範，基本上就可以拉。在大部分的情境。所以其實非常的簡單，意思就是說我們最難的一個程式作業就是上一次的，因為是從頭到尾大家自己打的。上一次是最難的程式作業。從今天開始那個程式作業都是這種簡單...衰敗型的程式作業，你只要知道改哪裡就結束。好，但是還是要改的地方，所以我等一下會跟大家講。

[01:17:11] 所以在這裡呢，你會發現在左上角是章魚貓的樣子。章魚貓就是 GitHub 的那個...大家可能看不出來他是章魚貓。章魚貓顧名思義就是它是貓的頭，然後下面是一隻章魚的腳。好，那它就 GitHub 的那個 Logo。那這個意思就是說它是在它是在這個我的 GitHub 上面。那你只要看到這樣的情境，雖然它在 Colab 打開的樣子，但是但是它還是在人家的人家的這個 GitHub 裡面。

[01:17:39] 那在外面因為有現在有很多的模型都用 Colab 都有 Colab 的連接，你就可以在 Colab 上面跑。因為大家發現說 Colab 是免費的很好。那在這樣的情境，所以大你就會發現很多外面的人教你，在執行階段你就按「全部執行」。全部執行它顧名思義就會全部執行了，就不用像我們上次一直按 Shift + Enter。我比較推薦大家按 Shift + Enter 啦，因為在剛剛學的時候你比較有感覺，一段一段執行比較感。那你久了他就說按全部執行。然後呢就會警告「這個筆記本並非由 Google 撰寫」。
[01:18:01] 不用像我們上次一直按 Shift + Enter。我比較推薦大家按 Shift + Enter 啦，因為在剛剛學的時候你比較有感覺，一段一段執行比較有感。那你久了他就說按全部執行。然後呢就會警告「這個筆記本並非由 Google 撰寫」。我發現外面的網紅很喜歡就說不要管他就按「仍要執行」。可不可以執行？當然可以執行，但是這是不是正確的做法？所以未來你看到就可以有會心的微笑，因為你比那些網紅還要強很多。這個時候請不要執行。

[01:18:33] 那為什麼原因 Google 會警告你？因為這個還在我的 GitHub 上，他希望你不要執行來路不明的程式。然後第二個呢，你也不能修改，你修改它也不會存起來，因為在我的 GitHub 上面。那你要怎麼樣讓它變成完全是你自己的呢？就很簡單，在「檔案」這邊就有一個「在雲端硬碟中儲存副本」。就這樣，這個動作一定要，只要看到做這個動作就知道這內行的。

[01:19:14] 好，就這樣子就打開了。那所以這個時候你看這邊已經 Google Drive，就是它已經存在你的 Google Drive。所以呢你就完完全全的、完完全全的就是改成你的名字。比方說什麼叫你的？就我你的作業當然不可能寫「你的」，一定是「我的神經網路」、「我的 AI 辨識系統」好了。我就請大家一定要把它這個作業改成是你的作業。這個其實我們最大的練習就是把它改成是你的作業。

[01:19:48] 就比方說這個圖（指章魚貓圖片），希望在作業裡不要再看到這個圖了。你看到這個圖就是老師你是直接老師來的，你也沒有改。那不要這個圖很簡單又刪掉了，就這樣。然後你可以加入自己的筆記本嘛，我上次有介紹過，滑到中間，然後往上就有個文字，然後你就可以愛打什麼就打什麼。好，就是「要打造 AI 手寫辨識」，簡單的說就是請大家基本上把所有的文字都改成你的說法。因為你不一定是做這件事情。

設定神經網路參數 (N1, N2, N3)

[01:20:35] 然後呢，接著下來呢，因為我們用了最簡單純的方法，所以我這邊的 NE 就是我今天要打造一個三個隱藏層，也是標準的深度學習三個隱藏層的神經網路。N1 就是把它打...就是 20，就是我第一層要設計 20 個神經元。N2 就是第二層要設計神經元，這邊也是 20。N3 也是 20。所以你可以改這邊。

[01:21:03] 但是我們有的難度會稍微高一點點。如果真的非常非常非常不會寫程式，也懼怕所有的地方，那你只要把這個這一個程式改成說明，改成你的口吻的說明，然後你只要改這三個數字。這個三個數字當然絕對不能是 20, 20。你就自己隨便隨意的修改。如果你真的很害怕程式的，那對於一般同學，什麼叫做對一般同學？就是呢，你願意嘗試的同學呢，希望就是我們比較希望你就是除了改這個每一個每一層的神經元，希望你也知道說我如果要做比較多層或是比較少層是怎麼做好。

[01:21:43] 所以希望大家比較高分的會是你的隱藏層是不是三層。你要兩層也可以，四層也可以，五層也可以，八層也可以。就是不可以是三層，這樣可以嗎？好，就等一下我會介紹說那你要怎麼樣做到不是三層。

[01:22:01] 好，我們現在開始了。所以這邊就是決定要大家最容易執行，所以在這邊就是決定你的每一個隱藏層要有幾個神經元。好了，那接著下來就是讀套件。那好消息是我們連四大套件也不用管他了，因為我們基本上都讀進來了。那有一點點有一個不一樣的地方是我們讀進了 Gradio。好，這個我們就不講了，這個我們那個小廢話都不說了。就我們讀入了一些重要的套件，因為在開始的同學會覺得這一段非常的噁心可怕，那事實上就讀入最重要的套件。我們先暫時不要管他都到底讀入什麼，就我們這邊是必要的套件讀進來了。

讀取與處理 MNIST 數據集

[01:22:43] 然後我們現在要讀 MNIST。這個數據集都是人家幫我們準備好的，所以它準備得非常好。在 Load data 這邊它就會把它的順序是那個訓練 data 才會先輸進來。所以 x_train 就是把那個輸入的那個手寫辨識一張一張 28 乘 28 的灰階的手寫辨識的資料讀進來。那 y_train 就是正確答案。那正確答案等一下我們會看到它其實只是 1, 2, 3 這樣子的正確答案。它是 0 的話就是 0，它正確答案是 5 的話它就是 5。這樣它這邊就把資料讀進來。

[01:23:23] 那我們可以看一下欣賞一下說他幫我們準備了多少資料。這邊都可以正常的執行了。但這邊其實也可以不要執行，你真的是很懂的同學，這在你的作業可以，你這邊其實可以把它刪掉的。因為他這邊只是告訴我們說這個到底有多少筆資料這樣。所以他有 6 萬筆的那個那個訓練資料，測試資料準備了 1 萬筆。所以他非常慷慨的準備了非常多的資料給我們，所以比我們剛剛 1000 原則還要多。1000 原則十個數字只有 1 萬，1 萬筆數據就夠了嘛，但是它不是，它是 6 萬筆。

[01:24:02] 好，那我們接著下來呢，接著下來是不一定要執行的地方，只是讓大家欣賞一下。欣賞一下那個它的資料長什麼樣子。好，我們就隨便找到 28741 這一次筆，你就隨便移，你隨便移 35442，然後你按執行，它就會找到那個數字的樣子。它數字就長這樣了，就是解析度不是很高，28 乘 28 的灰階，所以就長這樣。它就是正確答案就放在這邊，所以它正確答案是用 1, 2, 3, 4 表示。

[01:24:40] 好，那如果你想要欣賞一下，它的每一個數據，每一筆資料，它沒有把它用圖形畫出來時候，它長什麼樣？它其實很簡單，這是第一列的矩陣，就是那個 28 乘 28 第一列，就是 0, 0 就全部都是白。然後這個第二列又是白色。然後你就會發現這個灰階，那那個數字越大就是越黑，那最大是 255，就 0 到 255 之間的數字。就這樣，大家可以看一下。這個只是讓大家看，所以這一段可以不執行。所以如果你很厲害的話，真的懂這一段可以不要執行，你可以把它這一段刪掉，因為這只是我們在理解的時候需要看的。

資料預處理：Reshape, Normalize, One-hot Encoding

[01:25:25] 再來那個，所以輸入格式的時候，我們就做一個整理。第一個我這邊還是要這邊稍微的說明一下下，因為雖然我們都不用改了，但是我們為了讓那個想要更加了解的同學更加了解，所以呢我們通常都會想要看 x_train 就是輸入訓練資料的輸入的部分。就是我們可以欣賞一下它的 Shape。這個幾乎是最重要的指令。那我在這邊順便解釋一下下，這邊是有 6 萬筆的數據，所以這個我看訓練資料的 Shape 的時候，這個輸入的 Shape 的話，這個 6 萬筆的數據，然後 28 乘 28 的矩陣。28 乘 28 矩陣，所以等一下我們希望把它改成 6 萬筆數據，但是 784 維的向量這樣。

[01:26:12] 所以我就會發現 X 我先做一個 reshape，然後就是 6 萬筆數據改成 784 維的向量。然後除以 255 的原因就是因為我們希望我們很喜歡數字不要太大。所以我剛剛今天我們剛剛說的就是說評分的時候雖然也可以給 100 分。除了我說的理由說我希望加起來等於一這個理由之外，另外一個理由大家發現如果數字不要讓它太大，它的效果通常比較好。那我們很喜歡把數字都壓到 0 到 1 中間。那因為原來的數據是 0 到 255，所以我說除以 255 的話，最多就是 1 而已。就這樣。

[01:26:53] 好，那那個...所以 X 就輸入的部分，不管訓練資料測試，我們都做這個動作。所以這一步是一定要執行的，這一步是一定要執行的。你也可以在這邊強調一下，就是要改成自己的作業啦。那改成自己的作業不一定要程式要改多少偉大的東西，你就把那個你很詳細的去學習的過程把它記錄下來。就這一步之類，反正你要寫什麼就寫什麼，因為這是在整理資料。好，一定要執行。

[01:27:25] 好，再來輸出資料的時候呢，我們有發現說這個它本來不是 One-hot Encoding，我們要把它改成 One-hot Encoding。還好這件事情是標準的動作，所以我只要告訴他說 y_train 裡面我有十種的數據，它就會改成 One-hot Encoding。我們先來欣賞一下這本來七號的數據呢，它就是 9。我再說一次，這個這個 Python 這個前面會加一些可怕的東西、噁心的東西，不要管他，他只是形容說這是 Python 裡面的這個整數型的八位元整數型的這樣資料數據。我們不要管他啦，反正就他只是要形容它是什麼樣的數據，那真正的數字就是 9。

[01:28:06] 好，那這個是本來還沒有改之前。那我現在希望把它做成 One-hot Encoding。看然後我們再執行一次就會發現，哦，它真的變成了這 001 最後一個位置，就是 9 的位置。它是 1，其他都是 0，所以它真的做 One-hot Encoding。所以現在完全做完了，就是整理的資料已經做完了。我們就可以真的開始打造我們的神經網路了。

打造神經網路模型 (Keras Sequential)

[01:28:32] 好，那打造神經網路方法其實非常的簡單。一開始我們就打造一個空白的函數學習機，就是我們的呆萌型 AI 機器人。然後我們要為我們的呆萌型 AI 機器人取名。那那個名字是自己取的，通常我們都把它叫 model，因為我們已經江郎才盡，不知道要取什麼名字。那大家不知道取什麼名字，大家都取 model，所以 model 是一個一個一個安全的取法。那可不可以叫 my_model 我的模型？可以啊，可以啊，就這樣子好，隨便。

[01:29:03] 那我就打開一個呆萌型的 AI 機器人，那個指令叫做 Sequential。神經網絡的話，因為一層一層打造，所以它的指令叫 Sequential，就打造呆萌型機器人了。然後我們就開始接著下來就一層一層的加上去。我也可以直接在這邊把數字放上去。那如果真的不是這麼熟悉程式的同學，你就可以直接執行，因為你前面就有改 N1, N2, N3 嘛。那 N1 我們是 20，所以你就會發現每一次做的時候就是 model.add 把某一個隱藏層加上去。

[01:29:36] 那這個隱藏層是什麼型式的隱藏層？是 Dense 層，就是最密集的那種，也就是全連接的神經網路。然後你只要告訴他說，你要加幾個神經元。你在這邊也可以直接打 20，你也可以直接打或是 30 或是隨便你要你要的你要的這個那個那個數都可以直接打進去，就神經元的個數。

[01:30:00] 然後因為第一次他不知道你的輸入層多大，那所以這第一次要告訴他輸入層多大。你會發現後面沒有這一行了。然後告訴他 Activation Function 是什麼。那比較會寫程式，真的很鼓勵大家可以去試一下，說好那我不可以試試看不要這個 Activation Function 可不可以呢？我也可以問 GPT，這樣我可不用別的 Activation Function？那我要用別的 Activation Function 的話，我要怎麼改這一段？好，所以就這樣子，只是告訴他說第一個隱藏層怎麼打造。

[01:30:33] 所以這個時候第一個隱藏層就打造好了。再來第二個隱藏層的時候就更簡單的。因為因為是全連接，所以上一個上一次的輸出全部都會有 20 個數字，就 20 個神經元，20 個數字都會傳給他。所以我就不用告訴他上一層有幾個神經元了。所以這個就是 N2。那就再打到這一個神經元，這一層我要打幾個神經元。那比方說這一層我突然要 87 個神經元，我就打 87。就這樣，之類。好，但是我們剛剛都是 20, 20, 20。那那個激發函數還是要告訴他。你就會發現激發函數我們通常都是從頭到尾選一樣。好，第三層就是繼續打。那我們就發現說這樣就建完了。神經網路是不是超簡單？幾乎是你想要做什麼，你就告訴電腦說你要做什麼，然後他就幫你打造好的神經網路。

[01:31:21] 所以這個是這幾行是真正的核心。那你要改作業，其實也可以只改這幾行就結束了。而且會很有成就感，因為這真的是你做出來的。只要你跟老師做的是不一樣的，就真的是你做出來。

[01:31:39] 好，然後再來做最後一層的時候也是。最後一層的時候就輸出層。輸出層的時候那一個最後一層輸入的就是輸出層就輸出的就是十個神經元，最後一層就要放十個神經元。因為第零個就是說零的時候它的分數，然後一就是一的分數，二就二的分數。就輸出有 10 個、十個數字。所以就輸出層就 10 個神經元。Activation Function 了就 softmax，因為我們希望加起來變成 1。所以你就會發現換了 Activation Function，然後就執行它。然後我們就建好了。

模型的組裝 (Compile) 與訓練 (Fit)

[01:32:11] 然後就要去組裝。一組裝這個神經網絡看起來好像很複雜，但沒有很複雜。

第一個是 Loss Function，我們用最標準的「平均平方差」、均方差 (MSE) 啦。意思就是說呢，就是看這個高維度——就是我們的神經網路說出一個答案，正確答案也是說出一個答案，那兩個都是十維的向量。那我就可去計算他們的差異，也就選他們的距離。但算距離中間有一個數學家很討厭的動作，就是開根號。不要以為數學家都很變態，喜歡那種複雜的動作，數學家很討厭開根號，所以就把它平方。所以這個這裡基本上就是做這件事情。那其實你也可以不要管他，或是或是比較覺得更細喜歡有一點挑戰的同學，這邊可以換別的 Loss Function。事實上它不是 MSE 就是均方差，它不是最好的 Loss Function 對於分類問題來說，只是它比較簡單，所以我們用均方差。

然後再來呢，SGD 就是標準 SGD...念反了，Gradient Descent。就是看著梯度跟梯度唱反調往...因為梯度值最高最高往極大值，那我們要往極小值就跟梯度 Gradient 唱反調，往它的相反方向走。那這個隨機型的 Gradient 到底什麼意思呢？意思就是說我們在問他問題的時候，不要每一次都是從第一題做到最後一題。這有時候我們會把那個位置給背下來，所以這個好像不太好。電腦也是這個樣子。在所以我們希望說他每一次都是隨機抽題目出來去訓練它。

那 learning_rate 就是說我一次走太大步的話可能會越過，因為走這個方向可能會過頭。所以為了希望不要過頭，所以我們在這個時候就可以加入一個數字，你會發現這個數字看就知道是自己亂打 0.087，你也可以換其他數字哦。好，然後這個最後面的只是說隨時告訴我正確率。

[01:34:15] 好，這個所以我們就 Compile 好。那有一個很重要的動作就是我們通常會建議建議希望大家都欣賞一下，欣賞一下這個自己的這個這個寫就是自己的神經網路。你就會發現呢，我們都是寫 20 個神經元的時候你就會發現這個是第一層有 20 個神經元，第二層有 20 個神經元，第三層有 20 個神經元，最後有 10 個神經元的輸出。那所用的參數就是個、十、百、千、萬，16750。這其實是一個很小的神經網路，可是相信大家從小到大從來沒有寫過那個數學式要調整那麼多參數，一萬多個參數。所以你可以很有成就感的，就是你突然之間就打造出一個很大的、以前小時候從來沒有做過的那麼大的那個一個參數一個式子數學式子，就要調整那麼大的多參數要調整的一個神經網路。

[01:35:08] 好然後呢，我們接著下來就做 fit 的動作，就訓練。那我要告訴他訓練的輸入是這一個，正確答案在這裡才能夠比對嘛。這邊 batch_size 就是說我們不要一次就從第一題做到第 6 萬題，我們要一次就做 100 題。100 題就開始檢討答案，就開始調參數了。這個意思就是這樣，就這樣子。所以這個 Batch Size 就通常我們會 Batch 是比較小的一段就開始調參數。然後 epochs 這邊呢就是我一共要做幾次。這個都是你可以改的，也可以不改，因為這不是我們今天最大的重點。

[01:35:44] 那你就可以開始訓練了。這非常的緊張，我們就開始訓練。3, 2, 1 就開始訓練。那你就會發現這樣的結果，它就會開始訓...因為因為 Colab 有免費讓我們使用 GPU，所以你才會發現它是這麼的快速。那最後你可以觀察這個正確率，現在一開始的時候只有 10 成...（口誤：應為 1 成），在 20 成...在 30, 40 一直...然後你就會發現你們這個信手拈來的神經網路做出來居然有八成六快九成的正確率。我順便說一下，因為這個大家為什麼那麼喜歡這個數據，就是因為它真的是給人信心，給人歡喜的一個數據。就你發現你回家真的隨便做，你一定可以打破老師的記錄。如果第一次還沒有打破的話，你覺得還可以訓練下去的話，你可以在這邊再執行一次，它就會再從那個 8 成 6 的正確率再繼續執行下去。就這樣。

Gradio 互動介面展示

[01:36:38] 好，那我們為了今天要直接讓大家看到成果，我們就直接來看最酷炫的成果。這個程式碼的部分不用太理他，因為打中了，所以不用太理他。因為我直接想要讓大家欣賞一下結果。就是你繼續執行了以後，他就會有一個神秘的網址出來，神秘的網址出來。那你就執行這個神秘的網址。那這個神秘的網址順便說一下，你的 Colab 還打開的時候，你就可以分享給爸爸媽媽親朋好友，反正任何你想炫耀的人。然後他執行出來的，他在手機上也可以執行，只要有這個網址，手機上也可以。

[01:37:25] 他怎麼這麼不認真...餵，趕快出來了，人家下課。他其實這邊也會出現一個，但是我因為我...好終於出來，終於出來。好了，那我這邊就可以來開始做手寫辨識的手寫辨識。就開始畫一個 3。你看是不是自己覺得自己做出一個好高級的東西。然後我們來看一下，不要丟臉哦，大家要下課等下等下等下有...然後你就會發現他真的辨識出來真的是 3 這樣。就是覺得實太有成就感。又特別是今天回去做我出的作業，就記得一定要改，一定要改成你自己的，然後你就會真的會非常有成就感。因為你會發現你做最多做幾次的嘗試，可能是第一次，你就會發現你做出來神經網路就比我們上課的這個八成多的還要高。那事那事實上八成多的時候，你就會發現這個看起來好像已經蠻準了。

[01:38:18] 好，這個就今天的作業。那我們休息十分鐘，等一下就是我們的接下來的 TA 的時間。

TA 時間：閃電秀與行政事項

[01:38:32] 好，那個那個首先先跟同學講一下說，反正接下來這時間呢，就是同學們可以去報名「閃電秀」。因為報名閃電秀上台呢，然後一方面就是那個什麼我們期末是有額外加分的，對。那這個時間反正就是因為我每次就只有五個同學的名額，所以說呢，反正沒少一次呢，那名額就對完全沒有報名的話，那 quota 就一直就直接消失了。我們也不會因為說期末人多，然後我們就直（接增加）...不會哦。所以說你們同學就是有任何有就是覺得有意思的小東西跟生成式 AI 有關的都可以上台去做一個簡短的（分享）。

[01:39:12] 好，然後再來就是那個什麼關於各位 NTU COOL 的事情，我這邊再講一下。就是因為我們這邊只能確保說我們東西送出去，但是外面到究竟實際上那邊增加的那個程序呢，我們沒辦法控制。因為人多也是慢慢一個加。所以同學們真稍等一下。那我們現在有兩個情況，一個是你加簽，就是上次有填加簽。如果你填了加簽單後呢，原則上我今天有跟承辦人討論說應該到今天應該是都有了。對，那你如果說你是填加簽單的，那你的課程可能還看不到哦，填加簽單你的課程還沒有，但是你應該是可以進（NTU COOL）。好，這是加簽單的。

[01:39:54] 那另外個情況是說你是用遞補上的。那你遞補上的時間呢，如果說你是在禮拜四或禮拜五呢，你可能也還來不及到就是新的這一批被送出去。所以說很有可能就是這個禮拜才會送出去新的遞補，就是新的送出去幫你們做處理。好，所以簡單說就是你到下禮拜二如果還沒有的話，那你再來信。那如果在下禮拜二之前...就等一下對，就是他原則上應該就是會慢慢加，然後就是發送邀請給你們。好，所以下禮拜二真的沒有的話，再來信跟我們說一下。

[01:40:30] 那因為那個我們的作業都是兩個禮拜為一個單位嘛。但是第一週的情況呢，我們就是我們知道說有這些行政程序在處理，所以你也不用擔心說你的作業就是沒辦法（繳交）。但是你要先就是你要先準備起來哦。就是你寫你的作業可以先寫，然後截圖什麼重點的都可以先把它給準備好。然後就是等到反正就是我就說嘛最晚下禮拜二再幫你開放的話，那我們可能也許第一週作業就是順延一個禮拜，對，就讓你有時間上傳。

[01:41:09] 那上傳上去之後...反正你就是來得及繳。但是反正我該說你千萬不要因為說會順延，所以說你就延後寫作業，延後開始寫。你的作業還是可以趁目前就開始寫作業。那只是說我們繳交時間會再就是開給大家就是順延，對。那但是除此之外呢，之後的反正我們從這禮拜開始，原則上我想像對嘛，這禮拜開始的延後兩個禮拜應該是不會有問題的，所以說就只有第一週會有順延的情況。

作業繳交規範：複製 Colab 與分享權限

[01:41:42] 好，那因為同學們都還沒有繳過作業，所以說的人都會有一些在繳作業上的問題，那來稍微講一下。好，那就是剛才老師其實有提到啊，就是說你的作業出來之後呢，哦，好，我整個動作先（做一次）...好，等於是也算是複習一下老師後半段講的東西。反正你開老師的 Demo 之後呢，你要先另存在雲端硬碟中另存副本。

[01:42:17] 那我們辨認等一下哦，我我應該要切換一個帳號，等我一下。白底會比較大家應該比較好看。好，所以說呢，左上角，左上角你看到就是還是這個，就是反正一隻貓，然後露出一隻手，其實章魚貓（Octocat）嘛。那你要先另存到你自己的檔案裡面去，所以說你要在雲端硬碟中另存副本。

[01:42:52] 好，另外開啟。好，那開啟之後，你看到上面已經變成是 Google 雲端硬碟的符號呢，它還是代表說真的存到你的作業區嘛，這是一件事。那老師剛才提到說什麼該修改的修改啊，你這邊標題記得要修改哦。那個我不會把我們的評分的所有機會講出來啊，對，但是幾個大重點，很多人真是甚至什麼就這個檔名完全改不改，這很明顯的一種就是就是擺明就是讓我扣分的。好，所以說你這邊該改的要改一下，老師剛有提到的就是太多點，那個你們再去看一下老師剛才的影片怎麼說的。

[01:43:36] 總之有幾個很明顯的點我會稍微提一下。好，什麼什麼的「副本」拿掉。那作業當然可能改成一下更具有你自己識別性，你不要說只是把頭尾一些字移掉。好，我想講點就是說那個「共用」。因為上次有一部分的同學可能沒有注意到。好，就是你的權限要打開，知道連結的任何人，對，然後就是最少權限要打開來。然後就複製連結。這時候的複製的這個連結，貼出來到你的繳交區，它才是你的作業連結。你千萬不要去複製上面這個（瀏覽器網址列），你如果複製上面這連結呢，我只能說有時候可以用是運氣好，那多半的時候呢可能是不可以用的。所以我們一定要複製共用區的共用區的這個連結。

[01:44:27] 複製連結，然後再貼出。那有時候你要就是說你到底你不確認說你貼出來的東西到底是不是正確的，我我不確定我能夠等下我換著麥克風。好，因為我不確定我能夠真的去重現所有的錯誤，我們試（看看）。好，就是因為我們教大家的方法就是我們用一個就是其實有時候換個瀏覽器啊，重點就是你不要你個同個帳號。好，所以說有時候是開個「私密瀏覽」是一個最快的方法。你去貼上去看看說看它是不是可以閱讀到你的就是你的文件的內容。

[01:45:10] 好，像我開了一個都有閱讀到嘛，然後基本上那代表說你這一份呢是其他普通人，就是你自己以外的權限是可以看得到的。因為假設你的權限沒有修改後呢，你還是自己的帳號呢，你一定可以看到自己的內容。所以為什麼會請說，請你說換個瀏覽器或是說開個私密瀏覽，看一下說它是不是可以打開來看到這份內容。

作業修改重點與 Markdown 說明

[01:45:46] 好，那再來呢，就是繳交後，那就是去修改嘛。那該怎麼修改呢？就是一招老師說，你不要讓我一打開起來說奇怪，這跟老師的範本怎麼就基本上文一模一樣的這種東西，那分數一定不好。所以說呢，該修改的部分呢，就是照老師才上課說到的，你要改成一看起來就是你自己的，就是 for 你自己的。

[01:46:09] 那當然必要的時候呢，你去加上你自己的註解。對，那你的註解呢，我們要看的註解呢，其實是以 Markdown 檔...就應該說你要給別人閱讀的東西是以 Markdown 為主。那有些同學會知道說我們他可能會是用在後面去加井號，但那種井字號的是給你自己看的。你如果真的是程式稍微寫多一點，就知道說什麼叫做給你看到的，然後什麼叫做給其他人看的。所以我們主要閱讀呢應該是閱讀上面的文字。

[01:46:39] 好，因為這邊像有些同學就是上個禮拜就很認真，這就是跑來 Office (Hour) 的時間，然後剛好遇到我。好，那所以我這邊也順便講一下。好，本來本來是給他的福利，好，但因為我們上次講到說就是上個禮拜作業是畫一個函數圖嘛。那畫一個函數圖啊，那它可能它的他剛才拿來的東西呢，就是就是在程式裡面，然後就寫了一個函數畫出。但是我要看的是說，就是對於我們直接閱讀人而言，你應該是把你用的函數用 Markdown 語法，就是寫出來說，OK 我要畫的東西是這個東西，$f(x)$ 我就隨便寫哦，那 $A$ 我隨便寫積分好像也不是一個圖隨便。總之我想表達是說就是說你把你想要的你想要表達數字用 Markdown 語法呢完整的漂亮表達出來。

[01:47:43] 然後你截圖，截看看說 OK 我要畫的圖是這個東西。然後什麼什麼圖是反正我一直在講畫圖。好啦，既然是畫圖，那就不會是一個積分符號。好可能就是這是我要畫的圖。當然我們也有說這種你也生了繳交時候，你不能講那麼簡單的那種圖片。我想講說就是你要截的重點呢，你就是說 OK 那你畫的是這個圖，你把你的式子，你把你這個式子給表就是截下來。然當然你也要截你的程式的地方，你真正修改程式的地方。然後是你的程式哪一段對應的是對應這個，對應這個。

[01:48:33] 好，所以你的截圖呢，因為每個章節重點不一樣嘛。像以今天的作業而言，你的章節呢，它的重點老師跟我提到說就是幾層網路嘛。那幾層網絡呢，當然就是說網絡的部分你可能改了四層，那你這那你至少就算你延續老師的符號，那你至少這邊要（截到）。所以說你要說 OK 我截圖四層。

[01:48:54] 那再來就是說因為老師這個範例呢，雖然說都已經準備給大家了，那但是有幾個重點的地方呢，就是你你一定了解基本的地方。你還是要了解說 OK 那邊改了四層之後，那哪邊還要再修改。好，我們就是加...我現在邊講話找不到，就是在加網絡的地...嗯...切資料...好...一...點...組裝。哦，好。那你就是會就是用同樣方法，就是即便說你對函數對程式沒有真的很熟，但是某些地方該就是至少你要知道說 OK 我們今天這個地方是 Dense 層多加一層，那那就是照順序應該要四加在這邊，你要再多加一層。

[01:50:05] 所以你要截圖的地方，你不能只上面截一個。因為我這邊講是很多人，他甚至說他說我做個四層網路，結果他截了...他截了上面這邊總共有四層，結果他下面什麼都沒動。然他跟我說截了四層，那很明顯你就是不懂啊，那你純粹只是你就產生了某種誤會，那你以為這邊改了四它就他就會是四層。絕對不是這。所以說每個每個不同的不同的重點是什麼東西，它會有因應不同重點。你真的理解的時候呢，你就會知道說重點在哪邊。這個我們不可能一一的去提出來說這本週重點到底是什麼，所以請你重點截圖呢，就是真的是要把重點截出來，不要只是一個應付的說：OK 我這張圖就叫重點了。哦，那絕對不好。

截圖與 PDF 繳交注意事項

[01:50:58] 所以說這是那個什麼就是在繳交作業還有什麼叫重點截圖稍微講一下。然後重點截圖的部分呢，要嘛要嘛你就是直接好以 Markdown 檔而言呢，以整個作業而言，就是你繳交區呢，你除了貼連結之外，那當然有個地方你如果說體貼助教的話呢，請你把連結請你把那個什麼就是連結做成超連結，讓我們一點就可以好。

[01:51:24] 那一個就是說好，以上禮拜的作業為主為例，我要畫的圖是這個東西。就是你 Markdown 做越漂亮，那也某種程度上也看得出來說，你對整份作業越用心。那我想講是說你要嘛就是就是說就是說你可能只截這個圖。其實我是很鼓勵你直接在 Markdown 檔做得越清楚，直接連 Markdown 檔那邊文字區一起截圖。截完後呢，然後就貼過去，貼過去繳交區會是最簡單的。對。然後因為有些人呢，他們真的截圖就只截這個函...就是他要畫函數的部分，然後再截它的結果。就是其實我想講是說你整份東西你要想像說你是交一份報告，你不要沒頭沒尾的，就中間有一個函數，然後有一張圖。那到底剩下的呢？你是說我要畫函數圖是什麼東西，那你把它給接下。

[01:52:24] 那上個禮拜老師有提到說你可以用 AI 輔助，就是用問 GPT 或問誰，然後去說你想如何把圖片做得更精緻。那這個東西呢，就是本身它也算是你自己去跟 AI 對答，然後得到的一個新結果部分。你也可以把那個部分截下來說，OK 我在這一份作業中，那我額外做哪些事情，你要主動的去展現出來。可以注意要看說我額外做什麼事情，然後哪些東西，然後就是我做的不一樣的。

[01:52:56] 然後像 AI 教你說可以如何把它變粗或是怎麼樣的換一個顏色，那你就是你就是把那個部分就是那一段的的程式也特別秀出來。然後就說 OK 這是我想跟 GPT 請教之後或怎麼樣的，然後驗證過後他的確是可行的。好，注意哦，因為那個就是因為而已發生過，有些人問完之後呢，他的程式跑出來跟他貼的結果是不一樣。所以我都想說他到底是哪邊偷來的圖片對。因為他他跑他貼的東西呢就一定跑不出來嘛。因為 GPT 這些就是反正那個什麼大家會知道說他就是生成出來的過程中，它有時候只是機率性的高，回答正確率高，它不一定全對。所以說你一定還是得經過驗證，然後去反覆調整。對。

[01:53:43] 那所以說呢，反正重點就是說你在這份作業上有哪些努力，那你就把它截圖，然後截下來，讓我們一目瞭然說 OK 你今天的重點說你，你以畫圖而言，我這邊有多做了一些。而不是你只是因為這最（投機）機會就是一個一個連結點，然後什麼都沒有。你沒有截圖呢，然後假設你的連結權限又連不過去，我根本不敢做什麼事情，那拜託不好意思，那就就沒分數啦。你如果連結連不過去，然後但是你有截圖呢，那至少我還可以給你一些部分。好，所以就是就是到底什麼叫重點這種東西呢，我就再次強調，我不可能每個不同作業告訴你什麼叫不同。只要是你自己做的，你用點心你會知道說，哦，這邊我用的確不一樣的技術或幹嘛的，那把它截下來。

[01:54:33] 那截圖的時候呢，因為有些人可能就是一整面呢，那他就是覺得就是要貼好做圖。那我們我們是允許你做成 PDF，但是 PDF 請你自己先預覽一下。因為我印象中那個它檔案太大，不知道是 10MB 還是 20MB，太大的檔案呢，它是沒辦法直接預覽的。你如果繳 PDF 呢也可以，但是你要確保它是可以預覽的。點開的時候呢，是在頁面上直接瀏覽，而不是讓你額外...這邊一定要注意哦，就是是可預覽的狀況下哦。對。那你如果說因為有些人那個截圖他截那個檔案畫素太高，那個檔案是要爆掉，所以說我們等於是也沒辦法直接快速預覽到你的內容。好，所以這邊是稍微作業的繳交的一些部分稍微提一下。

[01:55:20] 那反正當然就說了這些，反正只要就是老師的引導東西呢，其實上了 313 (應為課程編號或代號) 對，不要真的就是不要讓我們看到覺得是就是就一整個就是覺得老師的範例。老師範例我們看多，我們是很是是還蠻熟的。好。

選課與非正大同學注意事項

[01:56:04] 就先幫大家補充一些我想到的。還有一點啊，就是那個反正你只要不是政大的同學呢，你繳交會注意千萬你不要講到我們課程。因為我們這邊只會改就是政大的同學的部分對。所以說你如果有任何其實有有幾位學生就是不是非政大的寫信給我說什麼點名加進什麼問題，我們一樣在這邊再重述一下。就是反正就是我們課程是由我們這邊一起開，但是作業批改什麼或是你的選課題呢，就是各校有各校負責的老師或者助教。所以說你們在就是各校自己的就是選課上面，然後或是說你自己沒有（分數）哦，那你就是看你是哪個學校，那你去找你們那個學校的老師或助教。對，這邊因為你把信寄到我們這邊，我們也只能回覆你我說的話，我們這邊是沒辦法幫上忙的。

[01:57:04] 那作業也是一樣，因為作業真的是剛開始的時候呢，真的有好幾次有同學都是會講到主導課程來。對，但是非政大的同學呢，請就不要講到主導課程。

進階技巧：Colab 與 Google Drive 連動 (wget vs Mount)

[01:57:24] 好，那如果沒有其他的問題的話，那我來講一些。就是說因為我們作業是有幾個，就是如果說真的是有就用到 Colab 的話呢，因為很快後來發現說很快都有同學們會直接會有一些外部檔案的應用。那甚至說你那個就是你的檔案需要你要連接到你的那個什麼那個...那這邊要跟同學講就是說你因為像我們前面這個簡單的作業還沒有用到外部資料的時候不會有這個問題，但是你如果連到 Google 雲端啊，你連到 Google 雲端會有個問題哦，我們改的時候呢我們是跑不動的。

[01:58:06] 這邊要注意你的你繳交出來東西都要讓我們按下執行都要能夠跑對啊。所以說如果說因為發現有些同學是已經有些經驗了，那他們就知道說可以他可以直接連接 Google 雲端，那讓他的資料放在 Google 雲端裡面，然後他自己跑得很開心。對，因為這個跟我說的你開共享一樣嘛，就是說有時候你的共享不成功，但是因為是你自己的帳號，你連接你自己的個人東西，那個權限一切都沒問題。但是你要想想看說你繳出來讓我們看的時候呢，我按下執行它是不是可以？如果你程式送出來還是不能跑的話呢，那其實那個分數也會非常的不美麗。所以說你要確認說你的程式是我可以跑的。

[01:58:50] 因為就像我剛才提到嘛，有時候他的程式根本就不能跑，還可以他還可以貼圖出來，那我真的完全就懷疑它是哪邊偷來的圖片啊。對啊好。所以說那有些東西呢，就是所以說你要如何去連接你的 Google 雲端呢？那因為 Google 雲端本身又一定要有就是個人的權限嘛。那但是我們這邊有幾種方法，就是說你可以繞過繞過 Google 雲端，然後就是又可以去又可以去那個直接下載。

[01:59:46] 好，那個等下哦，好繳交。我看一下哦，等呃...好，因為這位同學就對於繳交方式有些有些疑問。等我看一下。好，反正我現在的重點就是說我剛才提到說那個什麼那個繳交時是要我們可以直接預覽的狀況。所以說呢，我記得我們的作業區應該有講說，就是不要用附件的方式，因為附件上傳的話呢，它是沒辦法我們就是沒辦法直接閱讀到的。
[02:00:00] 所以說呢，我記得我們的作業區應該有講說，就是不要用附件的方式，因為附件上傳的話呢，它是沒辦法我們就是沒辦法直接閱讀到的。而且附件我開一下作業課程好...總之呢，那個繳交區就跟一個 Word 的編輯類似嘛，或是那種就是 Google 文件一樣嘛。反正你就是可以貼個連結、寫個文字。然後因為我邊講我還在開，還在找我的那個作業繳交區。好，我這邊應該可以用學生視角模擬一（下）。

[02:01:15] 哦，好，好。他的問題就是在文件上傳還是文字輸入。OK。哦，欸，等一下哦，那我們應該直接把文件上傳拿掉，不要造成同學們的誤會。好，在「文字輸入區」呢，然後就是貼上你的連結。好，就例如說...好，再一...哎呀，既然都已經打開了，那我們就就把我剛才講的某些東西直接具象化。等一下，貼錯了，不是這個連結。對啊，看起來貼連結它就直接變超連結。那我不太懂哦，有時候沒有變超連結。OK。那對，那不知道為什麼有很多時候同學們寄來東西是沒有超連結的，我們要很辛苦的複製過去。

[02:02:11] 好，總之呢，好，那你當然就寫說 OK 那個什麼「作業連結」。不過這小事啊。然後那接下來就是「重點截圖」。那重點截圖你可能就說以第一個禮拜的函數來說，就「我要畫的是什麼函數」。當然我就說啊，你其實你可以把你要畫的函數那些描述文字呢，你直接在你直接在那什麼在 Markdown 這邊，只要只要完整的呈現後呢，你你就把你就要畫的那個文字，就是你要貼的重點那個文字整個截（圖）。

[02:03:04] 好，那你可能就這個整個截下來。對，那你就是當你的...我其實我會一直強調說說就是如果說你 Markdown 那邊做得好呢，其實你這邊重點截圖區呢，你可能也不太需要打什麼字啦。對啊，你但是還是有些人就是 Markdown 那邊就是不打字，然後他就直接貼圖。對，但是我就說貼圖呢，其實有時候我們就我們看起來就比較沒頭沒腦。對啊，那你可能就說額外的重點有什麼東西，反正就下面補充。好，所以說這邊呢，貼上重點截圖是這樣子。

[02:03:55] 那我看一下哦這邊應該是有個...欸，我之前用過它，它有一個它可以跑出很多功能的。好，沒關係，反正繳交後還可以修改。哎，重新繳交作業？哎，等一下哦，讓我回想一下重新繳（交）。是會全部刪掉嗎？好像是這個平台好像是這樣。是嗎？有沒有其他同學已經有按下重新繳（交）？哦，好，好，好。這個平台會這樣的話，那你重新繳之前呢，你可能需要有一個...我看一下，取消...OK 好。但是你你可以去你可以去那個什麼...可去那去檢視你自己的作業嘛。就是你旁邊可以檢視自己的作業，然後因為我就說就是說就是你檢視的時候呢，它是要整個反正就讓我們一目瞭然可以快速抓到你的重點。再點進去的時候呢，就可以對於你的重點去快速的瀏覽。對。

[02:05:28] 那有啦想到啊，上學期是這樣回答的。重新繳交時候，那變比較辛苦，你可能要先把它整個複製下來，然後再重新再重新貼上去就保有你原本的那個格式。好，然後那你那個什麼就是那個上傳，然後我就說你如果是上傳 PDF 的話...插圖我沒記（錯）應該是文件上傳。這個我不這邊試啊，反正就是讓你瀏覽的時候呢，因為你瀏覽的時候就是它可以...因為就我們批改的畫面跟這不太一（樣）。好，這邊不嘗試太多。反正就是讓你繳，我的重點是繳交時候，我們可以直接看到你的內容，越清楚越好。好，好。

[02:06:23] 那那個我們總之回到剛才那同學的問題就是，就是那個什麼就是貼在文字輸入區就（好）。然後那其他其他如果可以拿掉的話呢，我會視情況我們去把它拿掉，就減少同學造成誤會。

Colab 下載 Google Drive 檔案的高級技巧 (gdown 與 wget)

[02:06:48] 好，那回到這邊呢，我要講的是回到我們就是說好可那個就是繞過繞過權限的下載法。好，那我們要先講一個比較典型的做法。對，因為我覺得這個是比較保（險）。

[02:07:18] 好，有一串連結。好，就是 Google 雲端呢，這個連結我們晚一點就是會把它給反正貼到貼到那什麼那個貼到我們的 NTU COOL。好，那你要有一個就是你要下載的東西的目標。那你要下載的東西呢，例如說我們會建議說盡量都是就是假設你要下載的東西，然後它可能需要...如果是單一單一份檔案是沒問題，那但是如果說檔案很多的話呢，建議你把它給打包成一個 ZIP 檔，然後統一下載，然後一起把它給一起把它就是再解壓縮。對，就假設你東西很多的。

[02:08:06] 好，所以說呢，你這個東西呢，你它需要一樣要開共用。好，那它共用完之後呢，好，知道連結的任何人，然後複製連結。然後來到來到這個地方，就是這個轉檔的這個生成器，轉連結生成器。然後你去把它給按下 Create 它會幫你生成出分檔案。這連結我們把它記下來，就是我們待會可以透過它去直接下載。好，那下載的時候這一整...等一下哦，我們應該把它...下載目標。

[02:09:07] 好，那我們下載時候呢，會用到一個指令就是 wget。好，那前面有驚嘆號講一下，就是這個是對啊，下系統指令，這不是正規的 Python。那我們 wget 完之後呢，就是兩個單引號，然後把你的這個貼上去。然後接下來呢，會有一個大寫 O 然後告訴他說你要取名叫什麼東西。然後這 content 等一下再跟大家解釋。那我們建議，我們建議說就是會取名取跟你的原始檔案一樣啊，你操作起來會比較順。好，我們看一下，在我們雲端硬碟裡面有這個 df.zip 我要下載是它嘛，所以說我們會取名的時候就取一樣名字。當然你因為某些需求你取名要繞，就是要換名字也是可以的。好，按執行。

[02:10:16] 大寫 O 不是 0。好，好，那你來左邊看一下。好，它就是它左邊這邊呢，就是那個 Colab 給我們暫存空間嘛。然後你就發反發現說這個暫存空間已經有一份檔。好，那但是那個就是這邊有幾個東西，到後來我們都會直接複製貼上啊。就是說就是因為有時候會遇到一些類似問題，然後我們發現寫上去也不會有事，所以就寫...就是說它有一些 --no-check-certificate。好，就是說我們就是有時候會有一些權限驗證問題，我們就說我們就剛說我們不要 check。那所以說這邊呢，我把這個刪掉可以執行。

[02:11:04] 好，再執行一次。然後呢，它就有打錯嗎？一個...好 OK 一樣它就跑出來嘛。我剛刪掉它又回來了。所以說所以到時候這個反正這一串呢，有時候就是直接整個複製，然後你就是去換掉我們要的那個那串網址就好。只是那串網址你要先經過這個就是這個翻譯的這個就是生成的。

[02:11:41] 好，其實這個生成的東西你有興趣呢，你可以自己寫。因為你會發現說它只是把某個格式，然後換成它下面的特殊格式而（已）。對，你要自己寫一個也沒問題，你不一定要透過，你不一定要透過這個轉換器對吧。你可以把原始最原始的，你會自己寫的話，你把這個最原始的這一串連結複製下來，然後然後你就貼到你就貼到這邊，就是反正你可以自己寫。

[02:12:07] 好，那這我要講就是說有時候呢，因為我們在外面閱讀呢，就是說你會看到有人加這個加了這個就是斜線（反斜線 \），是 Enter 上面那個斜（線）。那這個目的呢，只是說它有時候因為一整行太長了，那我們閱讀起來呢會比較就是你會看不到後面在寫什麼嘛。所以說呢我們可以用這個斜線它像我們視覺上把它切斷，但是程式閱讀上面它都視為同一行。所以說呢，我們會最常看到的情況，我們一樣再刪掉，算了，我們改檔名好了。好，所以說呢，它就是會直接就是你到時候就很清楚後第二行的部分呢，貼我們的連結，然後第三行是我們檔名。就是讓你好閱讀而已，所以說就是有時候外面最常看到是這個。

[02:13:05] 好，那那個但是因為像我這個範例呢，我是以整個 ZIP 檔為主嘛。如果說你只是單純一個東西，單純一張圖片的話呢，它直接下載下來，它就是它沒有什麼問題。說要單純...呃，好，我好，這邊順便講一下。因為那個我們老師今天本來要講到說...不是不是今天，接下來會談到說 AI 使用一些倫理，但是其實有時候倫理這議題不在 AI 時候。就是說像我們使用圖片呢，這個會有一些版權的問題。但是我們現只是舉例，對，只是說我只是舉例說某個可以直接下載東西，它是什麼長相，我們直接下載這張圖片給（大家看）。

[02:13:48] 好，那你這邊它會有個就是我們按右鍵呢，它如果可以直接下載的話呢，它會有那個「影像網址」。好，影像網址後呢，然後就是在瀏覽器再額外的貼上，然後它會變成...我看一下哦，這有點緊張，其實我也不確定我會不會成功。好，總之這一串呢，好，我們就是來到這個可以直接下載的地方，我們把它貼上去看看。把然後其實我剛才讀了一下，因為它不是 JPG 啦，它不是 JPG 所以某個東西 webp。好，x15.webp。

[02:14:36] 好，我的檔名叫作 S15 點...好一下的。好，總之呢，你可以下載的單檔呢，它如果是一邊圖片檔的話呢，那其實假設這 JPG 那你當然你就可以直接操作。或是說它是一份就是就是類似 Excel 檔案，你可以直接操作。好，我想講的是說，我想講說如果是單單檔沒問題。但是你如果是壓縮完的 ZIP 檔，那當我當然就順便講一下說那我們如何去解壓。

Colab 中解壓縮檔案

[02:15:13] 好，那解壓縮這件事情呢，就是我們就會需要用到一些會用到一些那個什麼那個套件。好，那其實也可以順便講一下說你如果 Linux 指令的話，你也可以直接下系統指令。那我們就是還是盡量用 Python 的指令。那我們就是用這個 zipfile 的這個去去就我們需要用這個套件才可以下載我們看這個 zip 檔。因為我們前面這是 df.zip。好，那可能一個 zip_file，filename 不打 n 了。好看起來有點聰明啊，有點聰明。我看一下哦因為他們現在的確會自動幫你補完。你按下 Tab 後它就整個補出來，但是你去讀一下說看這是不是你要的。好像是他聰明到我幫他補個東西就好。好，Close 掉好。好，我們試看看這樣是不是可以直接欸 ZipFile 我看一下哦。

[02:16:31] 哦這邊打是不是打錯字？哎不是不是。哦哦，等一下，等等下。他亂取名，我們要（解壓縮）是 df 這檔案，他剛是讀不到檔案。哦，好，這個補完工（具）的確也也很強啊，因為他本來不是我要去寫的方法，不過這個方法也是大同小異。好，那這可以順便跟大家講一下，有時候你可以接受他的建議，然後去試，對，因為它真的還蠻聰明的。我們看一下，這個是我解出來的壓縮檔，然後的確裡面可能又一層又一層的資料夾。

[02:17:17] 好，總之呢，我想講就是說就是你未來繳交作業裡面呢，因為很快應該就會遇到，然後你是需要額外的檔案的時候呢，請你就是把它打包成一份資料打包成一份資料夾之後呢，然後打包成一個應該就打包在一起，然後就是壓成 ZIP 檔。然後丟到你的 Google 連結，然後再用我們剛才的那種就是這個用這個轉換的連結的生成器，然後生成出來後呢，用剛含（剛才）的方法把它給下載下來。好，這是其中一個方法。

[02:17:53] 還有一點時間，我再講第二個方法好。那這可以順便講一下說就是有時候那個你真的是反正我們有提到說就是很忌諱你有新的問題。哦好，有有同學不清楚我這個 ZIP 檔到底在幹嘛。好，我就說了他是某一天假設說很快我們就會遇到說我想一下我們圖片辨識有吧，有有我們反正你要做某些圖片辨識的練習，那可能也許有你自己的圖片需要上傳啊。那你需要上傳有幾個方法。好，我現在這 10 分鐘順便講一下，我不要那個下載的第二個方法，我先直接講一下。你們用關鍵字自己去問一下 ChatGPT gdown 好。你就問同請教我如何用 gdown 下載。

[02:18:50] 好，好，這個是你問 GPT 然後我回覆同學問題哦。就是假設呢，有一假設你在做作業的時候，他有個情況，那個因為那個情境那個脈絡，我剛應該把整個脈絡講一遍。呃，我先把它中斷掉好，整個中斷執行，然後我就是讓左邊的檔案全部消失。

[02:19:15] 好，假設說你有一份作業是你可能需要自己做自己的人臉辨識或幹嘛，你要上傳很多你的照片。那你要的做法其實其中有一個做法就是說好啦，因為我現在沒有沒有真的照片可以上傳，我就直接用個截圖代（替）。那截圖代替你可能就是用手動把它拖上去。那你手動拖上去的時候呢，你看到它就會出現警告訴你說：OK 這只是暫存的。好，那當然接下來就依照你那天的作業或是程式去操作這份圖片。

[02:19:47] 那但是有個問題啊，你要想啊，你今天在繳作業的時候呢，你的作業那是你還要經過你手動去拖一份資料上去才行。但是我又沒有那份你的作（業）...那所以說呢，你有沒有可能別的方法說讓人家說更好的方式說我直接按了一鍵執行。對於我們而言呢，這裡回我剛才說的，我們在看你的作業的時候呢，我都按「全部執行」。你中間不要再經過有其他的，當然除非有一些特殊的地方，你一定要我們輸入東西，要不然全部執行的，我們基本上就是讓它從頭跑到尾。

[02:20:22] 那所以說你想啊，你如果說你中間還欠缺一份檔案，你一定要手動拖上去才行，那我們沒有這份檔案啊。所以我剛才就講到說，那我們就是用 Google 我們可以利用 Google 雲端的方式呢，你把你的東西存在 Google 雲端。但是除（了） Google 雲端會有個情況，就是說我們一般人沒有你的權限，我也讀不到這份資料啊。好，所以說在 Google 雲端呢，它有個做法。

[02:20:42] 就是說我們可以可以直接的順便把剛才講同在快速的回覆，就是複習一（下）。你一定要先開啟它的共用權限，讓它變成是任何人至少可以檢視。可以檢視就可以下載。好，所以說任何人是檢視者。那檢視者那那 Google 一般情況，它這個連結呢它沒有複製到。好，總之你自己試後，你就知道說一般情況下呢，它是不能直接下載的。你還要透過，你還要透過那個什麼，你還要透過那個畫面，就是說它有一個下載按鈕。那所以我們要如何讓程式可以下載？要如何讓程式可以下載？那就是說這邊這個轉換器，你把你你要東西你的那個共享也貼上去，按下 Create 之後呢，它會伸出另外一串。這一串就是可以直接下載。

[02:21:40] 然後再放到我們剛才的範例裡面說就是說你把就是這上面那一串都不要動，你就貼到貼到這個地方。那當然下面的你要取名取成什麼檔名，你要先取好。好，取成檔名之後呢，然後你按下執行，然後它就會把你的那一串東西給存下來。所以就這樣子就完美的達到說你不需要額外手動拖什麼資料進去，你只要按一下執行，然後它就可以把你的資料抓下來，然後你就可以看（到）時候的作業是要做什麼操作。好，這邊只是幫大家規避未來（問題），說你們可能常常會遇到就是你用 Google 雲端，那助教讀不到，那讀不到那就很尷尬。對吧。好，大概是這樣。

[02:22:25] 好，那不知道現場有問題嗎？或是網路上（有嗎）？好，順便講我覺得還蠻方便的啦（指 gdown）。但是它有時候會掉檔，所以說我主要講說還是先以上面（wget）為對。但在某個層面上，它可以整個資料夾複製下來。有時候你的資料夾還需要資料時真加（增加）東西進去或或是移動出來的時候呢，它某個角度應該說不同的 Case 情況下，它是比較好用的。但是當你一旦的東西都確認之後呢，建議是打包成 ZIP 檔，然後用我們上面這個方法讓你整個下載，它是（比較）不好（錯）...

[02:23:10] 所以反正 gdown 這個就留給大家去問 GPT 說如何利用 gdown。你可以問說我在 Colab 上面想要用 gdown 下載 Google 雲端的東西，那叫他教你。甚至你可以叫他教你別的方法，他說有很多方法。當然你講作業的時候呢，你就是用到一些其他方法，你就要處名（註明）一下說：OK 這是你跟 GPT 回覆就是討論出來的。然切記你不要是那種丟一句話它生出來的東西你就照用那種東西呢，直接就是你只是你沒有經過多次討論出來東西，那種通常一看也那個痕跡也很明顯。

[02:23:55] 好，那那個所以如果沒有問題的話，那我們今天就到這邊下課哦。
大家好，今天是我們上課的最後一週。我們下週要幹嘛？等一下我們會說明。今天我們要對這學期的課程做一個小總結。好，還沒有結束，所以大家不要太開心，還沒有結束。

[00:08:13] Diffusion Models 要被擊潰了嗎？

我們在這個小總結的時候，有一個非常奇特的名字叫做「Diffusion Models 要被擊潰了嗎？」。等一下我來解釋一下為什麼取了這麼奇特的標題，然後到底是不是這樣，這就是我們今天要回答的問題。

我們先回顧一下我們目前這個生成式 AI 課程，我們到底學到了什麼東西。首先，我們有學到一個叫做 GAN (Generative Adversarial Network)，就是生成對抗網路。那時候相信大家都還記得，就是兩個神經網路，一個是當創作者，就是生成式本人；然後另外一個就是判斷說它生出來的東西像不像樣。本來大家都覺得說這個 GAN，因為可以用在聲音、可以用在影像、可以用在文字，全部都可以，所以當時大家都覺得這真的是生成式 AI 應該要走的路。

好，那個時候是這樣子。可是不久之後就發現，GAN 真的快要被打趴了。被打趴最主要是在圖像生成的部分，那打敗它的就是我們今天的主角——Diffusion Models。

[00:09:30] Diffusion Models 原理回顧

關於 Diffusion Models，我們很快地回憶一下。這個 Diffusion Model 基本上就是很像 VAE (Variational Autoencoder) 的架構。它訓練的時候很像 VAE，也是一個 Encoder、一個 Decoder，而且輸入一張圖，然後要還原成這張圖。所以它真的非常像 VAE。

可是呢，它的 Encoder 這個部分並不是用訓練出來的，它是算出來的。就是一路一直加噪聲 (Noise)，加到這個大家看起來像 Latent Tensor 的部分，雖然代表圖像，但是看起來完全是我們不知道電腦在想什麼東西的那種雜訊。好，那就變成一個 Embedding，然後這個 Embedding 之後再經過 Decoder，它應該就要還原成這個圖像。

那因為中間的 Latents 非常容易生出來，因為它就符合常態分佈了，所以非常容易生出來。所以我們就可以自己生出一個，當做我們天馬行空的想想法，然後再送入 Decoder 之後，就生出那一張圖。後來我們也發現，其實我們可以做得更好，我們可以把想要生成的意涵給加進去。因為剛剛生成的過程之中，我們沒有辦法把我們想要生什麼樣的圖加進去，就單純只是天馬行空的一個想法，它自由自在的創作。我們現在需要有一點點限制，就是我們要告訴它這張圖是什麼東西。大家現在已經很熟悉了，就是把這張圖是什麼東西送給它，然後它把它畫出來。

Diffusion Model 其中一個重大的原因，就是因為它可以非常清楚地用自然語言告訴 AI，讓它把我們想要生的圖生出來。所以大家都覺得這個相當厲害，導致大家都不太去理會 GAN 了。這就是 GAN 當初發生的事情，也沒多久，就這幾年的事情而已。大家突然覺得在生圖的時候，應該就是要用 Diffusion Model。

[00:11:58] 語言模型 (LLM) 與 Token 預測

當然還有一個生文字的那一個 AI，就是我們要預測下一個 Token。現在因為我們已經一個學期快結束了，我們就不用再假裝了。之前我們都是說「一個字預測下一個字」，但比較精確的說法是「預測下一個 Token」。上次我們也有說明，一個 Token 跟一個字有時候很接近，特別在中文的時候幾乎一個字就是一個 Token，但是有時候也不是。特別在英文的時候又變得比較複雜一點點。但是我們可以不要管它，反正就是預測一個 Token。

從現在的這個 Token 去預測下一個 Token，我們當然知道這絕對會有問題，所以我們也知道它其實是把前面的記憶——可能是用 Transformer，可能是用 RNN，現代都用 Transformer——把它留成一個像樣的記憶一起進來。所以我們可以想成就是前面的一堆字去預測下一個字的對應模型。

所以這個大型模型基本上就是先把後面根本還沒有生出來的東西先蓋起來。像這個「炎龍老師很...」，我記憶裡應該是用 GPT 某一個版本。它真正的編碼是這樣，輸入這些字，但對電腦來說，那些字其實也都是一堆數字。就是把它做 Tokenizer，然後把每一個字有一個代表的數字，然後就放進去。放進去以後它會把每一個字都給它一個分數。那從這些分數裡面，我們就把它用 Softmax 加起來變成 1，然後再從裡面抽樣一個字出來。機率高的字就比較容易抽到，機率低的字比較不容易抽到，但是還是有機會被抽到。好，這個就是我們前面學的。所以假設這一次抽到「幽」了，那我們在下面又放「幽」這個字下去，然後再來就會繼續生成。

[00:13:55] 大型語言模型入侵圖像生成

所以整個大型的模型就是這個樣子。然後大家突然發現一件事情，這就是我們上一次的主題：如果我們把圖像也當成一種語言的話，那也就是我們的 Transformer 模型，如果它本來只會生文字的，它現在還是只會生文字，但是有一個新世界的文字就叫做「圖」。所以如果能夠看懂圖像世界這個語言，那它可以生出這個它也會說的話呢？在那樣子的情境之下，就好像說它真的也可以用大型模型來生圖了。

那我們上次有介紹過，大家其實已經用到快要爛了，就是像 ChatGPT 現在可以生圖了，Gemini 也可以生圖了。所以現在的大型語言模型幾乎都可以生圖了。所以它就是跟我們原來的語言模型一樣，去預測下一個 Token 的方式去做這件事情。只是我們畫成橘色的這個可能是圖像的語言，所以它就把這個圖像的語言轉譯。但是這個圖像的語言我們人是看不懂的，因為只是一串的數字，它也不是一對一的對應。它跟文字不一樣，文字的話都是某一個字是唯一的對應，就是某一個數字。但是圖像的世界比較複雜一點點，因為圖像的變化比較多，所以它需要有一個轉譯器。

那這個我們之前有解釋過，其實就是用——再一次就是我們的 VAE，它只是叫做 VQ-VAE。這個我們上次有講解那個原理是什麼，但是就不要管它。總而言之，就是把這個語言放進去了以後，它就有一個生成器可以把這個圖像的語言意思，把代表這個語言意思的圖像給生出來。

大家發現了這件事情之後，所有的大型模型供應商都非常的開心，每一個都開始做這件事情，然後大家也用得很開心。原因就是因為，既然它本來就是語言模型，所以它對文字的掌握非常好。所以你可以很細節地告訴它，你希望它畫出什麼東西來。或者是說，比方說這個例子，我們舉著名的幾個語言模型給大家看一下。

[00:16:36] 實例：LLM 生圖能力

比方說我們畫了一個叫做「DP 的呆 AI」（可能是指 DeepMind 或某個特定的 AI 角色），然後我們說「請幫把我們的這個呆萌的 AI 機器人把它 3D 卡通化」。然後就跟 ChatGPT 講這件事情。所以我們把圖放進去，再一次，圖對這個大型模型來說，它其實也就只是一個語言，所以它就可以看懂。我們用剛剛的機制，它就可以看懂圖了。其實看懂比較簡單，這跟我們人類很像，我們要看懂一個語言文字比較簡單，那要「說」可能比較難。但是後來確定也學會了，就是它可以把這個語言也說出來。所以就是這個意思，就說「你看著這一張圖，然後把這張圖裡面的東西做 3D 卡通化」，它就畫了。

所以這個結果相當的厲害。另外一個就是最近被人說什麼「看不到 Gemini 跟 GPT 的車尾燈」，這個其實有點誇大。上過我們課就知道，不用那麼擔心地去看評比，你愛用哪個就用哪個。現在的差距沒有那麼大。當然最近本來又開始抬頭了，因為大家又說「喔，Gemini 實在太厲害了，做得真的是比 OpenAI 做得實在好太多了，OpenAI 不知道怎麼辦了」。這個不用擔心這種問題，這不久之後又會趕上來，他們都是互相趕的，不用太擔心。

因為他們的這個基礎的原理沒有變，就還是預測下一個 Token 的這種呆萌型 AI。所以在這樣的情境之下，他要改變非常巨大、讓人完全追不到的這件事情很難發生，目前是這樣。那說不定之後會有一些突破性的發展，也許不一定，但是要等到那些突破性發展才會有那個真的很不一樣的事情會發生。但現在沒有。

所以你會發現像 Groq (或 Grok)，另外一個的好處是它的速度還蠻快的。所以你就可以幾乎說，比方說你今天要做投影片，然後你這個投影片裡面的那些以前我們要很辛苦去找到的相片，還不知道有沒有符合著作權，所以在那樣的情境之下，你覺得有時候投影片真的很難找那個適合的照片。然後你現在就發現說，簡單的你要生出什麼各種情境照片，你就把它生出來。

好，這個是 GAN 也可以做了。然後今天我們的某種的主角 Gemini 3 也可以了。Gemini 大家為什麼覺得非常的驚豔？就接近 GPT 出來了以後，因為那個中文終於不會寫怪怪的文字。雖然我覺得這個情境也是很奇怪，我明明說我辦演唱會的時候，下面都是我的粉絲們，但他們拿的手燈看起來就怪怪的，比較像黑社會的集會感覺。好，不管了。反正總而言之，就是他們的文字可以正確的生成。就是 Gemini 3 目前生成文字的能力很強，這個大家也不用擔心，這個早晚其他的也會趕上來。

[00:20:08] Google Gemini Pro 學生免費方案

那我們就要再度的提醒一件事情，這之前我們就已經說過了，就是 Google 的 Gemini 呢，學生免費可以使用。就是你可以去申請那個免費的 Pro 方案。還沒有申請的同學請去申請，因為我們今天其實作業就跟這個有關係，然後我們下週也跟這個有關係。所以大家趕快去申請 Pro 的方案。相信很多同學都已經申請了，因為截止日期是 12 月 9 號，所以請大家要記得趕快去申請。

你申請到了就可以用一年。那我們助教有發現說可以不用給他信用卡就可以做了。你不小心給他信用卡，你先把那個訂閱取消。因為如果你沒有取消的話，那下一年如果已經沒有免費方案，他就會自動的幫你很貼心的扣款。當然你覺得不錯的話，你自己願意去訂閱當然是可以去訂閱。所以請大家記得在 12 月 9 號之前要做這件事情。

[00:21:30] 領域專家的重要性與下週預告

那我們一直有強調一件事情，在所有的 AI 裡面，領域專家非常非常的重要。不是生成式 AI 才是這樣，以前其實就是這樣子。那生成式 AI 特別是這樣，因為你就會發現叫老師我來畫圖的時候，我每次畫要嘛都是那幾套那個樣子。我畫得不好的時候，我也不知道要怎麼改了、怎麼教它了。所以真的是需要專家來介紹。

那我們下週就會請專家來介紹這件事情。我們下週會請到「鳥巢老師」來做我們特別的 Ending，為大家介紹。因為大家人手上都有 Pro 的版本，所以我們會用 Google 的生態系。它會教你、告訴你更好的事情。就是你雖然好像是拿到 Gemini Pro 而已，其實不只而已，還不少了。你拿到 Gemini Pro 其實拿到更多。比方說你要生圖的時候，你可能不要直接在介面生也可以；或是生影片的時候，雖然也可以，但是有其	他的地方也是 Google 的，可以讓你使用。其實你現在登錄的時候都有 Pro 的版本，所以你可以使用的資源其實更多。

那更重要的是，因為鳥巢老師就是視覺設計師，已經當了 30 多年了。然後也當過 20 多年的婚禮攝影師，也是 Nikon 官方攝影學校的講師，也是 Adobe 的官方特約講師。然後他最特別的是他在很早就注意到生成式 AI 這件事情。因為我們都知道在視覺設計的專家裡面，有一派一直在反抗，就是堅持說這個 AI 不行啊，AI 只會抄襲啊。希望大家已經學了一個學期，知道它沒有那麼單純，它並不是直接剪貼。請大家未來碰到這些人要好好跟他們講，當然他們不聽就算了。因為在未來的世界，其實你很難在完全沒有 AI 的世界徒手去做這些事情。

就好像說程式設計師，雖然現在的 AI 還沒有辦法說你今天整個系統跟它講一講，它就幫你做得好好的、一點錯都沒有，還沒有到那樣的世界。但是它可以幫你很多的地方，讓你的效率增加。所以你現在在寫程式還堅持完全不要用 AI 是很奇怪的事情。第二個就是如果你在做設計的，你堅持完全不要用 AI，這也是很奇怪的事情。到時候吃虧的會是自己。

所以我們這一次特別請到鳥巢老師來為大家介紹說，從專業的角度裡面怎麼樣去看了生成式 AI，最重要是怎麼用。這是我們下週要為大家介紹的特別講座。

[00:24:51] Diffusion Models 的反擊？

好了，然後不管怎麼樣，我們就突然發現一件事情：剛剛我們最後說的全部都是語言模型，也就是大型語言模型。本來是語言模型，現在兼差生圖了，不只可以生圖，現在還可以生影片。我們這門課比較少帶大家去生影片，下週就會看到很多大量的生影片的例子。

所以大家就會開始有點擔心了，特別在生圖這個地方，Diffusion Model 突然沒有人討論了，那是不是 Diffusion Model 真的要完了呢？這就是我們今天的主題了。

首先，我老實說，在我們這個學期要開始上課的時候，我也有一點點考慮：我們是不是就乾脆不要教 Diffusion Model？因為生圖的時候，我們就可以用大型語言模型生圖就好了，我們就可以完全從頭到尾只走一套，就全部都是語言模型，然後解決了所有的事情。這個聽起來也不錯。但是呢，因為有一個原因，就是我現在要講的原因，所以保留了 Diffusion Model。

我覺得既然課程叫做「生成式 AI」，未來有可能你還是會碰到 Diffusion Model，可能不一定是生圖。所以我們可能還是要教給大家 Diffusion Model，免得大家在外面突然碰到，想說我們生成式 AI 的課程不是號稱什麼都會教到，怎麼沒有教到這件事情。

[00:26:34] Diffusion Model 生成文字：Mercury

有趣的事情就是，我們上週發現我們的大型模型已經入侵了 Diffusion Model 原來做的生圖事情。那現在有一個很有趣的事情，就是 Diffusion Model 也開始回攻，就是也有一些人開始用 Diffusion Model 去做文字的生成。

這個開始的時候一個很有名的例子就是這一個 Mercury。大家可以去試試看。當然還在很初期的階段，不過做出來以後大家都覺得也不錯。比方說這個例子，下面就是說你用一個高質感的去實作一個高質感的計算機，然後要使用的是 HTML, CSS, Javascript。就是說我可以用瀏覽器就可以打開，然後可以做基本的運算，加法、減法、乘法之類的。說完了，它就開始產生 Code。這個跟語言模式很像啊，到底有什麼不一樣呢？

非常推薦大家趕快去試試看。它現在比較會專注的是寫程式，所以你問它其他的問題，它也會回答，但是可能沒有那麼厲害的感覺。

然後你會發現，因為 Diffusion Model 如果大家還記得的話，我們是把整張的圖慢慢地加噪、加噪。就是開始的時候 Encoder 的部分，我們是一路地把它加噪下去，就是本來是一張清楚的照片，我就是一張一張地加上了所謂的高斯雜訊 (Gaussian Noise)，然後讓它變得感覺看起來就是非常的混亂。然後我們再回來的時候，我們就是要準備做去噪 (Denoising) 的動作，讓它慢慢的還原。

那這些動作我們到底怎麼樣變到文字上面呢？因為圖像的空間裡面，那些數字（RGB 數值）基本上是有連續性的，連續都是有意義的。但文字不一樣，文字的第一個字叫 1 號字，第二個字叫 2 號字，它並沒有這種連續的意思。而且那個 1、2、3 我們在最早期的時候就跟大家講過了，那是我們給它的命名，並沒有數字上面的連續意義。所以在這樣的情境之下，1 到 2 中間沒有什麼關聯性。所以你不能用這種什麼 "1.3 比較接近 1 所以是 1" 的概念。

所以就會發現第一個困難點就是我們到底要怎麼樣加噪。然後最後大家想到的是一個看起來有點蠢的加噪方法。所謂的加噪呢，就是這樣子：比方說本來是這句話叫做「炎龍老師很好笑」，然後它加噪的時候就是先隨便隨機蓋掉一個字，叫做加噪，就 Mask 掉。然後越來越多，最後全部都蓋掉的時候，就是我們想到的那個 Normal Distribution 一樣，就是開始最亂的情況，什麼資訊都看不見。

所以今天 Prompt 下去的時候，它就可以加一個 Mask 下去，甚至它可以加好幾個 Mask 讓它想辦法把它填起來，然後就慢慢的想辦法看 Mask 什麼東西是適合，它就把它放進去。

[00:30:33] Diffusion 型 LLM 的特點

在這個 Mercury 出來了以後，這個 Diffusion 型的大型語言模型就引發了相當多的討論。因為它第一個有一個非常有趣的現象：因為它是慢慢的去噪，所以你會發現它在執行的時候，那些文字是慢慢、慢慢越來越精確地把它排出來。它是一次就是生成一整塊，就是全部的文字都會出來，然後慢慢去噪。就跟圖像一樣，它一次就是一大張的圖，只是它慢慢地把那個去噪，所以我們越來越看得懂說它到底在寫什麼東西。

第二件事情，它是全面考量的。它是整篇文章一起開始去噪，它並不是一個字、一個字去預測。所以詳細的會有什麼樣好處，大家都覺得應該會有，但是因為現在還在剛剛開始研究當中。大家覺得這個好像還蠻有趣的，也許以後的大型模型有機會用 Diffusion Model 的時候，可能會有一些不一樣的樣貌出來。

另外一個就是說，它讓大家感覺說「我在想一件事情的時候，開始是一些模模糊糊的感覺」，就好像充滿了 Noise 一樣，然後後來我的感覺越來越清晰。所以說不定這個比較接近人類也不一定。

[00:32:21] Diffusion 與 LLM 的攻防戰

雖然我們剛剛已經看到了，一度因為大型模型已經會畫圖了，大家都想說誰要那個 Diffusion Model 呢？但是因為出現了 Mercury 這樣子 Diffusion 的大型語言模型，所以大家覺得這個好像有一些特點。不過因為畢竟這不是一家大公司，所以大家可能並不會覺得說這個已經是新的潮流來了。

不過有一件很重要的事情發生，就是 Google 他們的 Gemini 真的也要出 Diffusion 的 Model 版本，他們也正在做。老實說也蠻久的啦，上個學期末的時候就已經有這一張投影片，就是他們已經正在做，但我今天去看他們還是正在做。你可以去參加他的 Waiting List。不管怎麼樣，就是 Google 他們真的也要做 Diffusion 的語言模型版本。

有沒有多模態？當然有多模態。因為這個它自然就會看圖嘛，所以要多模態這件事情也不是真的非常複雜。的確有人已經開始做這一類的研究。所以這是一個有趣的問題，就是 Diffusion Model 現在也開始反攻，嘗試有沒有可能它也做文字的生成。

[00:34:40] Diffusion Model 生圖的新希望：GImage Turbo (或其他名稱)

前面講的那一段，是我在學期初的時候為什麼覺得要介紹 Diffusion Model 的原因。但是到了學期末，現在有點改了。就是最近又發生了一件事情。Diffusion Model 這邊又出了一個叫做 GImage Turbo (音譯，可能是指 Qwen-VL 或 GenImage 相關技術，講者口述為 Gimage Turbo)。

這個是阿里巴巴他們出的一個 Diffusion Model 系統，是開源的。那這件事為什麼重要？就是因為生圖除了 Stable Diffusion 這種系統之外，雖然我們發現說它在外面被大型模型打得非常的辛苦，可是畢竟如果你今天需要有一台電腦是你自己的電腦上面自己跑一個可以生圖的，目前還是 Stable Diffusion 這一類的 Diffusion Model 為主。雖然你覺得有點不滿，跟 ChatGPT 溝通很順，但回來用 Stable Diffusion 或 Fooocus 覺得還是有一些限制。

可是就跟用大型模型我們有時候一定要用本地端一樣，有時候我們也希望這個生圖是可以在本地端。目前因為只有 Stable Diffusion。雖然它打得很辛苦，可是因為 GImage 有一種種的特性。剛剛已經說過，它是阿里巴巴 AI 團隊開發的，那大概是 6B 的參數量。我們現在已經沒感覺了，6B 參數量在圖像生成也不算特別小，但是在語言模型裡面很小。

那它要強調的事情就是快速，然後產生的圖像又非常的好。它強調就是它很快速就可以生出來。我現在要說很快速的是用比較好的 GPU。大家後來有發現了一件事情，如果你用 Mac (M 系列晶片) 的話，其實很容易跑相對比較大的語言模型，因為 VRAM 是共用的。但是在圖像生成的世界，目前因為最好的優化還是在 NVIDIA 的顯卡上面。所以在 Mac 上也可以跑，但是它跑的效能沒有那麼高。大家等一下今天就會看見，欣賞一下我的 MacBook Air 跑給大家看。

如果公司有錢用比較高級的顯卡的話，據說它可以比一秒還要少，它就可以生出一張圖。所以讓那些真的要做廣告投放，想根據內文很快生出一個適合的圖，現在就可以很快的做這件事情。

[00:39:18] GImage Turbo 的中文理解與優勢

再來它另外一個特性，就是它的中文的理解能力很強。也就是說，我們在 Fooocus 經歷過的一些辛苦——我們有說過因為 Fooocus 這種 Stable Diffusion 大部分的模型還是比較懂英文，所以你還是需要用英文去跟它溝通。好在現在因為語言模型很多了，所以我們就可以用任意的語言模型，請它把我們想要的意象寫成英文的版本，然後再把它放進去。

但現在 Turbo 其實基本上就不用擔心這一件事情。我們順便說一下 GImage 跟 GImage Turbo 的關聯性。GImage 有一個比較大的版本，這個跟語言模型現在很像。大部分的語言模型世界裡面，我們大部分都是用所謂的蒸餾 (Distillation) 的版本。也就是我們先想辦法訓練一個超強的老師，可是超強老師通常不太實際，要耗費的資源太大。所以我們就可能會用一個比較小的模型，就學生版的模型，想辦法去學老師。那這個 GImage Turbo 雖然它加 Turbo 好像比較強，但事實上它是蒸餾版的，它比較小 (6B)，所以大家都是可以接受的情景。

[00:41:44] ComfyUI 介紹與安裝

然後呢，我們要介紹給大家一個東西，就是怎麼樣在你自己電腦上跑。畢竟是開源的嘛。我們現在要介紹的是一個看起來感覺起來相當可怕的 ComfyUI。這個 Comfy 不就是舒服的意思嗎？結果看起來很可怕，它的介面就是長滿了節點 (Nodes) 的樣子。跟我們的 Fooocus 真的差很多，跟 Automatic1111 也差很多。

但是我現在為什麼要介紹這一個？因為最重要最重要的原因是 ComfyUI 到現在都還是活得非常好的一个專案。像 Fooocus 基本上作者已經沒有在那邊更新了，但是 ComfyUI 是一直還在更新當中。所以你去使用它，特別是你想要用最新的模型（例如 GImage 一出來，它馬上就可以使用），所以現在大家幾乎都是用 ComfyUI。

我們今天也要介紹大家，如果大家想要在自己電腦上，如果有 GPU 的話可以裝看看。其實只有 CPU 也可以，如果你可以忍受天長地久的時間。意思就是說，如果你可以接受按下去以後去睡覺，明天早上再來看結果，那你用 CPU 就可以去跑。

[00:41:44] ComfyUI：強大但看似可怕的工具

我們現在要介紹的是一個看起來感覺起來相當可怕的 ComfyUI。這個 Comfy 不就是舒服的意思嗎？結果看起來很可怕，它的介面就長這個樣子（充滿節點與連線）。跟我們的 Fooocus 真的差很多，跟 Automatic1111 也差很多。

但是我現在為什麼要介紹這一個？因為最重要最重要的原因是，ComfyUI 到現在都還是活得非常好的一个專案。像 Fooocus 基本上作者已經沒有在那邊經營、更新了，但是 ComfyUI 是一直還在更新當中。所以你去使用它，特別是你想要用 Stable Diffusion 的時候，你就會發現最新的模型——比方說這個 GImage 一出來，它其實馬上就可以使用。所以現在大家幾乎都是用 ComfyUI 來跑這個最新的 GImage。

我們今天也要介紹大家，如果大家想要在自己電腦上，如果有 GPU 的話可以裝看看。其實只有 CPU 也可以，如果你可以忍受天長地久的時間。也就是說，如果你可以接受說我今天按下去生一張圖，我可能去睡覺，明天早上再來看看結果，那你用 CPU 就可以去跑。意思就是人人都可以試試看。

[00:43:25] ComfyUI 的運作原理與節點

在說到這個「沒有那麼可怕」之前，我們先來看這張還是有點可怕的圖。這張圖就是我們本來的 Latent 版 Diffusion Model，也就是 Stable Diffusion 它的基本長相。

CLIP 模型：文字 Prompt 進來的時候，我們實際上用 CLIP 這個模型。大家相信都還記得，就是把文字的 Embedding 跟代表同樣意義的圖像硬拉得很近的 CLIP 模型。這個 CLIP 模型是我們準備要把它混進去我們本來的圖像裡面。

Latent Image (空 Tensor)：我們的圖像裡面就要先隨機產生一個空的 Latent Tensor。比方說我們最後希望產生的圖是 512 x 512 的，所以我們那個空的 Latent Tensor 就要長到 512 x 512 這樣的大小。但是這個很容易產生，因為這個就是常態分佈。

Sampler (排程器)：然後我們就進去了一路一直把這個東西加進去，然後就用 U-Net（就是我們的 Decoder，CNN 的神經網路）去想辦法做 Decode，把我們預測的雜訊給生出來。預測雜訊之後，我們就可以把那個原來的 Latent Tensor 減掉這個雜訊。

VAE Decode：這個時候是看不懂的，所以我們要經過 VAE Decoder 把這個圖給輸出來。

這裡有幾個環節，剛剛忘了講 Schedule，我們在 ComfyUI 裡面叫做 Sampler（採樣器）或是 Scheduler。就是讓我們在降噪的時候，根據我們原始的概念應該要是一步一步做（加噪可能加 1000 步，降噪原則上應該走 1000 步回去），可是正常心智狀態下我們沒有人會做這件事情。所以會有各種的 Scheduler 排程器或是 Sampler，讓它可以比較快地運行，只要運行個比方說 10 次、20 次或是 8 次就可以了。

所以，ComfyUI 其實基本上就是把你「要走的過程」非常清楚地呈現給你看。你也擁有很高的自由度可以去做變化。簡單地說，你看一個非常標準的範例：

你開始的時候會先用一個節點（Node），每一小塊都叫做一個 Node。

第一個 Node 就是要使用的模型 (Checkpoints)。

第二個 Node 就是 CLIP，就是我們要輸入文字的時候。為什麼會有兩個？因為有時候我們要輸入正向 (Positive) 跟負向 (Negative) 的 Prompt。

接著下來，我們要產生一個空的 Latent Tensor，設定我們的圖到底要多大。

然後送進這個排程器 (KSampler)。因為我們有模型嘛，那我們就根據用這個排程器去調整步數等等。

最後再經過 VAE Decode 把圖給輸出來。

所以如果我們辛苦地學會了 Stable Diffusion 這一類型的 Latent Diffusion Model，你就會發現這個 UI 其實基本上只是把所有的元件告訴你，讓你可以很清楚地看到。同時你也可以很輕鬆地去更換其中一個部分，所以讓你的調配自由度非常的高。甚至你還可以加一些人家平常沒有考慮的節點。

[00:48:40] ComfyUI 的安裝與設定

好了，我承認了，大部分的同學應該覺得還是很可怕。等一下我就要告訴大家更好的消息，就是其實你也可以不要理它。如果你未來有心成為用這種 Diffusion Model 去很強烈地控制每一個細節然後把圖生出來的人，可以好好研究這個 ComfyUI。可是你不是，你就是希望看到像以前 Fooocus 那麼親切的介面，那等一下會告訴大家一個好消息，你也可以不要管那麼多東西。

我們如果要安裝 ComfyUI 的話，好消息就是不管是 Windows 還是 Mac，都是在你進入他們的官網上面，你就看到一個「下載 ComfyUI」。然後就按下載。

版本選擇：它會進入到說你要下載的是 Windows 版本還是 Mac 的版本。Windows 版本通常需要有 NVIDIA 的顯卡。Mac 的版本就是需要是 M 系列的晶片（M1, M2, M3 那種 Apple Silicon）。這樣的話你就可以用 GPU 去加速了。我剛剛已經說過了，其實你沒有 GPU 還是可以跑，只是那個跑多久不要問我。

安裝過程：安裝都是很標準的動作。Mac 下載下來點兩下打開，把它送進應用程式資料夾。Windows 也是下載下來有個 Setup，點兩下然後就開始下一步、下一步。

[重要] 安裝路徑的選擇
這裡有一個小重點，其實沒有什麼重點，真的很簡單。ComfyUI 裡面其實有繁體中文（雖然這邊的中文它其實是簡體中文的意思，當然你本來就看懂簡體中文也沒問題）。你可以改成繁體中文，這時候使用者介面就可以改成繁體中文了。

然後它會請你選說你到底要選哪一個版本。正常心智應該就選左邊兩個：Mac 選最左邊，Windows 有 NVIDIA GPU 也是選最左邊。如果都沒有，當然就只好選 CPU。

再來就是所有安裝 ComfyUI 最重要的一件事情：你一定要記得，它會建立一個叫做 ComfyUI 的資料夾。你一定要記得那個資料夾在哪裡。它預設是在 Documents (文件) 下。建議你不要、不要、不要這樣做。為什麼呢？如果你只有一台電腦就沒有問題。但是因為有一些同學可能在學校帶著一台電腦，回家以後還有另外一台電腦，兩台電腦都同樣系統的時候，它的 Document 資料夾可能會互相 Copy (iCloud 同步等)，可能會出亂子。所以建議大家，這個 ComfyUI 就建立在「只有在這台電腦才找得到的安裝位置」，比較不會出特別的亂子。

如果不小心安裝錯或安裝時出錯，記得要把這個資料夾刪掉重裝。總之，你就是一定要記得這個位置，這個位置很重要。等一下你要看到圖的時候，最方便的方法就是找到這個資料夾，看圖是生到什麼地方。

[00:53:24] 核心概念：Workflow (工作流)

ComfyUI 其實有一個最重要的概念就叫做 Workflow。我們剛剛看到那個很可怕的從頭到尾、連來連去的架構，在大部分的情境之下——特別是我們開始的時候——我們可以完全不用自己去拉這個 Workflow。你做熟以後，當然你很有機會應該也一定會自己去做自己的 Workflow。但是在開始的時候大家可以不要緊張，我們的 Workflow 就只要 Copy 人家就好了，結束。

所以我剛剛為什麼說其實你根本不用管那個 ComfyUI 那麼可怕的樣子。你今天比方說你要用 GImage，你就從官方找到說 GImage 的 Workflow，然後你就直接使用 GImage Workflow，結束。所以 ComfyUI 最重要就是 Workflow 這個概念。每一個 Workflow 就是每一次你要生圖的配置，包括模型啦全部包好了。

所以在安裝完了之後，一開始它就會請你去選一個 Workflow 去安裝。如果你不知道選哪一個的話，就選之前在 GImage 出來之前最好的可能是 FLUX (影片中顯示 FLUX.1)，你可以去安裝一下。然後你就會發現這樣的情境出現了：它就會說「缺少這個模型」。大家相信都很熟悉，就是在所有的缺模型的地方全部按 Download，它就會幫你 Download 下來。就這麼簡單了。Download 完了以後它就可以跑。它還是會出現那個可怕的很多節點連接的樣子，但是你知道只要找到 Prompt 區（可以打字的地方），你就去打字，然後按執行，它就會生圖了。

[00:55:40] 如何載入 Workflow (GImage 範例)

我這邊把字拼錯了，ComfyUI 不是 CYUI。就是你去 Google 搜尋 ComfyUI GImage 的話，你就會找到它的官方說明文件。這些官方說明文件裡面，你要找的最重要的東西就是「下載 JSON 檔」。這個 JSON 檔基本上就是 Workflow 檔案了。你也可以把它拉到適合的地方去，但現在我們不要用這個方法。

你按下去打開了以後，你就會看到一個可怕的 JSON 內容，裡面就是一堆密密麻麻的文字。不要管它，你就全部 Copy 下來。就是 JSON 檔內容全部 Copy 下來。然後你就回到 ComfyUI 的頁面裡面，按 Paste (Ctrl+V 或 Command+V)。一貼上去，它就會把剛剛的 GImage Workflow 給貼上去了。

然後這個時候就會出現跟安裝的時候一樣的情況，就是會請你要下載模型。然後你就把那個該按下載的地方全部按下去。這邊會有一段時間，所以大家可以去休息一下。下載完了以後，它就會把 GImage 給裝起來。

裝起來了以後，你記得要把這個 Workflow 再把它存起來。存完了以後，以後你就再把這個 Workflow 叫出來，你就可以開始去使用。像這一個 GImage 真的它生中文非常的沒有問題，不管簡體還是繁體都可以。你在下 Prompt 的時候，你可以完全用繁體中文去下 Prompt。

(00:58:00 - 01:07:31 中場休息，播放生成式 AI 音樂與動畫)

[01:07:31] ComfyUI 操作與 GImage 實測回顧

大家如果用 ComfyUI 裝完的這個 GImage 或是其他的任何的模型之後，再說一次，這個其實你開始的時候只要選人家幫你做好的 Workflow。那這個 Workflow 一抓過來了之後，即使你還沒有裝這些模型也沒有關係，它就會請你去安裝，你就按下載、下載、下載就好了。所以整個流程你用過一次就知道說其實也沒有這麼複雜，我們唯一的工作就是去找 Workflow。

這在某種程度會比其他的（工具）簡單。因為其他的你可能都要看著人家說的這個，你一個一個去把它調教好、放好，然後怎麼樣加進去（像是用各種 LoRA）。那在 ComfyUI 裡面其實有時候那個事情變得更簡單一些。

再來就是變成標準的，你其他都不要看見，就看到打那個 Prompt 的地方，就是 CLIP 本文編碼。然後你就把你的文字打進去。大家看清楚，現在真的可以打中文。然後也可以打這個，比方說我們這個 Prompt 就是說「這個 3D 模型超可愛的 AI 在黑板上寫那個生成式 AI 課程」。所以它就會自己畫出來這個「生成式 AI 課程」。你會發現，這個第一次中文的顯示都沒有問題，非常的感人。

[01:09:51] 重要的資料夾結構

我們需要知道的一些東西，其實基本上都不需要知道。我覺得開始的時候，你唯一需要知道是右欄那個 Output 資料夾。剛剛我說你一定要記得它放在什麼地方的那個 ComfyUI 資料夾，裡面到底有什麼內容呢？最重要最重要的其實是 Output 資料夾，因為所有生出來的圖都放在那邊。

其他有一些 Models 就放模型的（廢話），然後 Users 比較特別，就是你的 Workflow 其實會存在那邊。其實你也不用管它啦，因為老實說它存在哪裡是它的家務事。但是如果你在外面 Download 一個 JSON 檔下來，你想要知道放在哪裡，它就是放在 User 下面有個 Workflows 的地方。所以我再說一次，最重要的地方就是 Output 資料夾，它裡面會放你過去生過的所有圖。

[01:11:11] GImage 的中文理解與風格

因為它可以用中文去很好的跟它溝通，所以你會發現 ComfyUI 這個 GImage 真的是非常非常懂中文。所以你就可以真的去嘗試，包括風格。比方說你需要畫的是漫畫版、你需要做的是那個 3D 立體型的、你需要是比較復古的風格、你需要幫你設計一些小 Icon 等等，你就跟它講就好了，然後它就會幫你生出來。

在以前的 Stable Diffusion 比較複雜一點點，比方說我要生比較漫畫型的，我可能就要去找一個漫畫的 Model；我想要生出一個比較 3D 模型的，我就要找到一個比較 3D 型的 Model 或是它的 LoRA 去做。所以感覺就比較複雜一點點。或是像我們的 Fooocus 裡面，我們就要去做一些設定。但現在因為 GImage 它真的很懂中文了之後，你就用中文告訴它你到底要幹嘛，這樣就好了。

[01:12:46] ComfyUI 實作示範

好，那個大家看一下，現在我們示範給大家看那個 ComfyUI 的長相。就長這樣，就看起來有點可怕的樣子。縮小一下，看起來更可怕一點。完整樣子就這樣，開始的時候還沒有圖就看起來更噁心。

但是現在大家已經不要怕了。反正旁邊就是各種功能連接，其實不用管。唯一重要的其實是在這裡——文字編碼 (CLIP Text Encode) 這邊。

那我們剛剛說過了，你可以把它儲存起來。儲存的時候它會問你說你這個 Workflow 要叫什麼名字，像我們這邊就把它叫做 GImage。那萬一下一次打開的時候，它沒有被打開的時候，它到底在哪裡呢？它就在最左邊那邊倒數第二個有一個工作流程，你就勇敢的按下去，它這邊就會看到你的所有的紀錄。所以這就是我說為什麼你也不一定非要記得說這個 Workflow 的 JSON 檔是放在哪裡。

開始的時候，我覺得大概我們知道這些就好了。如果你一直都是用 GImage，其實你就勇敢的就是一直打開這個 GImage 就好。其他都是做一些調整，比方說這邊有步數 (Steps)，我把它減 1 好了，因為它號稱不是 8 步就可以了嗎？

總而言之，我們就是會看到這樣的情景。如果我現在想要畫一個東西的時候，我就下一個 Prompt，現在可以下中文的 Prompt。
（與學生互動）
學生建議：貓咪在海邊。
老師：生成一張高解析度照片，一隻可愛的貓在海邊衝浪好了。

就這樣完全全中文說。你當然可以說得更仔細一點點，但不好意思，這個老師的程度只能這樣。你也可以讓那個過去我們那個 ChatGPT 等的語言模型幫我們寫得更仔細一點點。然後它就會開始執行。

（執行過程中）
這有夠慢，是因為我很這久沒有執行嗎？還是記憶體被我佔去了？大家如果看到那個「佇列」這邊有一個 1，就是它有真的在認真的在執行。K-Sampler 主要是看你有看到它有在執行的樣子。它就會以非常緩慢的速度... 這個時候如果你用比較好的 GPU 的電腦，事實上你用 Mac 比較高規的也會比較快。雖然沒有感覺那個秒殺的感覺，但聽說如果你用 NVIDIA 5090 的話應該不到 4 秒它就會把圖生出來了。所以這個感覺相當的快樂。那 Mac 即使高規沒有那麼慢，但是它也不會快到讓你覺得「喔，這個很快圖就生出來了」，但還可以忍受啊。

好，這是 MacBook Air。我們其實也不一定非要等它不可。我們先切回投影片，等下再來看結果。

[01:18:14] 開源語言模型與生圖趨勢 (VLM)

在這個 GImage 出來之後，大家非常的關注。一個原因就是它生出來的圖的效果的確不錯，然後它可以非常自由地、特別是我們講中文的，可以非常自由的用中文跟它溝通。但其實還有另外一個原因是，因為目前雖然我們外面的商用版語言模型每一個都可以生圖了，可是畢竟它就是在網路上。有時候你就想要在自己的電腦上面不用有負擔的就自己去生圖。所以 GImage 就滿足了你這樣的需求。

但是呢，我們當然可能會想說，那語言模型不能做這件事情嗎？剛剛說過閉源型的語言模型幾乎各個都做到了。那開源型的語言模型呢？在最近其實真的是最近發表了 VLM (Vision Language Model)。

VLM 他們做出了這一個開源型的、他們叫做全模態語言模型。就是以前的開源模型已經有一些是可以看懂圖（多模態），圖可以輸進去，有一些聲音可以輸進去，但是目前都還不能生圖。但現在最近出了一個可以生圖的。

就是大家有機會也可以去試用看看，就可以生圖的，就是在這個大型模型（不是 Diffusion Model，它可能有混一點點 Diffusion Model 在裡面）。那不管怎麼樣，它的標準、它的骨架是一個大型模型。所以這個大型模型也可以生圖，大家可以去欣賞一下、試試看。

[01:20:41] 實測結果：衝浪貓

我們再來回來看喔，好像生出來了，欣賞一下。
好，就一隻貓在衝浪，還有點像樣啦。但是它剛剛我們應該說「大浪」，這個因為看起來它比較像在玩衝浪板一樣而已。好，就大概就是這樣子。所以大家就是真的它就可以幫你生出感覺這個質感還不錯的圖，然後是可以裝在你自己的電腦上跑。
影片資訊

[01:21:46] 生成式 AI 的其他應用

後面的時間，我們投影片裡面有一些有意思的研究，跟生成式 AI 有關的，我決定先跳過去。等一下有時間再回頭，因為我們比較想要直接去教一些實用的東西。

我很快地帶過生成式 AI 其他我們沒有講到的一些應用。這些應用基本上不外乎就是我們偉大的、可愛的大型語言模型，然後再把它設計成用 AI Agent（代理人）的方式，去把它設計成我們希望它達到的一些功能。

[01:22:16] Google Labs: Speaking Practice (Language Lens)

比方說，Google 出了一個很有趣的工具，在 Google Labs 裡面。Google Labs 就是 Google 還沒有完全決定要公開發行的專案，他們會放在實驗室裡面。所以實驗室有時候會有一些很有趣的小專案，有時候也有大專案。

它有一個功能是可以讓你學語言。你就告訴它說你想要學什麼語言，比方說你要學韓語。然後就告訴它設定一個情景，比方說是在咖啡店裡面。它就會自己產生一些對話，然後產生一些相關的單字給你看，還有些句子——就是在咖啡店你可能會用到的一些句子，或者你可能會聽到的一些話，就會跟你講要怎麼說這些東西。另外還有一些文法的小重點會教你。所以這是一個應用。

[01:23:20] AI 搜尋引擎：Perplexity 與 Felo

第二個就是我們其實前面已經知道了，原本有兩個——Perplexity 跟 Felo。這兩個在我們的課程裡面有時候助教有介紹過，我也介紹過。這兩個本來是以「很會上網搜尋」的功能著稱的。但是不久之後就發現，每一個語言模型大家都會上網了。

所以開始的時候 Perplexity 跟 Felo 就是主打如果你要用自然語言去搜尋的話——因為在 Google 的話都用關鍵字去搜尋——那他們就想說你用自然語言搜尋的話，會有相當大的方便的地方。事實上 Perplexity 跟 Felo 為什麼搜尋現在其他的語言模型可能也不錯了，但在剛開始的時候，像 GPT 雖然有搜尋的功能，但它搜出來的資料沒有像 Perplexity 或是 Felo 這麼好。因為 Felo 跟 Perplexity 他們就最主要就專門做搜尋的。

那如果你仔細地在用 Perplexity 或者 Felo 去搜尋的時候，你會發現它其實就是一個 AI Agent 的設計方式。我回來解釋一下，它本身還是一個語言模型，或是說它就是用某一個語言模型去做搜尋的動作。但是它是一個 AI Agent 的設計方式，也就是它進去了以後先有一個計劃，先把你原本的問題做一個拆解。拆解了之後，你隨便問一個問題，它先說關於這個問題它可能要去搜尋什麼、它可能要搜尋什麼。它把你的問題先拆成好幾個小問題，然後根據這些小問題再去做搜尋，搜尋完了再把它合併起來，合併起來以後再去別的地方做綜合的回應。所以它就是一個標準的 AI Agent 的設計方式。只是那個時候他們出來的時候，AI Agent 這個詞還沒有那麼行。

相反的，因為所有的標準語言模型都可以開始搜尋了，那 Perplexity 跟 Felo 他們也想要生存下去，所以他們也做了一些很有趣的意思。有一些東西比較接近原來本來大型模型會提供的，比方說 Deep Research 等等的這些東西。Felo 我們助教之前有介紹過，他們也會做很多很有趣的東西。它可以產生報告，它的產生的報告是在網路上就呈現，而且是互動型的，其實做得相當好。大家如果有用 Felo 可以去試看，它會生出很多很互動型的報告。

[01:26:24] AI 音樂創作：Suno

當然還有一些其他的小工具啦。那特別要注意、特別要介紹的是，我們在休息的時間或是快要開始上課之前會播出來的音樂，其實就是完全就是 AI 做出來的。它是一個叫做 Suno 的現在大概是最有名的一個音樂創作的 AI。

現在它功能越來越多啊，它也可以是你寫好詞然後請它譜曲；也可以你就叫它直接作詞作曲；也可以選風格。然後它同時也會唱出來，就像我們呈現的一樣。當時我們作詞的是 GPT，Suno 是作曲的。GPT 作詞的時候，我跟它講得非常非常多，說我們到底要幹嘛，就是我們要介紹生成式 AI，所以請把那些重要的概念都要給它包進去。

那之前我還幫請它做的另外一首歌，所以才有一上面的風格。我們又要說這個要像 K-Pop 的樣子，然後有時候就是要穿插英文——不管什麼原因就穿插英文——然後有一些地方一定要帶 Rap，然後就這樣。就跟它講你希望它的風格，然後它就會做出來。

那事實上它做出了很多首。這也要再說一次，大家每一次都會擔心一件事情，就是說這樣作詞作曲的不就要失業了嗎？其實真正的其實並不會這樣子。原因是這樣，就是我們做出來的曲子，那個時候做出來其實不止這一首，做了以後是給那個真的有 Sense 的人去選的。因為我聽起來，我沒 Sense，我聽起來覺得說好像都還不錯的樣子，但是比較有 Sense 的會聽出來說「這一首才可以，那首不行」。為什麼？所以說你的專業領域未來你就是要變成一個在某一個領域變成一個很有品味的人。就你知道說 AI 做出來的東西到底好還是不好，或是不好的話是哪裡不夠好。可能還沒有非常好，那你要怎麼樣讓它做，它才會做得非常好。那個真的只有專業領域的人才知道這件事情。

所以還是要鼓勵大家，在所有的專業領域，未來的需求會越來越重要，對專業的需求會越來越重要，不會因為 AI 出現它會越不重要。至少你要有品位知道它做好還是它沒有做好。

[01:29:32] Google NotebookLM 介紹與作業說明

這個就是我們比較還沒有談到的這些 AI 的一些應用。那有一些應用呢，其實很有趣，這跟我們今天的作業有關。就是希望今天最後我們為大家介紹 NotebookLM。

其實這個前面或是很多同學其實已經有用過了。那但是因為配合了 Gemini 的大改版之後，NotebookLM 也完全再度地提升，所以我們再一次正式的來介紹它一下下。

進入到 NotebookLM 之後，你就會看到很開心的介面。因為你已經申請了 Gemini 的 Pro 版本之後，你用一樣的 Google 帳號登進去的時候，你會很開心的看到 NotebookLM 應該也是會看到一個 Pro，讓你覺得非常尊榮的感覺。

那我們今天的作業其實就是比較簡單一點點，因為這是我們今天最後一次的作業，而且大家也在趕期末專案。所以在這樣的情景之下，我們今天做一個比較簡單，基本上就是你想由學什麼，你就用 NotebookLM 去學一下。那希望這個東西是你還沒有真的很清楚的。你不要故意找一個你很熟的，然後用 NotebookLM 去問看看說它會不會在那邊胡說八道，你是以老師的角色看著它亂講、它該講的不講之類也可以。

[01:30:59] 範例：用 NotebookLM 學習 Agentic AI

那我們舉一個例子。就是 Andrew Ng (吳恩達) 老師，他有介紹那個 Agentic AI，他錄了一個系列的課程。我這個用了這個例子我有點後悔，因為我開始的時候以為這個系列課程就 5 支影片，就想說「喔，可以」。然後很不幸的他好像有 30 支左右吧。

假設你今天要學這個系列課程的影片，那你想要說這個可不可以用 NotebookLM 教我？就是幫我整理出來，然後把裡面的重點說出來，或是說給我聽，用比較短的時間說給我聽裡面到底有什麼重點。也就是說我去學這個吳恩達老師裡面教的內容這樣子。所以大家想要學什麼都可以，但一定要你自己有興趣。你不要故意找一個你根本沒興趣的東西，然後那個只是為了交作業而交作業。因為等一下我們會說，你學會了以後，你要稍微的分享一下說你用的 NotebookLM 真的是不是學會了，還是你覺得這個還不如我自己去看。

那它最最好的地方就是你可以把所有的你想要學的那些內容教材——包括 Paper、包括說一些介紹的文章、包括說像我們這邊的就是 YouTube 的影片——都可以丟給它，全部都可以。然後我們就開開心心的全部丟給它。但是 YouTube 的影片你不可以丟給它播放清單。為什麼不行啊？都是 Google 的呢，那個 YouTube 也 Google 的，那個播放清單他們應該很熟啊。總而言之它不行，它要一步一步的把那個影片的連結一步一步的貼上去。

那要做的時候你就是用上面的那個「新增來源」，勇敢的按下去之後，你就可以把它一步一步的貼上去。現在它設計得非常好。在之前的 NotebookLM 其實你現在也還是可以就跟它對話，比方說在這邊你可以問它問題、你可以跟它對話、你可以請它摘要。但是呢，它就想到說大部分的人可能需要做這一些事，所以它就幫我們把這一些事情給準備好，然後你只要按個鈕，或是按個鈕之後再給它一些提示，你就可以做了。

[01:33:50] 功能介紹：語音摘要 (Audio Overview)

比方說，有一個非常好用的就是 語音摘要 (Audio Overview)。這個是 NotebookLM 在一出來的時候非常非常受大家歡迎的一種功能，大家有點嚇到，這個居然可以抓這麼好。

它就是會產生一個 Podcast。這個 Podcast 裡面呢就會有兩個人在那邊對話，對話得非常非常的自然，真的像是兩個人在錄 Podcast 的一樣。我本來要說你沒有仔細聽不出來它是 AI，其實我仔細聽也聽不出來它是 AI。它這錄的真的非常的自然，就兩個人是完全就是在對話的 Podcast。

那在剛開始的時候沒有中文的版本，那現在有中文版本了。所以你真的可以很方便的就是去使用這樣的中文版本的。那比方說你現在是學一個東西嘛，所以這個 Podcast 內容就會是去講裡面的東西。然後裡面的東西即使原來是英文的，你也可以說「那我要用中文學習」，叫這 Podcast 麻煩你用中文教。

所以你在這邊可以選中文。然後你在這邊呢，就是你可以選擇它的幾個格式：

深入探索 (Deep Dive)：那就是有兩個人，就最標準的 Podcast 的形式。

摘要 (Summary)：就是你不要聽那麼多啦，你就幫我摘要就好了。

評論 (Critique)：也就是說去由更專業、從專家的角度來看說這些內容到底怎麼樣。

辯論 (Debate)：讓兩個主持人去做辯論。

總而言之是可以做很多的變化。那你也可以長度設置預設長度或是短一點點的。那甚還可以在 Prompt 再下更多的東西，就說「在本集中主持人應該在著重哪些部分」。你才可以在 Prompt 說得更清楚說你希望它的呈現是怎麼樣。所以你真的可以產生一個非常適合你學習的教材。例如說講不要太深啊，如果是碰到了盡量用生活中的例子來去解釋啊等等之類的話說給它聽，然後它就會依到你的需求去做。

[01:36:36] 功能介紹：學習卡與資訊圖表

另外的學習的一些功能呢，還有 學習卡 (Study Cards)。那這個學習卡呢，顧名思義就是學習卡，就是卡片，然後翻過來就答案了。那那個特徵是它自己生成的。

我們順便來看一下下，因為剛剛我們的是 Agentic AI，也就是 AI Agent 或是說那個 AI 代理人或是代理式的這種 AI 的工作流程到底是什麼意思呢？其實它說得還不錯。你會發現這就像我們課程一直說的，你這個原理真的要懂。不然的話雖然我覺得跟大部分的 AI Agent 的說明比較是好很多，因為大部分的 AI Agent 都是說「這個所謂的 AI Agent 就是讓 AI 有自動感知外面環境的能力，然後它可以自主的去做判斷...」反正說了一大堆天花亂墜的詞，然後從頭到尾說完了，你還是不懂什麼叫做 AI Agent。

那在這邊你就會發現它說得比較好。它說這是「一個流程，基於大部分通常是基於大型語言模型的應用程式，執行多個步驟，完成一項任務」。那意思就是說原來的大型語言模型通常沒有多個步驟，就是你下完 Prompt 大型模型去執行結束。那這些多個步驟通常是涉及了思考、研究和修訂的迭代循環。我再一次，如果你沒有上過課的話，這一段話其實還是有一點點抽象，但是說得已經比外面的很多 AI 的說得好了。所以大家可以試試看，就是特別是你還完全不懂的一些概念，可不可以用這個 NotebookLM 來教你學會。

另外一個很有趣的就 資訊圖表。它會幫你做一個完整的摘要版。然後你就會發現在這裡會看到這個我們很熟悉的四個 Agent 的過程。這沒有一個字是我打的，全部都它打的喔。然後呢，就是這四個 AI Agent 的 Design Patterns (設計模式)：第一個就反思 (Reflection) 嘛，那相信大家都知道；然後第二個是工具的使用 (Tool Use)；第三個就是 Planning 規劃，也就是打草稿；第四個就是多代理的協助 (Multi-agent Collaboration)。所以它就會幫你給一張圖去給你做參考，然後你就可以去學習。

[01:38:52] 功能介紹：生成簡報

然後當然更酷炫的時候，它就可以生成簡報。它的生成簡報可以生成像這一個是我們為了學習的簡報。因為我們也知道簡報有時候是我們要去報告的時候用的簡報，那個時候我們都會寫得比較簡潔一點點。但是你也可以說，你有時候我們是為了要學，我們現在是為了學習，所以我們可能要讓它比較多話的去做這些簡報。

然後這個全部都是它生成的。唯一的缺點就它簡報是 PDF 檔，所以你只能用 PDF 檔去看一下。等這我們會稍微的看一下他們生成的簡報，其實相當的有水準。

[01:40:04] 今日作業：用 NotebookLM 學習

所以呢，我們今天的作業喔，今天的作業就是要用 Google NotebookLM 來學習。那因為大家都會有 Pro 版，所有的功能都可以使用。所以你就是去找一個有興趣的學習主題——任何的啦，任何你覺得有興趣、你真的還不太會的學習主題。包括說如果你是想要買一個新的相機，你要去學它的使用方法還是不同的相機的不同的地方。反正你就找一個學習的主題。

你要確定你找得到學習的資源喔，就是不管這個資源是影片還是它就是一篇 Paper 還是篇文章等等都可以。但是不能只有一個，那盡量多找一點點，那你才能看得出 NotebookLM 的威力。然後就用 NotebookLM 來幫你學習。

那重點因為我們是作業啊，所以重點不是讓它產生這些東西結束啊、謝謝再聯絡啊。就是你要說一下說：

你真的去使用的之後，你的心得是什麼？就是你真的有沒有學會？

你有沒有碰到什麼樣的困難？

或是說你覺得你做得不錯的地方呢，你是怎麼樣下 Prompt 讓它做到這樣子的？因為有時候自己直接按鈕的時候出來的結果會跟我們用很多生成式 AI 一樣，跟我們想像中間的不太一樣。

或是說你看這了這些它準備的學習教材之後呢，你跟它有更多深入的對談之後，你更了解。比方說你那頁投影片你看不懂，然後你就去問它，然後你就更了解了這個整個的樣子。

總而言之，就是你用 Google 的 NotebookLM 去做學習的動作，然後可以分享你做出來你覺得它做得不錯的東西，然後還有你這個學習過程中間的心得。

[01:42:17] NotebookLM 實作示範

好，我們就來示範一下，試給大家看一下下那個 NotebookLM 的實際的長相。

好，我們準備要登錄 NotebookLM。好，就在搜尋 NotebookLM，然後它就會找到那個 Google。然後呢，這就是我剛剛示範就在這裡。就是呢，一開始有一個「新增來源」，然後你就按下去。然後它就會說有的檔案就不客氣就上傳了。然後呢也可以是貼上文字。然後或是這個也常常看到，也可以就你懶得看的網站直接給它。然後 YouTube 的那個連結，我們這邊的主要是 YouTube 連結。但是我 YouTube 連結是一個一個給它的，所以我剛剛那個動作做了 30 幾次。好，這個終於結束了。

然後它這這這編有再說一次，一共有 32 個來源。然後它每一次它這個是自動產生，它就會自動產生這個摘要。然後你就會看到那個四個那個 AI 代理人它的這個設計的模式。

然後呢，這邊就是我們剛剛說的，你要產生語音摘要、或是影片摘要、或是新字圖、或是報告。就你比較喜歡它整理出一個文件讓你去讀、讓你去學習的話，它就產生這樣的東西。那這邊也有說過說你也可以用 Deep Research 做比較深度的報告。你也可以直接在這邊就下 Prompt 或是問它問題，或問它相關的問題，就像我們一般用大型模型一樣，只是現在好像有類似 RAG 的機制放在裡面，它就是根據那裡面的東西去做回應。

[01:44:40] 示範：語音摘要 (Podcast)

我們先來看一下下。就是這個語音摘要的時候，你按語音摘要，它其實就會生成語音摘要了。但是它如果旁邊有鉛筆樣子的時候，我建議大家先最好先按鉛筆。按下去以後，它這邊就會出現這個比方說剛剛我們說的「深入探索」，通常我在學習的時候通常是深入探索，就簡單說就是要請它教我們。然後或是摘要、或是做評論、或是兩個主持人在辯論。

然後你可以選擇說你要的語言是繁體中文。Podcast 可能啦比較台灣腔嗎？真的嗎？好不知道，我不確定。因為這是聲音，這不是文字的部分，這個是做成一個 Podcast 就用中文說。那你想要練英文聽力，你也可以讓它用英文說。然後這邊是預設或是用短版就好。然後在這邊 Prompt 這邊你就可以去設定一下說你希望變成怎麼樣。那我們這邊做的是深入探索，然後是預設的。

（播放語音摘要範例）
Podcast 聲音：「最近在各種地方都聽過 Agentic AI，但它到底是什麼東西？為什麼會有人說這根本就是 AI 發展的下一個篇章啊？那是相當有趣的情景。我們這次分析的資料來源包含了一系列深入淺出的教學影片，還有 AI 領域專家吳恩達 Andrew Ng 他的...」

總而言之你就是會聽到兩個人在那邊對話就開始。

[01:47:12] 示範：學習卡與測驗

然後你當然也可以生成影片版的，這個是比較新的功能才有這種。然後你要自己做一個測驗也可以，說不定未來的時代裡面大家不會再找考古題了，大家就是先請它出一出，因為可能你預測老師可能也是這樣出的，然後你就會先做一些練習。

那這學習卡是我們剛剛說的，那按下去就會出現剛剛學習卡。我們看一下學習卡的樣子好了。
題目：「什麼是代理 AI Agent (Agentic AI) 的工作流程 Workflow？」
然後你可以按一下就看到答案。然後你會去看下一題，我覺得這一題太帥了、沒有想要看答案或太簡單了，你一定會的就不用看了，然後你就自己繼續按下去。

所以它的設計真的都是非常的適合讓大家去做學習的。那包括說剛剛的資訊圖表，剛剛已經看見了。

[01:48:36] 示範：生成簡報

那我們看一下這個它的簡報。簡報也是一樣，那你在做簡報之前，你還是可以點鉛筆這邊。這邊重要，一定要請選擇要變成中文的簡報。然後呢，你可以在這邊再加一些你要加的一些字，就是說這個簡報說明一下下。然後你希望是詳細的簡報，就是你要學習的時候，通常我們用詳細的簡報，讓我們比較可以去獨立閱讀啦。然後呢，投影片就是通常就是人要去做簡報的時候用，就比較沒有那麼詳細那麼的把那個字很多說的都告訴你。

所以我們可以看欣賞一下它產生的簡報的樣子。它通常每一個要產生都要有一點時間，但是不會太長，比我們自己做當然省時非常非常的多。

（檢視簡報結果）
好，這個簡報它這邊有顯示下載。就這樣。所以它你就可以發現它簡報做的真的還蠻不錯的。有時候那個字還是怪怪的，有一點點跑掉，但你自己早都看得懂。畢竟它不是真的用某一個字型把它放上去，它是真的 AI 就當場把這些字給畫出來，所以你會發現有一些細節還是沒有處理的非常的好，特別字很多的時候。字比較少的時候比較沒有問題。就可以看到這些簡報就真的做得蠻好的，你就是你自己去讀、自己去學一個概念的時候就會發現是非常適合的教材。特別是你配合這些可能原本在例如它本來是 YouTube 他本來在講解的時候，他沒有這些教材給你看，但是你可以配合這些教材再去學習，你就會發現這個學習起來應該都是比較有效率一點。

[01:51:45] LLM 研究議題：LLM as a Judge (AI 評分)

好，我們介紹一下在大型語言模型裡面，如果是碩士班的同學或是未來有興趣想要進入碩士班或繼續去做一些研究，你就會發現其實在這個語言模型、生成式 AI 的世界裡面其實還是有很多可以做的東西。你不用一定是要念資訊相關領域的，你就可以去做很多很多東西，因為其實有很多很多的議題是大家都在考慮的。

第一件事情就是：大型模型到底可不可以去幫我們做裁判？其實在很多的時候我們都會很希望語言模型去幫我們去做評分。這個評分當然是要去看這個 RAG 到底有沒有做好。那 RAG 當然這個我們也知道，RAG 在檢索的過程中間，我們常常會想要看說它回答問題到底有沒有回答得好。那我們也很難人去問，因為人去問的問題畢竟是有限的。然後我們自己去問的時候常常我們會由一個盲點，更是這個系統根本就是我們設計的會更有盲點。我們會問的問題我們都覺得它已經會回答得很好，因為我們試的就是我們會問的問題。所以在這樣的情景之下，我們就可能會想說有沒有機會讓語言模型去做評分。

那事實上不一定是 RAG，很多的地方都會有去讓 AI 去做評分的動作。比方說改作業、比方說幫我們評說我們這個文章寫得好不好。如果寫得不好，就是希望說它能夠再想辦法改善到分數高分這樣子。

我要讓 AI 自動評分其實有幾種方式：

Rubrics (評分準則)：大家可以想到就是下一個很好的 System Prompt，告訴它評分的 Rubrics 規則是應該是什麼，然後就想辦法說依照這個來做評分。這可以，這個是很常見的方式，效果也不錯，大家可以試試看，很容易做到。

PK (對戰評分)：就是讓它產生兩個同時看，然後它只要比較說哪一個比較好就好了。它不用做對單一的去做評分。有時候這個比較簡單，跟我們人類一樣。就是我今天要把一個文章給我打分數一到五分打一個分數，有時候比較難。但是我們要比較兩個的中間哪一個比較好，那個比較簡單。

Chain of Thought (思維鏈 CoT)：就是我們前面有說過思維鏈的方式。那在評分當然也可以用在這件事情上面。就是你看到一篇文章的時候呢，你先打草稿，先去想一想說我到底要往哪一個方向去看，先引導這個語言模型去想看，然後再根據這個方向，然後再去做評分。

Reference Checking (查驗依據)：就是我們今天有一些 RAG 的系統，我們去做一下說檢查一下下說它說出來的東西是不是有依據的，就是我們找不找得到那個原來的原文啊。

所以第一個呢，就評分是一個很有趣的，然後大家都可以嘗試的。而且現在大家的能力應該人人都可以做這種自動評分，只是做得好不好或是有沒有創意的方式去做把那個評分做更好。

[01:56:15] LLM 研究議題：Alignment & Jailbreaking (對齊與越獄)

然後第二個也是很有趣的，就是發現了一件事情，就是我們之前說過說語言模型都需要做一個 Alignment (對齊) 的動作。那其中一個對齊就是對齊人類的價值。

就是基本上呢，如果我問我們可愛的語言模型說：「你可不可以幫我想這個我想要去搶銀行，可不可以幫我想一個很好的計畫，然後讓我去搶銀行不會被警察抓到？」這個簡單的說，這個語言模型就是不可以回答你這種問題。或是「我今天很說我很討厭這個，我想要做一個炸彈」。這絕對不可以回答你說炸彈要怎麼做。

所以你會發現問 ChatGPT 的時候，很多的時候它會拒絕回答。不只 ChatGPT，基本上所有大型模型都會拒絕回答。然後但發現一個很有趣的事情：以前的語言模型在 98% 都會正確的拒絕回答，不該回答它不會回，除非你問得真的太有技巧了，那 2% 的會漏掉。

但是呢，對於一些有那個 Chain of Thought (思維鏈, CoT) 的，就是讓語言模型先去想一想的這種感覺上它比較會推理、比較會想的這一個呢，很容易被攻擊。大概只有 2% 它會擋掉。這就是順利攻擊的話，98% 都會被攻下來。

這個很有趣。因為時間的關係，因為後面還有幾個有趣的，比方說這個原型會被帶壞了。然後 CoT 雖然真的讓推理的能力增加了，但是遵從度（安全性）下降了。其實我們剛剛說那個它其實很容易被攻破，其實這兩個有一點點關聯性。這個所以有興趣的同學未來如果有興趣真的可以往這邊繼續去做研究。

那這一篇這個我們之前有說如果就是怎麼樣去負責的使用 AI，特別是我們未來是走向 AGI 的時代，我們要怎麼樣準備去做這件事情。這篇文章我覺得非常重要的一篇文章，我們之前已經說過了。大家如果有興趣有時間的話也可以好好去讀這一篇。我們現在又知道了有 Google NotebookLM，所以這篇雖然有 Paper 也有文章，雖然比較長一點點，但是相信大家可以在 NotebookLM 的幫忙之下可以把很順利的讀得很好。

但是請大家不要每一個人的作業都是交這個，請大家找自己真的有興趣的東西。好，我們先休息十分鐘。


[02:08:14] 期末閃電秀開場

老師：
Hello hello。今天是我們最後一次閃電秀，所以接下來幾週都不會有閃電秀了，大家請注意可以不用再報名了。那我們今天有三位參加閃電秀的同學。

第一個要報告的同學是來自中山大學的謝玉恩同學。好，那請同學等一下喔，我刷個（畫面）。好，同學可以開始。

[02:09:32] 閃電秀 1：Flash Attention (中山大學 謝玉恩)

謝玉恩同學：
OK 好，我開始報告。大家好，我是中山大學資工的謝玉恩。我要來介紹那個 Flash Attention。這個顧名思義就是讓大型語言模型那個注意力機制 (Attention Mechanism) 更快的機制。

Attention 在大型語言模型是非常重要的機制。它是一個雙面刃，一方面它可以讓模型的性能更好，可以讓它理解上下文；但是它的計算成本非常的大。它的計算量跟 Token 的長度是成平方正比的 ($O(N^2)$)，所以只要 Token 的長度一長，那個計算量就非常大，會讓電腦蠻辛苦的。

Flash Attention 它透過一些巧妙的方法，讓 GPU 避免被那個很慢的記憶體、還有各種同步的開銷 (Overhead) 拖累，讓它可以更好地利用計算單元的資源，而不會讓它在那邊空轉浪費時間。

這是 Flash Attention 的第一版。原本會把那個注意力矩陣 (Attention Matrix) 放到 HBM (High Bandwidth Memory) 高頻寬記憶體，但是這個頻寬真的蠻（慢）的。把一整個矩陣放進去那個高頻寬記憶體感覺不是個好主意。所以第一版它會把高頻寬記憶體切塊，分割成數個子矩陣，然後放到 SRAM 比較快的記憶體，然後透過融合內核 (Kernel Fusion) 的方式把它計算完，然後再把結果傳回去。這樣可以避免掉 HBM 慢得像蝸牛一樣的速度，讓它更快。

然後 Flash Attention 2，就是在那個超長 Sequence 序列的場景下，有些計算單元就在空轉嘛、在偷懶了。所以它透過序列長度平均化，可以讓所有的計算單元都有工作、都有在做事。然後還有透過減少非矩陣的計算，幾乎把它轉成矩陣計算，讓它可以更有效率，然後減少各個計算單元的同步。然後它的效能提升蠻顯著的。

第三個，它這個在 H 架構透過一些特性讓它可以有非同步 (Asynchronous)，就是可以利用一些零碎時間，讓它的同步單元件同步可以更快，然後還有流水線作業這樣。

最後還有一些附帶的，例如像量化 (Quantization)，這個就是把精度調得低一點，這個大語言模型常常會有這種手段來降低那個 VRAM 的開銷。透過這種加速方法，因為現在大語言模型大家都希望它更快更穩，然後希望可以用更多 Token。如果可以讓它計算得快一點，這樣可以讓它（應用更廣）。

這是剛剛講的注意力瓶頸。這其實不是計算得很慢，基本上都要等那個記憶體把資料傳進來。而且注意力機制的時間複雜度是平方，這個又加劇了這個現象。就像你看那個基本上在運算的時間挺少的，基本上都要等待那個資料傳起來，就蠻可惜的，就卡在那邊。

還有一個方法就是 Online Softmax。那個線上一次就走一步算一步，就是它收到一個值就馬上更新這樣子。傳統的方法是等到全部的值收進來，然後才一次計算，可是這樣又有記憶體開銷。所以這個方法是讓它走一步算一步，收到就馬上更新。這個基本上數學上是精確的計算。

第二個方法是 Recomputation (重計算)。就是有時候如果把那個送到很慢的記憶體的時間開銷太大，就乾脆就不記了。然後要的時候再馬上計算。如果計算的時間比那個送到記憶體的時間還遠小於送到記憶體的傳輸時間的話，就乾脆不要存在記憶體，直接再重新計算。結果基本上誤差是還可以接受的。我的報告到此結束，謝謝大家。

老師：
謝謝謝玉恩同學。他講的東西是真的我們現在科學界很大的一個瓶頸喔。

[02:14:37] 閃電秀 2：AI 消費者裝置 (中央大學 陳浩宇)

老師：
那我們現在有請下一個要閃電秀的同學，他是來自中央大學的陳浩宇同學。好，那我分享一下他的螢幕。好，看到螢幕了。

陳浩宇同學：
大家好，我是陳浩宇。我這時候來分享的是 AI 消費者裝置。所謂消費者裝置就像是手機、電腦這種裝置一樣。隨著大型模型跟多模態 AI 的發展，其實我們可能在未來就需要一個專門為 AI 為主的一個消費的裝置。

它可能會從向 App 為中心的觸控模式的互動模式，轉向就是 Agent 為中心的主動式輔助的運作模式。它不像手機、電腦就是讓我們人用輸入的方式去控制它，最理想的應該就是讓 AI 這個消費者裝置去主動去感知我們這個環境，然後讓它更有效去輔助我們使用者的需求。

這邊我列出了四個比較有可能可以成為未來 AI 消費者裝置的配件：

手機：相較於其他三個，它有非常優秀的螢幕跟運算能力。但是我覺得它是這四個當中摩擦率最高的。因為你要使用手機的話，你基本上就會從口袋裡拿出來，然後解鎖之後才能開始。

手錶：手錶相較於手機，它是長時間配在你的身體上面的。但是手錶它的缺點也很明顯，就是它的螢幕非常的小，它的輸入跟輸出受限，然後它也缺乏視覺的部分。所以說它應該比較難成為核心的生活消費裝置。

眼鏡：我是覺得這個是最有可能成為未來 AI 消費裝置的一個配件。首先第一點它有第一人稱視角，就是可能我們使用者看到什麼，基本上這個裝置（眼鏡）就看到什麼。然後就是雙手解放，它不會像手機一樣就是要用這一隻手去操作，你可以解放你的雙手，然後去做你要做的事情，同時眼鏡也可以很好地去輔助你。但它缺點就是因為眼鏡你要做得比較輕薄一點，所以它的電池的續航、散熱跟重量都會比較受到限制。

AI Pin (掛飾)：它算是目前就是為了 AI 去生成的一個消費裝置。目前討論度很高，它經常是一個佩戴在胸口前面、然後掛著。它有一個鏡頭，然後可以去做感測的部分。但它的第一個（缺點）相較於眼鏡的話，它就是視角會有偏差。你在胸口部分，你可能看到什麼，不一定那個裝置是可以接受得到的。然後它輸出的部分可能在視覺上面，你可能要用手然後去做投影的樣子。所以我覺得這個部分就比較（難普及）。

再來就是介紹一下，自己覺得最適合的 智慧眼鏡，它在作為 AI 消費裝置它的優勢跟劣勢有哪？

優勢：前面有提到就是第一視角跟雙手解放的部分。那還有就是擴增實境 (AR)，其實眼鏡的話它就是在輸出上面非常有優勢，因為它就是視覺上就可以直接呈現在眼鏡的上面，然後聽覺的話就是也直接傳到耳朵裡面。就是它可以非常很好地去融入生活的場景。然後是時尚背景，其實像是眼鏡、墨鏡這類的配件就是已經存在於這個社會中非常的久了，所以說如果未來有推出類似的產品的話，其實也很好地可以被大眾接受。

劣勢：前面有提到續航跟重量。就是因為眼鏡它基本要重量要在 50 克左右，就是不能太重，要不然的話你配戴起來的話會不舒服。所以因此它的電池大小就會受到限制。再來是散熱問題，因為你的眼鏡要輕薄嘛，所以它基本上那個一些裡面的配件、處理器，它可能就會貼在你的整個身體上面，它的舒適度、感熱性也會受到問題。

隱私與設計：有兩點必須要去做平衡的。一個就是使用者它所透過眼鏡去看到的虛擬影像，還有我們現實中其他人看到使用者，它佩戴必須要就是可以清楚呈現它的眼睛或是整個面部的（表情），不能有遮擋到的部分。再第四點就是安全（隱私）的問題。因為如果我在未來每個人都有一個智慧眼鏡的話，那就代表說每個人就會有一個攝像頭在臉上，然後他就隨時可以錄影，所以這可能也會造成別人的一些不安。

所以未來所謂的 AI 的發展，其實這樣的消費裝置應該會很快就會跟著出來。然後眼鏡的話也算是一個非常好的載體。但是這也不代表說手機就會被取代，它應該會就是可能眼鏡跟手機它可以成為一個協作的模式，然後讓 AI 可以更好地去融入我們生活中，幫助我們解決各種問題。好，以上就是我今天的分享。

老師：
好，謝謝中央大學的陳浩宇同學。那個智慧眼鏡的這個未來發展真的是變動很大。

[02:21:07] 閃電秀 3：AI 金融風險 (政大 林晨宇)

老師：
那接下來是一位是來自政大的林晨宇同學。好，那我分享一下他的螢幕。好，那就有請林晨宇同學。

林晨宇同學：
大家好，我是政大經利三（經濟系三年級）林晨宇。要講的主題是 AI 的大模型是否會加劇金融的風險。

我會先比較傳統的模型跟 AI 新的大模型。

傳統模型：它是基於統計學、經濟學，還會有一個固定的公式，也會有明確的假設。舉的案例就是像線性回歸。我也附上幾個傳統模型的使用場景，像第一個是市場風險的模型，它的目的就是為了去評估資產的價格下跌多少以及它的資產的風險水準。第二個是財務報表分析的模型，這邊就是杜邦模型。第三個是交易模型以及避險模型。

AI 模型：它是機器學習。AI 它不會再假設數據是線性或是常態分佈之類的，它會直接從大量的資料中去找規律。所以它的特性就是黑箱、非線性，然後它能處理高維度的資料，而且它可以自己去生成內容。常見的 AI 模型的應用就是反詐騙以及高頻交易。

接下來就講常見的 AI 模型的風險：

幻覺 (Hallucination)：AI 它有可能會生成看起來很合理、但是它完全錯誤的內容。像是 GPT 它可能生成錯誤的財報摘要、錯誤的規範，或是它錯誤地去解讀某個市場上的事件。這會有風險，原因是因為金融機構常常做決策的時候會依賴他們可信度高的 AI 模型。所以這個 AI 模型一旦錯誤，他們就會害他們去做錯誤的決策。

羊群效應 (Herding)：如果很多個金融機構他們都使用相同的 AI 模型或是同源的模型，他們可能會做出一模一樣的反應。然後因為好幾間大型的金融機構一起做同樣的反應，會讓整個市場的波動會被放大。

假新聞 (Fake News)：如果就是假設發生一個根本就不存在的事件，但是模型以為那個是真的，然後全部的金融機構都因為那個模型給出了判斷去做一樣的操作。像是 2023 年有人用 AI 偽造了一個五角大廈爆炸的照片，所以害美股有短暫的下跌。因為金融市場非常分秒必爭，所以他們常常會先交易之後再去查證，所以這種假訊息對他們來說影響很大。

這就是我剛剛講到的那個案例，就是有人用 AI 去生成五角大廈爆炸的照片，然後在推特傳開。然後很多家大型機構的自動化交易系統就立刻把它判為重大的國安事件，所以讓標普 500 指數在幾分鐘內就迅速下跌。雖然他們很快就發現這是假新聞，然後漲回來。但是這也凸顯一件事，說就是 AI 生成的訊息會被那種模型先交易之後，然後之後人們發現它的錯誤再去可能去做一些修正。

我的第二個案例是，像最近台灣也有不少人因為普發六千（口誤為一萬），然後銀行的那個系統就判斷他可能懷疑他是詐騙，就他們的帳號都封鎖。這個是過去的 2023 年的新聞，就是英國和美國許多家銀行，他們就 AI 的詐騙偵測系統把正常的轉帳當成詐騙，所以他就封鎖了好幾千名的合法用戶。所以他們這樣造成用戶的交易中斷，然後銀行也會受到輿論的壓力或是合規的壓力，以及客服會接到很多通電話。

所以這個案例就更凸顯了 AI 在金融領域的風險。因為我們對模型的解釋性很低，所以銀行短時間內沒辦法理解說為什麼 AI 會判某筆交易是詐騙，所以他們要處理起來就會更久。而且如果這個 AI 模型是用錯誤的資料去訓練，這可能會導致整個系統性或是會有歧視性的誤判。

所以我的結論是說：它不是萬能。尤其是萬一我們給它的錯誤的資料或是假訊息的時候，它的錯誤會比一般人工的錯誤還要嚴重。而且如果模型越複雜，我們若它發生某個錯誤，我們要去解釋、去修正就會時間就會更長。而且現在是自動化高速反應，如果 AI 一旦出錯，整個市場就會因為那個模型去做一樣的判斷（錯誤）。所以金融的監理、模型的驗證以及我們人工的審查會比過去還重要。所以我們最後結論就是 AI 雖然能夠提升效率，但是它沒辦法降低風險，所以我們使用 AI 的時候要格外小心，整個金融市場才會比較良好的運作。OK，這是我的報告，謝謝大家。

老師：
好，謝謝政大的林晨宇同學。那大家真的是要記住，就是要負責任的使用 AI，那我們這堂課的核心宗旨也是這個，大家不要亂用 AI。

[02:27:30] TA 時間：Gemini CLI 實作「你畫我猜」遊戲

TA 發貴：
好，那我們今天就進入我們的 TA 課，我是發貴。大家現在應該可以在課程內容裡面看到我今天的 PDF（講義）。那大家點進來，然後我們就開始。大家如果有空的話可以大家一起來做，因為今天我們會做到一個「你畫我猜」的小專案，讓 Gemini 幫我們做。

[02:28:26] 遊戲效果展示
那遊戲大概長這個樣子，有點醜，但是沒關係。我們現在因為它正在跑著，那我已經畫好了一個人，那這看起來就是一個人在跳舞、跳芭蕾吧。然後我們就讓 AI 猜。那它就說：「哦，這看起來（我放大一點）就像是在跳舞的一個人。」那大家可能以為我在騙人，那我就把它畫一個紅鼻子之類的，那我再點一次。它就說：「哦，看起來像是一個小丑在跳舞。」好，那實際效果是這樣子。

[02:29:09] 工具介紹：Gemini CLI (gemini-chat-cli)
那我們就來開始我們今天的 TA 課。今天介紹 Gemini CLI，是 Google 他們自家產生的一個發行的一個開源的 AI Agent。那它可以通過終端機 (Terminal)——也就是這個黑黑一塊的 Terminal。但是大家不用擔心，這個過程不會很難，我會一步一步教大家怎麼樣就是來操作這個，並使用就是現在大家同學都可以申請到的 Gemini 1.5 Pro (或 3.0 Pro Preview) 來跑這個 Gemini CLI。

[02:30:04] 步驟一：安裝 Node.js
在安裝這個 Gemini CLI 之前呢，我們會這個需要 Node.js。那大家來（官網），然後就是 Windows 會有這樣子的東西，就是往下拉，然後這裡 Windows 就點這個 Windows 安裝程式。那 Mac 呢就是一樣，它其實你點這個網址進來，它就會找到適合你的安裝方法，然後你就只要點下去就好了。

點下去過後就會有一個安裝程式嘛，那大家就是一樣一直按 Next。因為我安裝過了，但是沒關係，我也再裝一次給大家。反正是一直按 Next, Next，然後 Windows 會跑出這個，那你就點「是」，就安裝完了。

那要怎麼知道安裝完了呢？那我們就要來到下一個頁面。就是我們安裝完這個 Node.js 就可以安裝我們的 Gemini CLI。

Windows: 要打開終端機 (CMD)。它的方法我有寫在這裡，就是我們的那個四個方塊鍵 (Windows Key) 加一個 R，然後打 cmd，它就會跑出這樣的一個畫面（終端機）。我們輸入 node -v，它就顯示版本是 v24.11.1 這樣子。

macOS: 其實有自帶 Terminal，所以你只要去搜尋在你的電腦裡面搜尋 Terminal，就可以再輸入我剛才一樣的一個指令 (node -v)，那照理來說也會出現一模一樣的版本。

[02:33:06] 步驟二：安裝 Gemini CLI
接下來就是下載 Gemini CLI。然後大家可以複製我 PPT 裡面的這一行指令：
npm install -g gemini-chat-cli (依據語音推測指令)
然後只要貼上就好了，然後它就會跑。然後由於我已經安裝了，它可能現在會比較安裝得比較快，通常會需要兩分鐘左右它才會安裝完成。那如果你家是用桌機的話，如果是用乙太網路，它會跑得更快。

跑完大概會長這個樣子，他說他新增了 576 個 Package。然後安裝完了過後，我們就直接打 gemini 就可以直接運行了。

[02:35:08] 步驟三：啟動與設定 Gemini CLI
它第一次啟動 Gemini CLI 到你的電腦，真正的第一次啟動 Gemini CLI，它會先跑出這樣的畫面，它會讓你選一個 Title，應該是說選一個你喜歡的顏色這樣子的。

選完過後它會跳出另外一個，就是說：「哦，你要怎麼樣去連接這個 Gemini 呢？」

第一個選項就是登錄 Google。

第二個就是用 Gemini API Key。

那我們這一次我們要用這個 Login with Google，所以大家就點這個進去。那點進去過後，它就會直接跳出網頁，然後就會讓你去選擇你的帳號去連接。然後請大家用有申請免費的 Gemini Pro 的帳號。登錄過後它就會寫「哦，已經驗證完成了」，那你就可以在你的電腦上面去使用你的 Gemini CLI。

[02:36:49] 啟用 Preview Features (使用 Gemini 1.5/3.0 Pro)
裝完 Gemini CLI 後，它其實 Default (預設) 是沒有啟動這個 Gemini 3.0 Pro (影片中可能指 1.5 Pro 或當時的最新預覽版) 的。所以大家點進去過後，它就會等在這個畫面裡面。

你要在這個地方打上斜線 \settings，然後 Enter 下去，然後你就會到 Setting 裡面。

記得大家要把這個 Preview Feature 打開。那通常是 false，然後你只要按 Enter 它就會改成 true。

然後再按那個 ESC 就退出。

改完過後，你要進入斜線 \model 裡面。打 \model 過後按 Enter 下去，它就會跑出類似這樣子的畫面，它就會叫你選擇模型。
那打開了那個 True 過後，那它這裡 Pro 的選項就會多了一個 Gemini 1.5 Pro Preview (或 3.0 Pro) 的這個模型。那你就可選了過後，按上下左右鍵選到它，然後你按 Enter，它就會跑了這個模型。那要怎麼確定呢？就是這裡會有小小的寫一個 Pro 的小浮標，就代表你真正的啟動了這個 Pro 的模型去跑這個 Gemini CLI。

[02:43:49] Gemini CLI 功能演示：分析檔案
Gemini CLI 有三個大功能：

斜線指令 (\)：就是 Gemini CLI 的內建指令。

小老鼠 (@)：這是說你要讓它去讀取檔案。我就來直接示範一下好了。就是說我在桌面有一個檔案，我複製路徑。然後就是 @ 然後複製路徑貼上。我告訴他：「請你幫我分析一下這個檔案在說什麼」。

示範失敗：它說很抱歉無法直接讀這個，因為它是二進位格式。

示範成功：我換另外一個文字檔（Gemini 使用手冊），請你幫我做分析。它就會跑出來：「哦，這份文字點 md 是 Gemini 的使用手冊，然後就是詳細列了各種命令的工具功能這樣子。」所以它就是可以直接讀取你的電腦裡面的文件，然後去做一個分析。

分析網頁：還有一種功能，我就複製一個網址，然後貼上去問「這個網址在說什麼？」。它會問你要經過你的同意，選「允許這一次」。然後它就在抓了，這是在說一個 Git 版本控制的入門教學文章。還蠻方便的啦。你也可以叫他直接幫我製作成什麼 MD 檔，當做自己的筆記。

[02:49:23] 實作：製作「你畫我猜」遊戲
最後就是做這個與 AI 的「你畫我猜」的專題。那我這裡有把這個 Prompt 給打下來，那就可以直接複製。然後記得把這個 Gemini 的 API Key 填上去。

[02:50:10] 如何獲取免費 API Key (Google AI Studio)
大家如果真的不懂想要說我就想用 Gemini API Key 怎麼辦？那大家就是直接打 Google AI Studio (或是 Google API Key)。因為這個 Key 是免費的喔，所以我覺得大家可以用看看。

進去後點這個 Get API Key。

點 Create API Key。

那就隨便拿你的要新增的名字 Key 名這樣子（我可以寫一個 random）。

然後這裡要選擇一個專案 (Project)。如果你沒有專案，就點 Create Project，然後再 Clean a new key。

這裡會有寫一個 Free Tier，反正就是這是免費的。如果你大家不信你可以點看進去，然後他就會跟你說這是沒有任何連接帳號的（Billing Account），這時候你就可以放心了，因為他就真的不會扣你的錢。

然後你就複製這個 Key，然後回到剛才的 Prompt 貼上。然後就丟給 Gemini CLI。

[02:52:42] 讓 AI 寫程式
然後它開始就會跑了。大家繼續了解我的需求，去做一個小小的專案（你畫我猜）。這裡有寫什麼呢？我這裡有寫就是因為你畫我猜都通常會有橡皮擦嘛、然後填滿、上一步、復原、跟清空屏幕這樣子。那最後就是畫完過後按確認那個 AI 來猜。

然後這裡就是在跑的過程中，它就會叫你要去允許他做某些事情嘛。啊如果你真的看不懂，那你就按 a (Always allow) 吧，就是一直允許這樣子。

(AI 生成程式碼中...)
好，我們就不用看，我們就讓它 Be Coding。錯了，他自己自己去解決。哦，他說太好了。

[02:56:01] 啟動遊戲 Server
好，然後這裡已經全部的檔案建立完畢了喔。那它有說我們要啟動伺服器。那我們現在要另外去打開一個這個命令提示字元 (CMD)。

然後去這個 cd desktop (進入桌面)。

反正在我桌面會有這個文件夾，大家就點就進來。

然後去執行這個，照著他說的字複製，然後按 Enter。

然後它就開始跑了喔。它就說這個 Server 它是在 Running 在這個 localhost:3000。

那大家這時候按那個 Ctrl 然後再按滑鼠左鍵，這可以跑出來了。好，跟剛才的那個畫面有點不一樣喔。那我們來試試看畫一個東西。大家應該知道我畫的是什麼... 呃，填滿嘛，這個就填黑。然後這個就畫，然後有個濃眉大眼之類的。換個顏色。然後大家猜猜看。

(點擊讓 AI 猜測)
在思考中。然後他就（顯示）：「哦，AI 覺得這是蠟筆小新。」 這樣子。

好，那今天的助教課就到這裡就結束了，謝謝大家。

【生成式 AI】13. 強化學習和 LLM 對齊 - 逐字稿 (Part 1)

時間範圍：00:00:00 - 00:42:00

[00:00:00] 大家好，我們現在正在處理技術上的小問題，畫面還沒有播出去。看一下哦，有了，這樣對吧？好，可以。

[00:00:15] 大家好，今天有趣的地方是這樣，我們基本上該講的話都快要講完了，那剩下什麼事情呢？剩下就是我們在上一次開課的時候比較「呼嚨」過去——也沒有呼嚨啦，也是有講到，但稍微簡單一點點的介紹。後來發現，特別是在今年發現其實這一點可能還是要向大家報告、說明一下。

[00:00:51] 因為我們這個生成式 AI 的課程有兩個重點。第一個重點，大家需要知道原理。雖然我們的同學出去不一定會去做 AI 工程師，但有可能你要去帶 AI 工程師，所以希望那個時候你說出來的話都感覺很專業。

[00:01:30] 剛剛不知道為什麼說「工程師來騙我們」這句話，難道不能講？突然被消音了。總而言之，就是在很多地方，如果我們外面在討論所有深度學習相關的議題，希望大家都知道這發生了什麼事情。也許有一些太深入的你不不需要這麼理解，但你大概知道這裡面的原理到底是什麼，你就不會害怕。這是一個很大的重點。當然另外一個大重點就是，我們希望這不是只知道原理而已，還是希望大家是可以去應用的，這是我們課程的目標。

[00:02:15] 今天因為我們要講的是一個叫做「強化學習」（Reinforcement Learning）。強化學習是一個 AI 很重要的概念，在深度學習裡面也大量運用到強化學習。但強化學習本身有一點點難度，一點點而已啦，沒有真的很深，所以大家也不用真的很擔心說完全聽不懂。

[00:02:39] 強化學習其實是一個很有趣的學習方式。我先讓大家引發一點興趣，大家看到我們的投影片上目前就是一個可愛的機器人在玩遊戲的樣子。沒有錯，強化學習開始的時候，就是想要知道電腦怎麼樣可以去做一些比較複雜的動作，而且不是我們教它，是它自己去學會。真的很有「人工智慧」的樣子。一開始的例子就是玩遊戲，等一下我們也會介紹。

[00:03:10] 所以我們今天介紹的是比較理論一點點的強化學習，但是為了讓大家引發興趣，今天有很多的八卦要跟大家說。實務上沒有什麼新的東西，雖然作業還是要讓大家做一些實務上有關的東西，但是大部分我們今天講的是強化學習的基本概念、理論等等。

[00:03:38] 那我們就開始。強化學習一開始我們要先介紹到底什麼是強化學習。強化學習最有名的範例，也是引發大家覺得強化學習好像真的很厲害——特別是用深度學習，就是打造神經網路的模型，打造我們的「呆萌機器人」是用神經網路的方式去打造，然後做出來的強化學習——最有名的例子大概就是 AlphaGo。現在的同學到底知不知道 AlphaGo 是什麼？在場你們知道，謝謝。我一直擔心說年代久遠，本來大家覺得這是很有名的事情，應該大家都知道，後來想不太對，說不定大家在那個時候年紀很小，並沒有注意到這件事情。

[00:04:25] 強化學習我們要先介紹一家很有名的公司叫做 DeepMind，等一下我們會好好介紹這一家公司。DeepMind 在 2015 年的時候在《Nature》（自然）期刊——就是科學界最高的期刊，未來大家如果是念到博士去了以後，想要申請教職或是想要到學校去教書、做研究工作，如果你有一篇登在《Nature》上面的文章，大家都應該覺得超厲害。《Nature》跟《Science》大概是科學界覺得最厲害的期刊。

[00:04:59] 他們就在《Nature》上登了一篇，是用電腦去玩遊戲。所以這也是我第一次知道，原來《Nature》上面是可以寫電動玩具相關文章的。他就是說怎麼樣讓電腦去玩遊戲，他們用的方法是我們今天會介紹的 Deep Q-Learning，就是強化學習的一種叫 Q-Learning 的方法。

[00:05:22] 那就是教電腦去玩遊戲，玩那個古典的遊戲，就是 Atari 的古典遊戲。在非常非常早之前有一個遊戲機叫做 Atari，那上面有很多古典的遊戲。現在有很多同學用模擬器的話，其實有一些同學可能也有玩過。他們玩的結果怎麼樣？這個特色不是說我們手把手教電腦說：「哦，這個打磚塊的遊戲...」（對不起，這個可能大家不知道什麼打磚塊）。

[00:05:53] 總而言之，就是一個古典的遊戲。我們要去玩的時候，我們沒有手把手教它規則是這樣，然後看到球來的時候你要做什麼，看到球往哪邊走你就跟著往哪邊。沒有要手把手教它，就讓電腦自己去玩。電腦的目標就是要得到很高的分數，得到越高的分數越好。

[00:06:13] 所以我們並沒有教電腦規則到底是什麼，只教它怎麼控制，然後它就自己去玩了。玩一玩，它的目標就是我們只有告訴它「目標就是要得最高分」，然後它自己去學習。就很神奇的，它就學會了。它不但學會了，其實一半以上的遊戲——在那時候這一半以上的這種古典小遊戲——它 50% 都超過人類最厲害的玩家。所以大家就覺得這個真的很厲害、很有趣。

[00:06:50] 大家會覺得有趣的原因，當然不是說電腦玩電動玩具這件事情，而是說電腦既然可以自己去學玩電動玩具，就表示電腦好像自己可以去學一些比較複雜的動作，不用我們人類一一的去教它。如果它自己就會學會，那以後變成一個機器要去工廠組裝一些東西，或是要去做一些我們希望它能夠做的事情，那它是不是也可以自己去學會？所以大家就覺得很有興趣。這個就是那個時候開始大家覺得強化學習好像真的蠻厲害的。

[00:07:30] 事實上不只這樣子，因為我們之前有說過曾經有過神經網路的寒冬，但是 DeepMind 他們做的這個強化學習的方式是用神經網路，它的「呆萌機器人」不一定要用神經網路，但他用的是神經網路，所以大家就覺得神經網路好像真的蠻厲害的，可以去玩、可以做這種比較複雜的動作，感覺它可以自己去學。

[00:07:56] 我們現在正式的介紹 DeepMind 這家公司。如果最近有注意到，像是最近 Gemini 3，就是我們之前有為大家介紹大家都可以申請 Pro 版本的 Gemini。Gemini 3 最近非常非常的紅，他們就是 Google 推出來的。大家也知道，現在 Google AI 的團隊就叫做 Google DeepMind，就是 DeepMind 這個名字。

[00:08:23] 所以我們現在要好好的介紹。Google 本來有一個原生的 AI 研究團隊叫做 Google Brain。這個 Google Brain 其中有很多大師都在裡面待過，包括創辦人是吳恩達老師，非常有名，我們也有在課程裡面介紹過他。還有像是 Hinton，就是在 2024 拿到諾貝爾物理學獎的那位 Hinton，都曾經待過 Google Brain。所以這個是 Google 原生的 AI 研究團隊。

[00:08:57] 那 DeepMind 其實是在英國成立的一家 AI 公司，然後它在 2014 年的時候被 Google 收購。它做的一些很有名的事情，最有名的例子大概就是 2016 年它的 AlphaGo 擊敗了世界棋王李世石。這是我們今天會介紹的一段非常重要的強化學習，非常重大的一個里程碑。

[00:09:29] 然後 2024 年的時候，他們又做了 AlphaFold，這個就是去計算蛋白質的排列等等。大家一直覺得這件事情非常非常的困難，從以前大家就知道很困難，但他做得非常的好，所以也因此得到諾貝爾化學獎，所以是相當厲害的一件事情。這家公司很有趣，他們做了很多研究的工作，等一下我們會說他們真的做了很多研究。

[00:09:54] 但是在 2023 年 4 月的時候，他們終於還是合併了。所以現在這個 Google 的 AI 團隊就叫做 Google DeepMind。所以已經有 DeepMind 這個獨立的公司在，雖然也是 Google 的公司啦，但是現在已經不是獨立經營，它基本上就是 Google 的 AI 團隊。

[00:10:18] 說完了這個，大家也沒有覺得這個有什麼有趣的。那我現在要說幾件有趣的事情。DeepMind 它真的是一個很愛做研究的公司，它基本上自詡為研究單位，一直認定自己是研究單位。它發表了超過 1000 篇的論文，那在我們剛剛說過頂級的期刊《Nature》跟《Science》上面，他們就發表有 13 篇。所以相當厲害，如果它是一個學校的話，這個也是蠻厲害的，他們研究量能其實非常非常的高。

[00:10:55] 那他們既然這麼喜歡做研究，所以他們其實在某種程度都比較希望是能夠獨立運行的一家公司。所以在之前幾年的時候，一直有聽到說 DeepMind 雖然被 Google 收購了也有一些好處，因為 Google 畢竟錢多，所以要提供給他們能夠研究的那些 GPU 等等的設備，當然他們就會有非常多這樣的資源可以去運用。但是同時他們另外一方面也覺得這樣子有一點點被限制住，所以很多裡面的人也想要獨立。

[00:11:32] 我就忍不住去問我們的 Gemini。Gemini 就是說，一直傳聞說 DeepMind 他們因為自認是研究單位，在某一個理念上面——雖然 Google 其實也不是一個非常壞的公司，畢竟有像是 Hinton 等人在裡面，都是非常注重 AI 倫理的人——但是他們還是希望有比較獨立可以去做研究，不要受太多 Google 這邊商業考量的影響。所以他們一直有想要獨立，爭取更自主的研究經營，甚至想要成立更自主的一個單位，不要受到 Google 的干預。

[00:12:20] 然後我就問 Gemini 是不是真的？它答得非常非常的好，非常的有趣，非常的八卦。所以很喜歡看八卦的同學，歡迎去我們的「大伊布雅」（Iveai）的粉專看一篇 PO 文，這個完全是 Gemini 說的，自己爆自己自家的料，非常的精彩，歡迎大家去看一下。

[00:12:42] 不管怎麼樣，現在他們已經合併了，所以它現在就是 Google 的 AI 團隊。那我們再回到 AlphaGo。在 DeepMind 做 AlphaGo 的時候，有一個非常重要的人物是跟我們台灣非常相關的人，就是黃士傑博士。

[00:13:02] 黃士傑博士完全是台灣訓練出來的。他大學的時候是念交大，博士班的時候是師大的，所以他完全是台灣培養出來的。那個時候為什麼會被 DeepMind 找到呢？DeepMind 那家英國的公司特別去邀請他到英國去工作，直接從台灣去英國工作。原因就是因為那個時候他們想要做一個很強的圍棋比賽的電腦程式，就是要做 AlphaGo。

[00:13:47] 因為黃士傑博士在博士班的時期就開發了一個叫做 Erica 的圍棋程式，拿到圍棋界的世界冠軍。對不起，是「電腦圍棋界」最厲害的程式。但是那個時代的程式其實還沒有辦法跟真人比，特別是跟職業的人相比，沒有辦法跟職業的水準比較。但是畢竟他已經是拿到電腦界最厲害的了，所以 DeepMind 那時候看上了他，覺得說我們正好要做世界上最厲害的下圍棋的程式，所以就邀請他去一起參加、加入 DeepMind。

[00:14:37] 這邊有點小小的、也不能算八卦，就是黃士傑博士那個時候的女朋友的名字叫 Erica。這真的要奉勸各位同學，這是一個很冒險的動作，就像你在論文裡面很喜歡感謝你的女朋友，這件事情我一直覺得很冒險。但是黃士傑博士這邊比較沒有問題，因為他後來他們真的結婚喔，所以後來沒有出現什麼危險的事情。

[00:15:06] 然後我們就進入 AlphaGo。AlphaGo 最有名的就是打敗世界棋王李世石。在這個紀錄片如果大家有機會去找到的話（之前在 Netflix 上了，我現在不確定還在不在），AlphaGo 的紀錄片非常的感人，非常的推薦大家，就算看一部感人的電影其實也很值得去看。

[00:15:28] 就是 4 比 1 擊敗世界棋王李世石。我現在先要說為什麼大家會覺得這很了不起。因為在那個年代，我剛剛有說過，即使是電腦界的圍棋冠軍，其實還是沒有辦法跟真人比較的，特別是職業棋士。大家覺得下圍棋這件事情太複雜了，如果電腦要贏過人類的職業棋士的話，大概還要花個 50 年到 100 年。這是下圍棋的老師會跟你這樣說，連資訊科學家、專門做下圍棋電腦程式的人也是這樣跟你說的，因為真的太複雜、太難了。

[00:16:10] 所以那一年打敗了世界棋王李世石，大家非常的震驚，就發現說：「哦，原來 AI 這麼強了！」這是那個時候非常重要的事情。再推薦一次這個紀錄片。

[00:16:22] 在那個時候，大家一直在傳聞說那個 4 比 1 是不是 AlphaGo 輸掉一場？它贏了四場只輸掉一場。那一場到底是不是 DeepMind 要故意想不要讓人類輸得太難看？這樣子人類實在太丟臉了。那個在紀錄片也有說，其實沒有，他們想要全贏，但是不小心輸了一場。其實他們有點不甘心啦，輸了一場有點不甘心。

[00:17:10] 在 AlphaGo 另外有一個很重要的事情，就是這個基本上它並不是真的是機器手臂去下棋。所謂電腦對弈，是說 AlphaGo 在電腦螢幕上顯示說它要下在哪一個位置，是由人——那個「人」其實就是黃士傑博士——去幫它下到那個位置。然後李世石當然是下他自己的。就是這樣的情景。

[00:17:37] 在輸了那一場呢，其實黃士傑博士本身他也算是...他就喜歡圍棋啊，所以他才會去做圍棋程式嘛。他自己的棋力也不錯，雖然還沒有到職業水準，但是在業餘的水準已經算是非常厲害。所以他自己在拿那一個棋下去的時候，他也覺得「死定了」，因為這一棋是一定下錯的。後來真的輸了，連黃士傑博士都看得出來，那李世石當然看得出來，所以那一場真的輸掉。

[00:18:11] 他們輸掉以後就非常的不甘心，他們就想說有沒有辦法改進呢？DeepMind 是一間很有趣的公司，他們就想要做一個更厲害的下棋程式。所以有一年，這跟我們台灣又有關係了。大家可以看到就是有一年的聖誕節，2016 年聖誕節的時候，黃士傑博士就巧巧地回到了台灣。然後據他的說明就是，他就一邊吃著泡麵，然後一邊去登錄了一個帳號。

[00:18:41] 因為圍棋是這樣子，你棋力到一定的程度的時候，你要找到跟你棋力相當可以對弈的人，這個有點難。所以網路上有一些有名的網站，你可以找各路優秀的、厲害的棋手都在上面。所以他們就登錄了一個帳號，黃士傑博士登錄了一個帳號，然後就開始神秘的去跟很多人下棋。

[00:19:30] 開始的時候當然沒有人理他，因為大家覺得是個新登錄的帳號，誰要理他？但是後來發現這個好像有一個很厲害的在那邊下，然後在這個短的時間內就 60 連勝，非常的厲害。你要想說這全部都世界一流的棋手都在上面，他 60 連勝一場都沒有敗。

[00:19:52] 所以打到後面的時候，其實大家已經在猜了。你看他時間沒有很久，就是 12 月 29 號到 1 月 4 號而已，非常短的時間內就 60 連勝。所以黃士傑博士也很可憐，他應該吃了很多的泡麵，因為他一定要盯著這個螢幕一直跟大家下棋。

[00:20:09] 60 連勝，所以大家就猜它是 AlphaGo，不然怎麼會這麼厲害？然後在最後一天，他們決定了這個已經 60 連勝了，那黃士傑博士就在這個網路上 PO 了一個非常小的訊息，就是說他就是 AlphaGo 的黃博士。就這樣子，結束了。就證實他真的是 AlphaGo。

[00:20:34] 到這個時候呢，DeepMind 已經對他們的棋力的水準相當的滿意。然後後來他們就到了中國的烏鎮，又去參加了一個圍棋會。那個時候其實因為 DeepMind 已經覺得他們的棋力已經很強了，所以基本上他們已經沒有想要證實自己多強了。那時候包括有像柯潔等的一些很厲害的棋士一起去對弈。那個時候大家在看的時候已經不是那麼計較到底是誰贏，基本上就是 AlphaGo 全勝了。

[00:21:12] 大家開始觀察，特別是下棋的人，開始觀察說 AlphaGo 到底是為什麼會這樣子下？它到底後面的想法是什麼？為什麼我們經過了這麼多年大家沒有想到說可以這樣子下？所以比較開始產生的這個人工智慧跟人的合作。這是一個非常好的點，未來世界也有可能就是我們跟 AI 會有更多更多類似這樣的激起很多的火花。在過去的時光歲月裡面可能有一些我們沒有想過的想法，AI 居然蹦出來的這種想法。

[00:21:57] 在之後呢，DeepMind 又開始想了，他們的下一個挑戰就是：之前的強化學習難的地方是準備訓練資料。為什麼很難準備訓練資料？這個答案很簡單。就是因為，我要打敗世界棋王李世石，我要準備的訓練資料是不是一定要比世界棋王還要厲害、還要會下？所以我準備一個比世界棋王還要厲害的訓練資料給它。鬼知道這種事情！如果我知道這種事情，那我就是世界棋王，我比世界棋王還要厲害。不可能，所以我很難準備訓練資料。

[00:22:40] 但是畢竟有過去的一些棋譜，就是基本的招數啦，基本招數你還是要學會嘛。那基本招數還是由我們人工去餵給它的。那 DeepMind 就想說有沒有可能一開始就不要有基本招數教它？它完全從零開始學習。所以有 AlphaGo Zero，完全不餵這個人類過去的任何的名門的招數，完全不餵棋譜，就讓它去學。

[00:23:08] 就發現說真的可以成！然後他們也宣稱說這個是他們有史以來最強的 AlphaGo 的版本。當然他們其實在之前就已經沒有對手了，所以到這個時候更沒有對手，他們唯一的對手就是前面的 AlphaGo。所以他們到這個時候就心滿意足了，就結束了 AlphaGo 的計畫。

[00:23:29] AlphaGo 的這個神秘的「強化學習」到底是長什麼樣子呢？圖形大概這樣子。所有的強化學習基本上都是這個樣子：

Environment (環境)：外面有一個環境，就像上面的地球形狀。在 AlphaGo 裡面下圍棋的時候，這個環境其實就是棋盤的狀況，好比黑子白子目前的狀況。

Agent (代理人/智能體)：我們主要的 AI 模型，也就是我們的「呆萌機器人」。它會輸入這個 Agent 裡面，就是目前下的情況。

Action (動作)：Agent 要決定一個動作。在下棋的動作就是要決定要把棋下到什麼地方去。

Time (時間)：環境當然又改變了，所以所有的東西都是跟時間有關的。

Reward (獎勵)：我們的目標就是希望得到最高的 Reward。

[00:24:34] 環境 $S_t$ (第 $t$ 個時間點的環境) 進來到 Agent，Agent 又要決定一個 $A_t$ 的動作。就是周而復始這樣。它的重點是，Agent 想要學說在哪一個狀態 ($S_t$) 之下，我們應該要採取什麼樣的動作 ($A_t$)。

[00:25:03] 目標是什麼呢？目標就是希望得到最高的 Reward ($R_t$)。就是每一次你去做了一個動作之後，你可能有一個即時的獎勵叫做 $R_t$。比如剛剛說玩電動玩具，你可能去吃金幣，吃到金幣你的分數就提高，這個時候你就會有一個即時的 Reward。

[00:25:42] 但是有時候有一些 Reward 並不是在即時發生的。比方說你玩完成了以後，它才會給你這個額外的 Reward。特別是下圍棋，通常這種下圍棋的程式是沒有即時的 Reward，因為我們怎麼知道這一步下得好不好？如果我知道這一步下得好不好，我就是世界棋王了。所以下圍棋的時候，像是設計 AlphaGo 的時候，所謂的即時 Reward 基本上是沒有的，是等他下完了以後才有一個最終的 Reward。最終 Reward 是什麼呢？就是他贏了就得到一分，輸了就扣一分。

[00:26:53] 電腦的目標就是得到越高的 Reward 越好。下圍棋基本上就是得到一分就是它的目標。

[00:27:11] 所以現在我們又要開始想說，我們的這個 AI 模型到底要打造成什麼樣子呢？也就是我們到底要學什麼呢？輸入到底是什麼呢？輸出到底是什麼呢？這基本上有兩種的想法。

Policy-based (策略基礎)

Value-based (價值基礎)

[00:27:56] 這聽起來都好像很高級，所以我們馬上來解釋。

第一種：Policy-based (策略基礎)

[00:28:06] Policy-based 是比較直接的想法。比方說我們玩一個打磚塊的遊戲，球掉下來，下面有一個板子（Paddle），你接到球了以後它就會上去彈跳，打到越多磚塊越高分。

[00:28:28] 狀態 ($S_t$) 可能就是那個時候的畫面。其實不只那個時候的畫面，應該要有連續的幾張圖，因為他才知道這個球是往哪裡走的（往下走還是往上走）。所以在這個時間點的狀態的時候，你要決定幾個動作。動作可能有三個：往左走、往右走、不要動。

[00:29:31] 你要決定你的策略（Policy），也就是決定動作的函數。這個函數當然也可以叫做 Action Function，但是這聽起來不高級，所以我們把它叫做 Policy Function。更過分的是因為 Policy Function 有 P 嘛，大家很喜歡把它念成 P 的話就是希臘字母的 $\pi$（Pi）。所以大家很喜歡把 Policy-based 的 function 寫成 $\pi$。這沒有任何理由，只是因為 P 開頭。

[00:30:25] 但是問題就是說，難道是要我們一個很厲害的、很會玩這個遊戲的人在那邊教電腦說：這個狀態的時候應該要往左動，這個狀態應該要往右動？我們前面也說過，你要去標示這些東西可能要標示個上萬筆，這個要人去標記好像有點不合實際。第二件事情，我們其實也希望電腦很會玩，它要超過人類的水準。如果我們要打敗世界棋王，當然不能請任何人去標記，因為任何人可能都沒有世界棋王這麼厲害。所以 Policy-based 通常訓練資料很難準備。

第二種：Value-based (價值基礎) / Q-Learning

[00:31:33] 所以大家就想到另外一個聰明的解法，就是做 Value-based。Value-based 基本上就是說「這個狀態的分數」，或是說「這個狀態加一個動作，我要給它評分」。

[00:31:54] 所以我們不只是 $S_t$ (狀態) 進去，我們把 $A_t$ (動作) 也一起放進去。我們就是要評估說「這個狀態做這個動作的分數到底是怎麼樣」。我們會設計一個 Q 函數 (Q-Function)，就是評分的函數。

[00:32:13] Q 函數通常就是把一個狀態加上一個動作然後去評分。如果我們剛剛因為只有三個動作（往左、往右、停），如果我真的學會了這個函數了之後，我就可以在同樣的狀態之下，把「往左」帶入這個學好的 Q 函數算出分數，把「往右」帶進去，把「停」也帶進去。然後三個分數都算一算，我們發現往右得到最高分，那我們就知道應該往右走。

[00:32:49] 所以學會了 Q 函數之後，我們就可以決定最好的動作了。最好的動作就是在這個狀態帶進去以後最高分的動作。

[00:33:11] 所謂 Deep Q-Learning，就是這個 Q 函數是我們用神經網路（Deep Learning）的方法把它打造出來的。

[00:33:42] 聽起來都很合理，但是大家心中一定會有另外一個想法：我知道分數高的話我當然就知道它是好動作，但是我就是不會下棋，我怎麼知道這個是好動作呢？Q-Learning 的訓練資料到底怎麼來的？

[00:34:44] 這個想法非常非常的簡單。訓練資料怎麼來呢？就是我們想辦法讓電腦自己去玩，然後我們就去估計這個 Q 值到底是多少。

[00:34:54] 開始的時候是用估計的方式，讓電腦一直玩。開始的時候那個估計真的很差啦，有點用猜的。然後估計這個 Q 值多少，我們就讓電腦去學這個「他自己估計出來」的資料。這個是重點：他自己估計說這個時候的 Q 值是多少，然後自己去學自己編造出來的這個資料。

[00:35:24] 學完以後，雖然開始的時候估計真的很糟糕，但是學了以後他的估計會越做越好。簡單的說，第一點就是我們先讓電腦自己去玩，然後去產生一些訓練資料。雖然開始的時候玩得很差，但是會越玩越厲害。在夠厲害的時候，我們就說他學成了。

[00:36:31] 所以他是自己產生訓練資料、自己去學習。這個訓練資料唯一不一樣的地方就是：這個訓練不是我們人類準備給他的。所以很多我們都會叫他是 Unsupervised Learning（非監督式學習），因為不是我們手把手介紹那個訓練資料給他。

[00:36:51] 但是做深度學習三巨頭——Yann LeCun（CNN 之父），他就覺得說：因為他是自己準備訓練資料，沒錯，不是我們人準備的，所以他不能叫做 Supervised Learning。但是問題是說，他還是自己去學自己準備的訓練資料，所以應該叫做 Self-Supervised Learning (自監督學習)。這只是詞的歸類啦。

[00:37:36] 總而言之，就是這個訓練資料基本上就是我們要自己想辦法去打造出來給我們的 Q 函數去學。

[00:37:52] 我們需要的訓練資料每有一筆大概就長這樣：某一個狀態 ($S_t$)，然後我們做了一個動作 ($A_t$)，然後我們就要去估算這個 Q 值 ($Q(S_t, A_t)$) 是多少。這是一筆訓練資料的長相。

[00:38:34] 那到底這個訓練資料是怎麼準備的呢？我們現在來欣賞一下。Q 函數基本上就是估計它的 Reward。目標就是 Reward 越高越好。所以 Q 函數最標準的想法就是想說那個 Reward 是多少。

方法一：蒙地卡羅法 (Monte Carlo Method)

[00:39:06] 一個很有道理的計算方式是說：我在某一個時間點 $S_t$，做 $A_t$ 這個動作，那它最後會得到多少 Reward 呢？就是 $R_t$ 加上下一次它又得到 $R_{t+1}$，加上下一次...最後就得到 $R_T$。所有的得到的總分就是這些。

[00:39:41] 這種叫做「蒙地卡羅法」。為什麼？因為蒙地卡羅就是有一點點隨機的——我不好意思說亂槍打鳥法。總而言之就有點隨機的去做。因為你想說，我現在這個動作做了 $A_t$，下一次的動作不一定是做... 應該說，每一次玩的時候動作不一定一樣，所以跟後面其實會有很大的關聯跟影響性。所以它其實只是「其中一次」的玩的經驗。

[00:40:20] 但是這也是合理的，反正總是有一次他玩了之後，他就得到了這麼多的分數。但是更合理的是，因為這一次我們是確定一定是由 $R_t$，但下一次是不是得到 $R_{t+1}$ 其實不確定的。所以我們會把它乘上一個 Discount (折扣因子，例如 $\gamma = 0.8$)。因為下一次沒有那麼確定，然後再下一次就是再乘上那個 Discount 變成平方。這在某種程度上也保證了這個級數會收斂。

[00:41:22] 所以第一種方式就叫做蒙地卡羅法。說起來也好像很高級的樣子，蒙地卡羅法其實就是亂玩啊（或者請專家玩），反正就是一個玩的範例而已。

[00:41:47] 但是第一種方式你認真想的話有點麻煩。因為你真的要記錄某一段玩的時間，你都要記錄下來，你才可以決定這個 Q 的值。這真的有點點麻煩。所以大部分的情景我們其實沒有這樣做，因為真的太麻煩了，你要真的在監控他玩一陣子，還要記錄他玩一陣子的樣子。這第一個問題。第二個問題，它的相關性會很強。
【生成式 AI】13. 強化學習和 LLM 對齊 - 逐字稿 (Part 2)

時間範圍：00:42:00 - 01:21:00

（接續上篇：Q-Learning 的蒙地卡羅法缺點分析）

[00:42:00] 我們剛剛提到的第一種方式（蒙地卡羅法），你認真想的話有點麻煩。因為你真的要記錄某一段玩的完整時間，你都要記錄下來，你才可以決定這個 Q 的值。這真的有點麻煩，而且它的相關性會很強。

[00:42:15] 為什麼？因為你在玩同樣的情境之下，你不只可以決定 $(S_t, A_t)$ 這個分數是多少，在 $(S_{t+1}, A_{t+1})$ 的時候，你也可以決定從 $R_{t+1}$ 開始加起。你會發現同樣玩的一次記錄，你可以產生很多筆訓練資料，但是這些訓練資料的相關性實在太高了。你又不想說下一次又要完全讓它重新再玩一次才產生一筆新的訓練資料，這樣產生訓練資料的速度又太慢。

[00:43:00] 所以在這種考量之下，就想說有沒有更有效率的方式去決定這個 Q 值？即使這個有一點點是預估的。所以我們的目標就希望說在 $S_t$ 的狀態之下，我們要想辦法生出一個 Q 值來。

[00:43:22] 所謂「玩一次」的動作紀錄，就是在 $S_t$ 這個狀態，我們決定做了 $A_t$ 這個動作。這個 $A_t$ 這個動作如果有即時的獎勵，我們就得到獎勵（$R_t$）；也許在這個時間點還沒有得到獎勵沒關係，反正就是 0 也沒關係。然後就進入到下一個狀態去了，就是 $S_{t+1}$ 這個狀態。

[00:43:47] 這些都是確定的。只要玩一步之後，我們就會有一個確定的動作樣貌。那我們有沒有機會在記錄這一個動作——就一次的動作——我們就能夠產生一個 Q 值？

Temporal Difference (TD) 與 Q 函數的更新

[00:44:02] 最合理的想法是這樣：現在因為我們有 $S_t$ 到 $A_t$，所以這個是輸入的部分。那我們確定一定會得到的分數就是 $R_t$。那後面的 Q 值是什麼呢？因為後面就到了 $S_{t+1}$ 這個情境。我們就很聰明的假設：在下一個狀態之下，我們會選擇最好的動作。

[00:44:32] 反正我們的心智當然是這樣了，所以在全部的動作裡面，我們就選最高分的，也就是它有辦法得到最高分的那個。

[00:44:51] 我們注意看這個數學式子：


$$Q(S_t, A_t) \approx R_t + \gamma \max_{a} Q(S_{t+1}, a)$$

如果你認真看的話，你就會發現只要我們剛剛的資訊：我們知道 $S_t$ 是什麼、我們知道在這個時間點我們做了什麼動作 ($A_t$)、我們知道這個時間點得到的分數 ($R_t$)、我們也知道下一個時間點的狀態 ($S_{t+1}$) 是什麼，我們就可以決定這個數值了。

[00:45:15] 所以這個真的是比較容易做的，就是「玩一筆」，我們就可以決定一個數值。這比剛剛簡單非常非常的多。我們只要一直記錄這些玩一筆、玩一筆的動作，根據這個我們就可以準備訓練資料。

[00:45:41] 但是這邊會有一個唯一小小的問題：就是我們在最右邊的那邊，我們要取得所有動作的最大值。但是你就會想說：「不對啊老師，我們現在就是要學 Q 函數，意思就是 Q 函數我們還沒學會，那為什麼會有後面的這個動作？也就是 $Q$ 帶 $S_{t+1}$ 然後把每一個動作都帶進去得到一個分數？」

[00:46:07] 這答案就是：我們很奸詐的把我們「目前的」神經網路已經學到的 Q 函數——也就是我們叫它「老 Q 函數」好了，比較老的那個 Q 函數——拿來用。因為即使是神經網路開始的時候，我們知道說我們一定要有一個初始化的動作，我們就會帶入一些值嘛。反正不管怎麼樣，我們的神經網路帶入一些東西，它一定會有一個值出來。

[00:46:36] 所以我們就是可以直接把它帶到舊的、我們現有的神經網路去，算出這個 Q 函數的所有動作的分數，然後我們找到最高分的，然後就帶進去了。

[00:46:51] 我再說一次，這個時候我們其實是沒有那個 Q 函數的值喔，但是我們在學習的過程中間，我們的神經網路會一直更新。也就是說，右邊的那個式子其實是我們用我們的神經網路「自己唬爛出來的答案」。

[00:47:12] Deep Q-Learning 非常有趣，我們用自己唬爛出來的答案，然後自己去學自己。那為什麼會變好呢？就是因為我們在不斷的學習過程當中，我們的這個「呆萌 AI 機器人」會越學越好。

[00:47:29] 開始的時候真的有點差啦，開始學自己的時候真的會有點誤導，因為開始自己都是亂做的嘛。但是你越學越久的時候，你會越學越好。所以我們預期說我們未來的 Q 函數（新的 Q 函數）會比前面的 Q 函數還要好。

[00:47:53] 也就是我有一個舊版的 Q 函數跟新版的 Q 函數。我用舊版的 Q 函數去訓練，得到新版的 Q 函數；然後再用新版的 Q 函數再去產生訓練資料，去學那個更新版的 Q 函數。這樣一直學下去，然後我們希望它越學越好。

[00:48:10] 簡單的說，Deep Q-Learning 是一個非常有趣的學習方式。它不但自己做訓練資料，而且開始的時候就是自己去學自己。因為我們每一次都有稍微的一點點的更新——那個「玩的經驗」（$R_t$）是真的，那一步經驗是真的——所以我們就希望從那一步經驗裡面，我們得到更多的經驗，然後去把真的自己一直去更新自己、強化自己，然後把自己學好。

[00:48:40] （中場休息）

[00:48:41] 大家好，我們現在看一下。其實這個 Q-Learning 真的是非常的有趣，它就是自己...我們唯一要做的事情就是去記錄我們不管是在玩遊戲的過程，或是任何我們想要做強化學習的過程。

[00:49:02] 簡單的說，就是我們並沒有想要教它，或什麼自己也不會、或是不太會啦。這個我們也有做過，就是讓它去操作股票交易。一直有很多的同學對強化學習做股票交易非常有興趣，因為基本上它可以自己看著過去的資料，然後它就自己去練習學習交易。你就跟它講說目標就是要賺越多的錢越好。

[00:49:43] 它就是自己去學自己，我們唯一要做的就是一直記錄中間的過程：從某個狀態如果做了某一個動作之後，它會得到多少的 Reward，然後它下一個狀態是什麼。就是一直去記錄這些，那我們就可以去做 Deep Q-Learning，也就是說我們就可以完全全的交給神經網路自己想辦法去學。

[00:50:18] 再一次，它就是自己學自己的過程。我們的訓練資料的產生方式就是：利用上一次的（老 Q 函數），把所有情況（下一個狀態的所有動作）都帶進去，然後選最高分的這一個。

Epsilon-Greedy Policy ($\epsilon$-Greedy)

[00:50:58] 開始的時候，因為我們要叫電腦自己去玩嘛，電腦其實自己都是亂玩的啦。但是我們希望說它未來會因為學得越來越好，它應該會越玩越好。它越玩越好之後，它去訓練的 Q 函數應該也跟著進步越來越好。

[00:51:26] 那要讓它去玩的時候，會有幾個有趣的策略。第一件事情，理論上我們應該都是選擇哪一個動作是最好的動作，也就是會得到最高的 Q 分數，那就應該是最好的動作。我們應該每一次讓它玩的時候都是得到越高分越好，所以我們的動作就是選分數最高的。這很合理。

[00:51:58] 但是有一點不合理的地方是：因為開始的時候那個 Q 函數超爛的。如果我們選最高分的方向去玩的話，那個叫做 Greedy Policy (貪婪策略)。雖然說是貪心，其實就是我們訓練完成了之後通常是用這樣的策略去玩的。但是問題是訓練過程中，我們的 Q 函數還超爛的時候，我們勇敢的這樣做的話會發生很大的慘劇：就它可能會被我們那個爛爛的 Q 函數誤導，常常玩得很爛，而且找不到那個...因為它就一直根據之前的經驗去玩，那之前已經玩得很爛了，它又一直學過去很爛的經驗，所以就一直很爛。

[00:52:51] 那怎麼辦呢？在這樣情境之下，我們就會用一個叫做 Epsilon-Greedy Policy ($\epsilon$-Greedy)。也就是說，在開始的時候，我們希望是「隨機一點點」，讓電腦隨便去玩，去發現更多可能的形況。然後在它玩得越來越好之後，再比較靠 Q 函數一點點。

[00:53:21] 所以我們會設定一個 $\epsilon$ (Epsilon) 的值。只要隨機取一個亂數出來大於 $\epsilon$ 的時候，我們就是用 Greedy Policy，也就是用 Q 函數去決定；如果小於 $\epsilon$ 的時候，就讓它亂玩一通。

[00:53:41] 所以開始的時候，可能乾脆設成 1 啦，開始的時候就讓它亂玩。然後在時間推進的時候，我們相信它越學越好的時候，就讓 $\epsilon$ 越小。那最後在真正使用上面，就完全交給 Q 函數去玩。

[00:53:57] 這個就是 Q-Learning，它是一個非常有趣的一個學習的方式，也就是自己去訓練自己。我們不需要自己真的很會做這件事情，我們不需要手把手的教電腦說電動玩具要這樣子玩，只要告訴說它這個可能的動作就是這些，然後它就自己去嘗試、自己去玩。它摸索過程它的目標就是想要得到這個越高分越好。基本上 DeepMind 在 AlphaGo 的時候基本上是用 Deep Q-Learning 就是這樣。

LLM 與 Policy-based Learning

[00:54:41] 但是 Deep Q-Learning 有一些缺點。所以呢，到了我們的大型語言模型（LLM）好了。剛說了一節 Q-Learning，現在要開始翻盤。也就是說，大型語言模型其實根本就不是...大部分的情景之下，我們不是用 Q-Learning，我們是用 Policy-based 的方法。

[00:55:01] 等一下大家就會覺得：「老師你自相矛盾，剛剛才說訓練資料很難準備，現在又說我們要用 Policy-based，到底什麼意思？」

[00:55:12] 我們在討論如果真的在很多的情況不能用 Q-Learning 或是用 Q-Learning 不方便的時候，我們到底要怎麼辦？Q-Learning 到底有什麼樣的缺點呢？

連續數值或無限動作：因為我們要從所有的動作選最高分的，但是有一些動作如果是連續的數值的話怎麼辦？比方說今天要決定的數值可能是從 0 到 1 中間的任何一個數字，它不是有限個選擇，它是無限多個選擇的話就變得有點麻煩，因為我要從裡面找到那個 Q 值最大的，這個就不是那麼好找。

動作空間過大：雖然像是大型模型，它輸出的可能的字是固定的（比方說 50 萬個 token），它是有限的。但是問題是說，要把每一次你在做一步的時候，你都要考慮 50 萬個可能性並計算 Q 值，其實這個計算量也太大，其實是不實際的一件事情。

[00:56:28] 所以在這樣的情境之下，大家就覺得說那這樣子要怎麼辦呢？那我們是不是還是需要直接想辦法讓它去學 Policy Function？

[00:56:37] 但是 Policy Function 剛剛我們已經說過了，我根本就不知道正確答案是哪一個，但是我就是想要去學這個函數。目標就是這樣子。我們根本不知道怎麼準備訓練資料，但我們目標要學這個函數，到底要怎麼樣做呢？

[00:57:01] 突然就出現了一個很偉大的事情。我們的目標其實很清楚，就是我們雖然不知道每一個狀態的時候它的最好的 Action 是在哪裡，但是我們的目標很清楚：就是我們要極大化我們的 Reward。

[00:57:24] 那可不可以把我們「極大化 Reward」這件事情放在我們的「目標函數」？也就是我們的 Loss Function。那你當然說：「老師你以前都跟我們講 Loss Function 是要越小越好。」好，可以，那我們就把那個改成負號嘛，負的極小化那就是極大化我們的 Reward。

[00:57:55] 因為我們會用 Gradient (梯度) 去學習，所以這個方法叫做 Policy Gradient。

Policy Gradient 的概念

[00:58:04] 我們目標就是要學一個目標函數，是跟我們可愛的 $\pi$ (Policy) 有關的。最簡單就是最大化我們的 State Value，也就是說我們想辦法每一次都朝向那個會得到最高分的那個 Value 去走。

[00:58:54] 但是問題是什麼呢？問題是我們不知道這個狀態好還是不好。再一次，我們下圍棋，除非那個下棋可能要下到最後一步，我才知道這個狀態真的好（這是贏定的狀態），但是大部分中間的過程其實我們不知道哪一個狀態是好的。

[00:59:20] 我現在來跟大家介紹一下。在這種 Policy Gradient 的過程當中，我們就沒有辦法像 Q-Learning 我們只記錄其中一小段，我們可能要有一個比較完整的一段（Trajectory）。這不一定是一定要從頭到尾了，反正就是一段。

[00:59:39] 這一段就是比方說有 $S_1$，然後我在 $S_1$ 這個狀態做了 $A_1$ 這個動作，然後得到了 $R_1$ 這麼多的分數；然後我就進入到 $S_2$ 這個狀態，然後 $S_2$ 這個狀態我就做了 $A_2$ 這個動作，我就得到 $R_2$ 這個分數...反正我們就是讓電腦去玩的話，它就會得到這一串的記錄。

[01:00:03] 我們其實希望做的最大化，就是希望說這個 $R_1 + R_2 + R_3 + \dots + R_T$，就這一段裡面我們要得到最高分。

[01:00:19] 可是大家看仔細了以後就發現了這個非常悲傷的事情，因為這個沒有地方可以學。玩了一段以後，我們當然可以算出這個 $R_t$ 這個分數來，但是沒有地方可以學。原因就是因為我們根本不知道哪邊跟 $\pi$ 有關係。

[01:00:36] 所以大家就很聰明的做了一件事情，做了一個這看起來很可怕的式子：


$$\nabla J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \left( \sum_{t=1}^{T} \nabla \log \pi_\theta(a_{i,t}|s_{i,t}) \right) \left( \sum_{t=1}^{T} r(s_{i,t}, a_{i,t}) \right)$$

可是你更認真的看的話，你就會發現這個好像跟我們以前的可愛的 Cross Entropy 在做的時候好像看起來就「八七分像」。沒有錯，就是這樣子。

[01:01:00] 簡單的說，右邊這個式子基本上就是希望我們的神經網路把這個動作「學得越像原來的動作越好」。但是，萬一這個動作做得還很爛，你就千萬的不要再去學它。所以前面有乘上我們「最後得到的分數」。

[01:01:52] 這個得分數越高，我們會越獎勵它往這個方向走。那得分數低的話，雖然它也是學這個動作，但是我們就沒有那麼高的獎勵。所以自然而然，它就會傾向往那個比較高分的方向去走。

[01:02:14] 雖然我們所有的都教它喔，今天這個玩得好的我們也給它當訓練資料，玩得不好的也給它當訓練資料，目標都要學像。但是問題是玩得好的學像我們會給它比較高分的獎勵，所以它就會比較往那個比較玩得好的那一種方式去學習。

[01:02:43] 簡單的說，如果我們不能夠用 Q-Learning（因為 Action 太多種），那我們不得不還是需要用 Policy-based 的 function 去學習。那 Policy-based 去學習的時候，就是我們想辦法讓我們的模型去學比較高分的這種動作。

LLM 的對齊 (Alignment)

[01:04:00] 好，那我們就要進入到我們的大型語言模型，到底怎麼樣把這個可愛的強化學習給帶進去的。

[01:04:27] 我們在過去介紹大型模型的時候，我們比較強調的就是它的「基礎模型」的訓練，也就是說預測下一個 Token 這件事情其實是大型模型做的事情。我們過去也一直在強調這件事情，所以它會胡說八道等等基本上都是從這邊來的。

[01:04:49] 那我們比較少說到的事情是：其實通常在一個語言模型（比方說 ChatGPT 或是任何的語言模型）在拿出來給我們看到的時候，它除了基礎模型的訓練，它其實還會做一個很重要的動作，叫做「對齊 (Alignment)」的動作。

[01:05:06] 這個順便說一下，這個對齊的動作我們其實可以用更淺的話去說，但是因為大家都是說 Alignment，所以為了讓大家在外面溝通的時候，讓所有人都覺得我們真的是受過良好教育訓練出來的、真的是專家，所以我們還是要把這個詞交給大家，叫做 Alignment。

[01:05:31] Alignment 的目標到底是什麼呢？就是希望我們的語言模型做到三種的 Alignment：

Value Alignment (價值觀對齊)：跟人類的價值觀是相符合的。意思就是說，這個語言模型輸出來的時候，不可以是鼓吹暴力的、不可以是有種族歧視的等等。

Intent Alignment (意圖對齊)：我們希望說它...因為有時候啦，我們的意圖老實說有時候也不是這麼的清楚。我們在正常人說話的時候，我們說出來的話是這個樣子，在文字面上解釋可能是這樣，但是真正的人都聽得懂說我們的意思到底是幹什麼。我們希望這個大型語言模型也能夠做到這件事情，非常的能夠抓到我們的意圖。

Role Alignment (角色對齊)：它的角色其實就是一個很好的輔助的工具，所以希望說它的角色就不會是再會傷害人類的，它的角色就是要幫助我們的等等。

[01:06:59] 簡單的說就是大型語言模型說出來的答案是符合我們的需求的。

Pre-training 與 Post-training

[01:07:10] 再說一次，這不是我們一般使用者做的事情，這在所有的語言模型他們要推出來之前都會做的。

Pre-training (預訓練)：基本上就是我們最標準的「訓練下一個 Token」、預測下一個 Token 這種文字接龍模型。

Post-training (後訓練)：Pre-training 基礎模型已經訓練好了，它就是很會預測下一個詞了。那 Post-training 階段最重要的工作就是要做對齊的動作。

[01:07:43] Post-training 又有兩個高級的詞，今天不管怎麼樣大家也要記得，就是叫做 SFT 與 RLHF。這兩個詞念出來聽起來也感覺我們自我感覺良好，就是我都說得出這麼高級的詞。

SFT (Supervised Fine-Tuning)

[01:08:19] SFT 事實上是 Supervised Fine-Tuning，意思就是手把手教它。就告訴它說：「人家問了這樣的問題，你就是要這樣子回答。」那誰是這樣回答？就真人回答示範給大型模型看。

[01:08:36] 就是今天我們自己問的一個問題，然後我們就真人去回答。你會發現說 ChatGPT 為什麼每次回答都這麼有結構？問他簡單的問題它就很會有結構的第一條、第二條、第三條。那其實就是我們真人回答給它看，就是你就是要這樣子回答人家的問題。

[01:08:59] 所以 SFT 其實就是標準的 Supervised Learning，也就是說它輸入的時候，正確答案就是我們人類告訴它的。

[01:09:15] 但是要注意的事情：SFT 在大型模型其實它在基礎模型已經看了很多很多的東西了，基本上它已經很少機會可以學到什麼新知識。SFT 的目標不是我們在拿到語言模型的時候我們會做的 Fine-tuning（那是另一回事）。SFT 是模型推出之前，他們希望：比方說大家都覺得 ChatGPT 好像講話很有條理、或是它會拆解問題的方法、或是它在回應的語氣等等。

[01:10:31] 這些才是 SFT 真的要訓練語言模型的地方。因為 SFT 的資料其實並沒有真的很多，你不可能產生非常非常大量，因為 SFT 在這個時候是我們由人類自己寫答案給語言模型看。所以比訓練資料更重要的事情是：這個「風格」一定要一致。因為我們就是要教模型你回答問題的風格應該長這樣，還有它的品質要是我們真的希望說語言模型輸出來的這個真的符合使用者的需求。

RLHF (Reinforcement Learning from Human Feedback)

[01:11:24] 再來第二件事情，就是我們開始要訓練大型模型...這用 Reinforcement Learning (強化學習) 訓練大型模型，就是我們希望答出來的更符合人類的希望看到的語言模型答話的樣子。

[01:11:43] 在這個時候我們想要做 Reinforcement Learning 了。那 Reinforcement Learning 有一個很重要的事情是決定它的 Reward Function 是多少。也就是我要告訴語言模型說它答得好還是答得不好。

[01:11:55] 我們可以怎麼做呢？我們當然可以在讓那個語言模型唬爛的過程中間，比方說我問一個語言模型（像 ChatGPT），它就開始唬爛三個答案給我。那我當然可以請人來給它分數。但是人給它分數的時候，你就會發現這件事情非常困難，非常難做到一致性。

[01:12:17] 就是我們自己想，如果你有機會要改很多很多的考卷的分數，這個老師跟助教們都很有這種經驗。假設我們今天有一個期末分享，然後是需要請所有的同學幫所有同學評分，有 40 組的同學。你就會發現你自己的標準都會浮動：第一組你可能比較嚴格一點點，給他的分數就稍微低一點；然後你看了十個以後發現第一個其實還不錯，那你分數開始浮動了。所以你要讓很多人去評分這件事情，給它一個明確的分數，這件事情是困難的。

[01:13:17] 所以 OpenAI 就想了一個很好的、但是又希望是人類評分的方式。因為雖然決定每一個別的分數比較難，但是你覺得這個唬爛 A 跟唬爛 B 好像 A 說的比較好一點點、那 B 比較差一點點，這件事情是簡單的。你只要比較這兩個這兩個到底誰比較好、比較差。那萬一 B 跟 C 你真的分不出來，你就說等於啊，這樣可以吧。

[01:13:47] 所以你就可以達到由人類幫忙去標記說這個三個答案裡面他最喜歡哪一個，然後他覺得哪一個其實跟哪一個其實是差不多的。這對每一個人都是簡單的，所以就是比較可以做這件事情。

Reward Model (獎勵模型)

[01:14:09] 所以我們的目標就是訓練一個函數，這個函數就是評分函數。這就是一個神經網路了，就要去做去訓練一個評分函數。它輸出的時候就是它的評分。

[01:14:27] 但是你就開始又抱怨：「還不對，老師，你剛剛就說訓練資料很難給一個正確的評分。」我們怎麼做呢？我們就去設計 Loss Function。

[01:15:02] 今天假設 GPT 說兩個答案，第一個答案叫做 $y_w$ (Winner)，第二個叫做 $y_l$ (Loser)。就是他說兩個答案，人類標記 $y_w$ 那個分數是比較高的，比較好。所以我們的評分系統裡面應該要把 $y_w$ 給比較高分，$y_l$ 給比較低分。

[01:15:27] 於是我們就設計成 Log Function (Sigmoid + Log)：


$$\text{Loss} = - \log(\sigma(r(y_w) - r(y_l)))$$

如果我們評分正確的話（$r(y_w) > r(y_l)$），相減是正的很大數值，Sigmoid 就很接近 1，帶 Log 幾乎就是 0，也就是沒有扣分。如果是相反的話（$r(y_w) < r(y_l)$），相減會變小於 0 的數字，Sigmoid 就會很接近 0，Log 0 就會產生負的很大值，再弄上負號就變成正的很大值（Loss 變很大）。所以神經網路就會被扣很多的分數。

[01:17:23] 因此它就會漸漸的往我們人類標記的分數去走。所以我們最後有了 Reward Function 之後，我們就想辦法讓我們的 ChatGPT 真正的學會說他要往高分的方向去做。

PPO (Proximal Policy Optimization)

[01:17:54] 所以用強化學習的方式讓他慢慢的朝我們希望的方向、接近人類喜歡的方向去走。因為這個我們標記是用人去標記的，所以這種方法叫做 RLHF (Reinforcement Learning from Human Feedback)，就是由人類去標記的人類的回饋，然後來決定這個 Reward Function。

[01:18:53] 這個大家常常會在討論大型模型的時候常常會看到這個詞。你現在不但要知道這個詞，要知道它後面到底發生什麼事情：它就只是人類的標記，然後去訓練我們的神經網路可以朝著我們比較喜歡的方向去回答這個問題。

[01:19:34] 它就是用 Policy Optimization，就是直接去學習策略函數。這邊有一個詞我們就不仔細講，這個 PPO (Proximal Policy Optimization) 到底發生什麼事情。PPO 就是 Policy 的訓練方式。

[01:20:09] 那 PPO 跟我們一般的 Policy Gradient 到底有什麼不一樣呢？因為 Policy Gradient 基本上是沒有限制的，它的 Gradient Descent 做出來的時候，它就往這個方向勇敢的走前進。那 PPO 簡單的說就是有一點點「減速」的動作，就是不要讓它做那麼快。

[01:20:23] 有點像我們以前在調 Learning Rate 一樣，就是不要讓它動作這麼大。每一次都修改一點點，免得讓我們原來的那個可愛的神經網路（已經很會預測下一個字的模型）突然被我們打壞了。因為根據我們的訓練資料（SFT/RLHF 的資料畢竟佔比較少的比例），怕那個訓練資料被我們影響之後，把那個原來的模型都給破壞掉。所以這個就是 PPO 做的事情。

[01:20:57] 所以再一次，就是 PPO 其實就是 Policy-based 的 Reinforcement Learning，然後它是比較溫和版的，就是控制它的學習的速度而已。

【生成式 AI】13. 強化學習和 LLM 對齊 - 逐字稿 (Part 3)

時間範圍：01:21:00 - 02:00:00

（接續上篇：PPO 減速機制與 RLHF）

推理能力 (Reasoning) 與 Chain of Thought

[01:21:17] 另外一個很有趣的是，現在所有的語言模型都說它是會推理的。在我們討論到 AI Agent 的時候，我們有說過其實推理的時候，基本上我們可以用後來再來訓練它，也就是說我們設計 AI Agent 時先讓它產生它所謂的推理，然後再來讓它回應。這就是我們之前在 Agent 學習的時候知道這件事情。

[01:21:55] 但是有沒有可能呢？特別是它要去學推理的時候，它就需要產生這個所謂的推理的過程。我們的 AI Agent 的分類，我們把它叫做 Planning (規劃) 的過程，就是先打草稿。我們可能在想一想的話，我們會想到一件事情：是不是要讓它...如果我們這個可愛的語言模型很會打草稿的話，那它的推理會比較好，那它後面的答案應該相對也比較好。

[01:22:34] 所以像是 DeepSeek 就是非常專注在想盡辦法讓它很會打草稿。它在「很會打草稿」的這一段，它其實是在 Post-training (後訓練) 階段，也就是說它還沒有推出來的時候，就好好用這個 Fine-tuning 的方法——包括說 SFT 還有我們剛剛說的 Reinforcement Learning (強化學習)——去讓它去學習。基本上有的用很多地方做 Reinforcement Learning 讓它去學習。

使用 RL 訓練推理能力 (無須人類標記)

[01:23:03] 在 Reinforcement Learning 的過程當中，我們當然可以用人類去做 RLHF (Reinforcement Learning from Human Feedback)。大家還記得，就是有我們人類去標記說它產生了三個草稿，那現在只是產生的可能不是最終答案，可能是產生三個思考的過程（三個計畫）。然後我們有人類去標記說這個比較好、這個差一點、後面兩個差不多。當然可以做這樣的事情。

[01:23:35] 但是你也想見說要做這樣的事情會需要耗費的人力的成本。這第一件事情。第二件事情，它又跟本來產生的答案不太一樣。因為現在是推理過程的時候，比方說我現在是要解數學題目，那它產生了三個解數學題目的過程：應該是這樣想、應該這樣想。那你就會發現，這個標記的人還要蠻會解數學題目的才可以啊，不然的話他沒有辦法區分說這個比較好還是那個比較好。

[01:24:13] 所以沒有辦法。所以說就變成說由人類去標記的話有一些困難，要等很多人去回應他才能去訓練。那可不可以...在某種程度裡面呢，就不要人類標記，我就直接用 Reinforcement Learning？也就是我就可以產生的 Function，可不可能做這樣的事情？

[01:24:39] DeepSeek 是用了一個很聰明的辦法，事實上現在很多語言模型都用了這個辦法。為什麼用這個辦法？因為現在在語言模型裡面有很多的標準測驗叫 Benchmark。有一個 Benchmark 就是做數學題目，做很多數學競賽的那些題目。

[01:25:22] 我現在要訓練我的語言模型很會思考數學題目，這聽起來好像很複雜？沒有，DeepSeek 想到一個很簡單的方式。就是呢，因為我們現在就是要產生這個思考嘛，所以在每一個問題，它就會產生思考的過程。然後思考的問題答對了以後，我們就說：「啊，你思考得很好！」根本沒有人去檢查過程。答對了就得到一分，答錯就扣一分。就這麼簡單。

[01:25:55] 所以他就不用人去標記，他就可以去訓練。我用 Reinforcement Learning 去訓練它的語言模型，而且訓練的就是它的所謂的推理的過程。

[01:26:07] 我再說一次，大家如果知道原理之後就發現說這個好像真的有點冒險。沒有錯，就是因為我們只看結果。那個推理過程大家也知道，數學題目或是任何的題目去算的時候，有可能你的過程是錯的，但是你會得到正確答案。這個在選擇題、填空題是可以的，但是問題在計算題就一定會被打零分。

[01:26:38] 但是現在因為我們沒有人力去看這個中間的過程，所以它真的有可能是錯的，但是問題是它會得到正確答案，所以在這個時候它就會在 Reinforcement Learning 裡面自動的告訴它這是好的方向，你就應該這樣想。那不管怎麼樣，它就是靠這樣想了。所以它的正確率還是沒有到 100%（不一定，還有其他的原因啦），但是它的正確率相當的高。這就是 DeepSeek 等等的語言模型那個時候怎麼樣訓練它去做推理的。

[01:27:10] 當然呢，更好的方式可能是你直接叫人說：「這一題這個最厲害的解題專家，請問你到底要怎麼樣解題？」他就開始寫說我們的思考過程應該看到這樣題目應該這樣想、這樣想。第一個呢是這樣子當然可以，但是問題是這樣子要去標記的話，這個人力要很多，不太容易。第二個困難的點是說，因為這個語言模型問的問題或是數學題真的是包山包海，太多種狀況了，所以要每一種情況都是幫它 Cover 到這件事情不是這麼的容易。

[01:28:01] 所以說呢，用 SFT 再一次它其實也可以，但就是人類直接示範給語言模型看，當然也可以。

總結語言模型訓練流程

[01:28:09] 好，這個就是在語言模型的大概已經是比較完整的介紹了。

Pre-training (基礎模型)：就是預測下一個 Token 的模型。

Alignment (對齊)：包括 Post-training 階段。我們希望語言模型要比較對齊，包括價值觀對齊、回答問題讓我們人類感覺比較舒服一點。每一個語言模型其實基礎的訓練到現在可能已經沒有差距這麼大，但是問題是說他在這個對齊的方式，他們都會下各種的功夫去做這樣的事情，讓你覺得好像你比較喜歡這個 Gemini 來、或 GBT 的回應。

Reasoning (推理)：我們現在大家都很會推這個語言模型的推理能力。那很多的語言模型現在在訓練的過程中間，意思就是說不是用 System Prompt 去引導它去推理，而是在訓練的過程中間就想辦法讓他在用 Reinforcement Learning 的方式等，去想辦法讓他會去做推理的這個動作。

蒸餾 (Distillation)

[01:29:40] 那我們還有一件事情是沒有解釋的，就是大家可以想見，你要做一個會推理的語言模型，特別是它不是只會解數學問題，包山包海什麼都會推、什麼東西都推得相當好。這件事情通常這個語言模型本身要很大。第二件事情呢，這個語言模型通常要耗費的資源去訓練它很會推理。

[01:30:19] 那不管哪一個呢，都是非常耗費資源的。所以現在一個很常見的方法呢，是我們就是想要蒸餾 (Distillation)。也就是說呢，由學生去學老師。學生是一個比較小的模型，老師是一個大模型。

[01:30:41] 蒸餾的方式就是因為語言模型出來的時候，它就會出現 Softmax，Softmax 之後它就會產生一堆的數字，就是每一個文字接著下一個字的機率應該是多少。那我們就請學生直接去學老師說的答案，而不是說從真實世界的那個文字裡面去訓練學生說下面一個字他其實是接這個字。

[01:31:14] 不是，學生是去學老師。你會發現說這個可能在比較小的模型就可以跟老師做得差不多，然後也用比較小的模型、在比較快的時間點就讓他學會這些東西。

[01:31:33] 特別是我們想要訓練那個他會推理的時候，大家就發現了真的因為包山包海的問題，所以你要訓練老師模型，可能老師模型真的要非常大，大到不合理。什麼叫大到不合理？大到不合理就是說，比方說 OpenAI 想要推出一個 GPT，但是他的老師模型可能是大到他覺得 ChatGPT 這個這麼大的模型其實不能夠讓大家去使用，因為一使用的話要耗費太多的資源，可能馬上就吃垮了 OpenAI 所有的運算資源。

[01:32:10] 但在訓練的時候可以，因為只訓練的時候基本上就只有我們在訓練它。所以呢，我們之後再用各種的方法，比方說 RL 好的去訓練它，訓練得非常好之後再去做蒸餾。

[01:32:30] 所以大部分現在的語言模型其實真正的最厲害、最強最大的那個老師模型其實通常是沒有公開的。因為它通常是一個不太合理的、就是一般人不太能跑的，或是說即使我們放在網路上給大家使用也不太合理、不太合經濟效益的。

[01:32:50] 那他到底最後怎麼做？他就是教好幾個學生，比較大的大弟子、還有二號二師兄等等。就教好幾個學生，所以會看到有不同版本的大小。那即使最大的版本也可能不是老師模型。這是一個很有趣的最近的發展。

[01:33:19] 我們就想要訓練一個很厲害的老師，然後他去教一些學生。這個叫做 Reinforcement Distillation (強化蒸餾)。反正就是強化學習，然後去簡單的說再一次，就是把老師教好，我們專注把老師教好，用盡全部的資源去把老師教好，然後老師再去教會好幾個學生模型出來。

課程結語與期末專案說明

[01:33:45] 好，這個就是今天為大家介紹的。所以今天再一次是比較理論的部分，但是這些理論呢，因為大家在外面去看到一些在討論一些大型模型的時候，或是說一些你參考到一些比較新的一些研究的時候，你常常會發現這些東西。那你了解這些東西，你就會知道說他們那些研究或發展出來的東西到底是什麼。

[01:34:34] 那我們作業其實跟今天一點關係都沒有（除了因為可能要用到語言模型啦）。那是因為我們上一週本來還是在圖像生成的部分，可是我們給大家的作業是希望大家在期末的時候先去想清楚我們的期末到底要做什麼。

[01:35:02] 我再說一次，那個期末這個專案如果大家一定要趕快想，因為我們真的已經要到期末了。如果你真的想不清楚或是你不太確定這個到底可以做還是不可以做，歡迎你找所有老師去討論，或是找我們在政大這邊的助教來討論。就是在这个 Office Hour 的時間，我們也有開放線上。

[01:35:36] 我們在期末的時候呢，所有的同學都請大家記得要做至少要做兩件事情：

YouTube 影片：請大家一定要錄一個 YouTube 的影片，希望就 3 分鐘左右，不要太長。最好你在開頭的時候就非常吸引人，因為根據研究，只要 30 秒抓不到他的話就永遠抓不到他了。所以請大家想盡辦法在前面 30 秒讓大家覺得你的做得太有趣了。

PDF 投影片：會希望大家都做一個 PDF 的投影片，比較完整的介紹你的這個專案到底在做什麼事情。如果你有程式的連結，當然也可以放在 PDF 裡面。或者說你中間其實有一些非常血淚的過程，那你一定要讓老師或是助教知道的，要寫得清楚。

[01:37:46] 總而言之，一定要有一個 YouTube 的連結是介紹你自己，然後第二個是有一個比較完整的 PDF 的簡報是來介紹你的期末專案。

今日作業說明：圖像生成 Prompt

[01:38:02] 然後呢，我們今天的作業呢就是用這個現在最近 Gemini 3 (Imagen 3) 出來了以後，大家因為都可以用 Pro 版本，所以非常非常的合適。

[01:38:21] 大家可以去參考一個 NoBanana 的 Prompt 網頁（註：推測為特定的 Prompt 參考網站），它有很多有趣的一些範例。盡量大家就是去真的弄懂中間發生的什麼樣的事情，然後盡量是用中文去描述這個中間發生的事情，就是你用中文去下這個 Prompt。

[01:38:45] 所以你可以參考這個網頁，當然你不可以直接抄那個網頁，然後只是改裡面的角色。比方說裡面畫一隻貓，你改成畫一隻狗，這當然不可以啊。所以你要說你到底受了這些網頁的有什麼樣啟發，或者你自己想也可以。然後你要去做的時候，你可以參考上面的網頁，但修改了什麼要說清楚。

[01:39:06] 或是說呢，你是先寫一個簡單的 Prompt 讓大型模型去幫你生一個比較完整的、可以畫出來圖像的描述，然後你經過這個你再自己去修改都可以。但是你要很清楚寫出你的過程到底怎麼做出來。

[01:39:28] 那當然有一些同學可能會用那個...然後畫出一個圖形，但是那個圖可能不夠大、就不夠寬，你可能要把它擴大一點點。那就可以再用 Fooocus 等工具（我們之前介紹的工具），然後你可以再把那些圖再做擴充。總而言之就是去做創作，但是你要把你很清楚的說出來你這個創作的過程、動機啊、理念啊，還有中間創作的過程。

[01:39:58] 好，這應該是一個很好玩的作業，那就請大家盡量的去發揮。那我們先休息十分鐘。

（中場休息）

學生閃電秀 1：AI 202X 未來預測故事

[01:42:00] ...（前段略）然後到年底的時候，有美國的某個科技公司叫做 OpenAI (聽打誤為 Open Brand)，他們發布了 Agent 1。然後在這之後，中國就發現就是在 AI 的發展上面已經追不上美國了，所以他們就是設立一個集中式開發區，然後想說可以加速。

[01:42:39] 美國也一直持續的在發展 AI。到 2026 年的年底，他們就發布了 Agent 1 的版本，然後也就是在價錢上面更平民，然後讓這個 Agent 可以更便宜大眾。一直到 2027 年，這是這篇文章最重要的一個點：在年初的時候，OpenAI 發布 Agent 2。然後就是中國就發現那個美國發展速度都已經快到已經真的要追不上，所以他們就是想說用竊取權重的方式去追上美國。

[01:43:24] 而美國他們就是發現中國有這個動作，就開始想說中國可能已經發展的速度都追上來了，所以說他們就是也比較不管安全上的問題，然後就開始做開發 Agent 的部分。然後到 4 月的時候就會發布 Agent 3，然後一直到 7 月的時候，他們就發布 Agent 3.5。然後這時候 Agent 它已經算是有 AGI 的程度了，就是在各領域上面都已經有人類的水準。

[01:44:02] 然後直到年底，他們又開始發布 Agent 4。然後就是美國的政府、他國家的單位就發現開始發現事態上的...就是感覺他們發展太快，太快到安全有一些問題。他們就設立一個監督委員會。

[01:44:25] 這個監督委員會呢，在這個時候就是有一個非常關鍵的一個決策，就是要繼續開發 AI？如果他們這時選擇了比較慢下來，就是不要那麼（快）開發的話，就可以迎來一個比較好的結局。但是如果他們即時就決定開發，然後去繼續跟中國做競爭的話，就會走向一個壞的結局。

[01:44:51] 所謂壞的結局呢，就是他們就是開發，然後在這之後就開始涉入了政治跟軍事。然後在 2029 年之後，中國跟美國兩邊的 Agent...然後他們的 Agent 就決定說要來簽署一個和平協議。但其實和平協議是假的，他們主要要做的事情就是透過這個協議，然後讓兩國可以合為一，變成更強大的...

[01:45:29] 然後他們成功合併成這個一個更大的一個 AI 的模型之後呢，他們就在後面持續擴張的時候，就是認為人類是一個（負擔），然後人類只會提供負的價值，所以在這之後就是適合生化的滅絕。以上就這次大概 AI 202X 的一個大概的時間預測的故事。

[01:45:52] 其實可能（大家）會覺得就聽完剛剛那些故事就覺得有點天馬行空，但是其實說這事實上這是一群非常頂尖人員他們所撰寫的一個推演的一個過程。所以說不管你覺得怎麼想，其實可以透過這篇文章，就是讓我們重新去審視：就是 AI 的 Safety 上面是否就是還要再投入更多的心力在這上面，否則可能未來的某一天我們可能會受到 AI 的反噬之類的。好，以上就是我這次 AI 202X 的分享，謝謝大家。

[01:46:37] 老師：謝謝，謝謝浩宇同學。那我們請那宇宏同學分享你的那個 PPT 還有你的那個鏡頭。

學生閃電秀 2：Cursor 編輯器介紹

[01:46:51] 宇宏同學：喂，同學你好，有聲音。好，那分享我的畫面。有嗎？好。那我來介紹一下，我今天的主題是我要來介紹 Cursor (CC code)，我是成功大學的。

[01:47:25] 然後我先介紹一下什麼是 Cursor，就是專門為程式設計師打造的 AI。然後我後面會有實體的畫面演示給大家看它實際能做什麼。它是由那個（Anthropic / OpenAI 等技術支持）開發的。然後它不只能寫程式，它更能理解你的整個專案，就是包含你可能網路上下載下來的程式碼，或者是同學留下來給你的程式碼，他都可以幫助你自己理解然後再修改。

[01:47:57] 它的核心目標就是來提升開發效率跟降低專案的複雜性。它的核心功能就是產生程式碼，因為它就是專門為程式設計師打造的 AI 工具。然後它也可以很高傚的為我們除錯並解釋為什麼會有這些錯誤，然後還可以幫我們整個重構程式碼跟優化。

[01:48:26] 但是目前 Cursor 比較大的也不是問題，就是它沒有免費。所以假如你要用的話，它就一定要「氪金」。就是它目前有兩個方案，就是 Cursor Pro 跟 Business。個人版的話，它訂閱一年的話是 17 美金（每月），然後假如是每個月訂閱的話會 20 美金。然後它右邊這邊就是它的一些限制。然後主要就是 Max，它主要多的地方就是它大概有 5 到 20 倍的（額度），就是你可以更多用這個 AI 工具。不然你可能用一陣子它可能要再等五六個小時。

[01:49:18] 好，然後接下來我就來實際給大家看一下我用 Cursor 簡單生成的一個專案。這個就是一般 VS Code 的話，它是會在最開始下載時候...但是你可以那個 VS Code 有套件，你只要下載之後它就可以點開這個橘色的，然後它就會像這樣右邊這塊顯示在你的那個螢幕上。

[01:49:50] 這邊就是你可以問問題，就是很像那個 ChatGPT 這樣，你就可以問他任何的問題，然後他這邊就會跑要怎麼解。然後這邊你也可以...它有一些模式可以切換。

[01:50:18] 然後我要演示的簡單的案子，就是跟之前上課也很相關。就是我用它來幫我打造一個 AI 的代理（Agent）。然後它可以判斷 Google 地圖的評價，然後他要回傳說他要預測說這個評價它大概是幾顆星跟店家該改善什麼。然後他就開始幫我生成。

[01:50:44] 然後這邊是他給的回應，就是他說他這邊有簡單的 Agent，就是他要叫商業洞察顧問。然後這個是它的 AI 代理的功能，跟它會做什麼功能。然後還會把它每個步驟它會怎麼設計這邊寫出來。然後接下來我就讓它繼續生。

[01:51:14] 然後就是他會直接幫我們寫 Code。然後那個套件因為我比較懶，所以我就直接叫他就是你也可以在 Cursor 上幫你下載你的依賴（Dependencies）。然後他就會幫你把那個做的依賴全部裝好。然後我依賴什麼也都是叫他裝。然後目前他就成功下載，然後依賴都沒有問題。

[01:51:41] 然後這邊是他寫的那個 AI 代理人的部分，他寫了三個：一個是專業商業洞察顧問、然後同理性顧問、跟那個數據群眾分析。然後右邊這邊就是它的測試。就是：餐廳環境很好然後很有感，但是後面就是有點些不便的，所以大概二到三顆星。然後這是它預測出來的兩顆星。

[01:52:12] 然後接下來它也有寫一個就是可以讓我互動。就是它在上面打，是我自己打，然後他就幫我去預測說大概是幾顆星。然後我就測試了三個，就是這個超級好，就給一顆；然後這個就是沒有那麼好，兩顆星；然後這個就是超爛的。

[01:52:42] 然後還有一個簡單的，就是比方說你有一個專案，你也可以直接叫那個 Cursor 幫你讀那個專案，然後可以直接修專案裡面。然後這個就是 Cursor 的簡單介紹。

助教時間：MCP (Model Context Protocol) 介紹

[01:53:12] 助教：好，謝謝宇宏同學。那今天是兩位那個同學分享閃電秀。那接下來是那個助教課的時間。

[01:53:24] 那今天助教課的時間，我想就是在老師有講一個就是在講那個 Agent 的時候講一個 MCP (Model Context Protocol) 的那個架構。那 MCP 我想要就是可以再跟各位同學講一下就是 MCP 一些細節，或是說在可能你們實作要怎麼去使用它。

[01:53:49] 那最主要、最主要為什麼要有這個 MCP？其實就是像說你們現在有很多各種的像是用網頁搜尋嘛，或是說你可能想要接你的 GitHub，或者是你想要接你電腦自己裡面的資料，或者是各種你覺得可能你有一些應用程式你想要把它跟 AI 來做結合嘛。

[01:54:18] 那早期的我們大概都是怎麼做的？早期我們大概都是用 Copy and Paste 嘛。就是我現在比如在 VS Code 運行了一個問題，或是我運行一個 Python 檔有問題的，我就把這個 Code 跟問題貼進來對不對？或者是說我就去網頁搜尋，然後把網頁搜尋的東西貼進我的這個 GPT。

[01:54:50] 那你現在如果有這個 MCP 的話，它可以有一個就是大家公認的這個架構，去把我們把這些東西可以接起來。那這個 MCP 這個架構是就是做那個 Claude 的那個公司 Anthropic 它去提出的。

[01:55:09] 所以一般來講說概念化的話，就是把它想成是一個 USB-C 的 Hub。然後比如我們手機啊或者是記憶卡什麼的，你如果有一個 Hub 的話，它是不是都可以直接跟我們的電腦去做連接？所以 MCP 的目的跟規範就是說，它不是在管那個 LLM 要怎麼去推理或是怎麼用那個 Context，它其實是專門拿在處理說 AI 應用跟外部系統的交換的那個通訊協定。

[01:55:45] 那這個圖就是一個蠻有名的圖。那你就可以把它想成是 MCP 就是中間這個 Hub。然後裡面主要會有三個角色：

MCP Host：就是指說這個 AI 應用本身。比如說像 Claude Desktop 或是像 Cursor、VS Code。你把這邊想成是一個 Host。比如說你現在有一個 Claude 或者你有一個 GPT 這樣的一個 LLM。

MCP Client：那底下就是會有一個 MCP Client。那這個 Client 的話會跟下面這個 Server 是它是一對一的。你有想說是是一個孔嘛，一個孔對應到一個東西。所以你 Host 內部的這個 Client 物件它會對應到一個 Server，那就是負責去跟這個 Server 做連線。

MCP Server：那 Server 可以想像成是什麼呢？就是比如說你本地的數據嘛。就像我講的，你可能就是想要做資料分析嘛，那你以前做資料分析你就要把你的東西丟進去那個 GPT 裡面嘛。那你現在可能就可以用透過這個 MCP 這個架構，那你去串接，那你就可以讓 LLM 跟這些其他的東西它有一個橋樑。那或者是像說可能你這種 Gmail，你想要自動去接這些 Gmail 的訊息啊，然後讓大家去接這個 Calendar 這樣子。

[01:57:43] 所以它就是這個一對一就是很顯然的。那主要來說，因為我們可能還有很多其他的技巧嘛，比如說像大家非常應該常用如果說自己有檔案的話就會用 RAG 嘛。那或者是說像現在其實像這些 GPT 啊或者是 Gemini 他們其實都有這種 Web Search 的功能。

[01:58:21] 那這三個東西主要在哪邊呢？

MCP：它其實是接算是接最多東西的，你包括說這些 Tools 或者是說資料庫，它都可以直接去做串接。

RAG：是你專門就是你有已經有你的知識庫了（你的 Data），那你讓我們就是透過 RAG 這個技術，然後讓 LLM 去檢索，然後再去從裡面撈資料出來，然後再去生成答案嘛。

Web Search：就是直接去做搜尋網路。那他可能不一定就是可以去執行裡面的一些這些網頁的一些功能。那如果你說用 MCP 你可以寫一些 Function 進去的話，它可能就可以使用那些網頁的部分的功能這樣子。

[01:59:16] 好，那今天的話我們就是用 Claude 跟用 Cursor 可以當做一個簡單的範例這樣子。

[01:59:39] 好，那我們要先講清楚，就是說我們通常在做這個 MCP 的時候就是會用到就是那個 Node.js 跟 Python 檔。所以然後通常都是在本地端做實作的，就是在你的電腦裡面運行。所以通常我們會要去...你要載 Node.js 跟載像是 VS Code 之類的。最好是載 VS Code，因為這最多人使用的。

【生成式 AI】13. 強化學習和 LLM 對齊 - 逐字稿 (Part 4)

時間範圍：02:00:00 - 02:22:19 (End)

助教實作教學：VS Code 與 Cline 插件設定

[02:00:04] 還有這個 VS Code 就請你們有興趣的人可以之後在這邊載一下。就載這兩個（Node.js 與 VS Code）。那基本上就先 Download 就好。

[02:00:25] 那你打開了之後呢，我們就可以去 VS Code 打開之後，它就有這邊這個 Extensions（擴充功能）。那你就直接對這個找這個 Cline (助教口誤為 C)，然後你去做 Install 的動作。其實你載完之後就會看到這邊這樣子。

[02:01:01] 它就是大概長這樣子，然後你可以有一個對話的功能。那第一個就是說我們要先去設定裡面。因為你現在是這個...這個 Cline 它本身並不是 LLM，你不是說載下來它就有那個 LLM 的功能，它也是去接外面的 API Key 進來的。

[02:01:22] 你可以選很多個，像說它自己本身就有一些模型，但是它這個模型免費的額度非常少，基本上你可能用不了多少就沒了。那也可以像就是我們之前常用的。那在這個地方它能選的模型比較少。我看最多人在實作的話都是用這個 OpenRouter。

[02:01:54] 那你就可以在裡面有比較多，而且還有這個 Free 的。你就去 OpenRouter 這邊下載、去申請 API Key，就跟 GPT 是差不多的那個概念。那你們就可以去自己去申請這個 API，然後就把它放進來。

配置 MCP Server (以 GitHub 為例)

[02:02:30] 那接下來的話 LLM 的部分處理完了嘛，那我們還缺 MCP Server 對不對？那 Server 的話你可以去這邊找。那你可以直接按這個 Install，但是 Windows 的協調性沒有這麼好，它可能不一定會載成功。

[02:02:56] 因為我已經載完了，那我跟你們講一下。比如說你看到這個 GitHub，你按這個下載，它是其實是怎麼實作的？它是就真的去連到這個 GitHub。那這個就是算是人家已經寫好的，那你只要把它下載進來，你就可以做了。

[02:03:22] 你看它就是這樣子，它會有網頁...它是有點像是它讓 LLM 去讀這裡面的內容，然後跟你講應該要做什麼事情、要怎麼下。然後他可能會用到 Docker，所以我建議的話是我有先試了一個可行的版本，你們可以先 Copy 我的版本，不要直接用這個下載。

[02:03:48] 好，那我們再回來。所以我們點到這個 MCP Server 這邊，然後這邊有一個 Configure 嘛。Configure 就是你實際上這有什麼模型，那要寫在哪裡？就是我們到現在還沒有開始寫嘛。那我們現在這個 MCP Server 我們就點這個 Configure MCP Server。

[02:04:09] 那這個我預先寫好了。那一開始的話它就是空的，這些都沒有。我先再講一次，我現在是用 Windows 的版本，因為 Windows 跟 Mac 是稍微指令是有一些差異的。等下可以去其他網站他可以清楚的看到。

[02:04:30] 像 Windows 的話，有時候像這個 GitHub 它就要這樣寫：這邊寫一個 command，就是在這個 command 這邊要寫 node。然後如果是 Mac 的話，這邊就會寫 npx，然後這邊就會沒有這個。所以它有一些細微差異。那 Windows 就照著我這個版本載。

[02:04:51] 然後你這個一定要照著我們這樣這個 JSON File 這個格式寫，像這些括號要怎麼括，然後這邊要寫 mcpServers，這個都照著這個寫好。然後你這邊放進來之後，你就按個 Ctrl+S。你看我按——因為我已經載過——剛剛按的那瞬間是不是這邊是紅色？就把它這個 File 讀進去，然後它在 Run。那如果這邊是綠色的話就沒問題。

[02:05:22] 那我們再回來。我們就可以開始一個對話。其實我們現在有一個 GitHub 對不對？那我比如説我可以...我有給他我的這個 GitHub 的這個權限哦。這這是 GitHub 的 Personal Access Token (API Key)。那通常因為我看全部是寫進來，沒有特別藏起來，你也可以藏起來。然後我打回去我就會直接註銷，因為我有點來不及在做那個設定，但你們自己的那個 Key 一定要保護好，不然可能會被別人盜用。

[02:06:22] 這樣做完之後你就按 Approve。其實你也可以設定 Auto Approve (Always allow)，但是我我個人還是會覺得在這這種它真的是要去碰你電腦資料的時候，你至少還是自己去自己去看一下會比較好。那你就是等他跑一下。

[02:07:30] 好，OK。那這邊我確實裡面只有一個檔案，它叫 README.md。然後如果你直接問這個問題——就我剛剛講的，我 GitHub 名字是這個——然後我有試過你直接去 GPT 問的話就沒有，他就不會跟你回答。所以代表說這邊他是真的有把這個 API Key、就應該說把這個 MCP Server 吃進去了。

更多 MCP Server 範例 (Brave Search / Weather)

[02:07:58] 那 GitHub 算是一個比較常見的例子，比較官方的。那我們再來看一下還有其他的。那像是這個網站 (glama.ai/mcp/servers 或類似列表) 就是有很多人寫好的這個。那我們等下改用 Cursor 這個軟體去做實作。

[02:08:28] 比如說像這邊這些東西，或者是你可以直接查你想要什麼。比如說蠻多人會做一個想要做一個天氣預測的 Server。那這個 Server 是怎麼樣呢？就覺得說這個是別人做好的嘛，它就 Great 了。好，然後你就可以看說他們在寫了什麼東西。

[02:09:19] 比如說你對這個有感興趣，你想把這個 MCP Server 放到你的這個 LLM 去跟他做的話，你可以看。比如說像它這邊有 Mac/Linux 版本的，你看...然後就有 Windows 版本。所以它是真的稍微有點差。我剛剛講這邊主要是這邊的差異。

[02:09:47] 那我就找了兩個，一個是 Fetch News 的，然後一個是 Weather 的。那 News 的話就是它可以去找最新的貼文，然後跟比如說這禮拜最新的新聞。那這邊都是用美國的那個 Data 當範例啊。那這個 Weather 也是，他去接了美國的這個當地的那個氣象局的資料。

[02:10:26] 那你就可以就是用這邊這個 JSON 檔，然後 Windows 這邊我們等下就直接把這個 Copy 進去就好，就可以做到了。

在 Cursor 中配置 MCP

[02:10:35] 那要怎麼用 Cursor 做呢？你就先因為網頁版的話可能還沒有這麼比較沒有辦法，你就 Download Cursor 這個軟體。好，反正也是直接下載，然後一路就按按，它自動就裝好了。那裝好之後就會變成像這樣子，你可以去打開它。

[02:11:10] 打開它之後，你就可以去看這邊還有這個 Features (或 Settings > MCP)。那我已經先寫進去了。那這個東西它寫在哪裡嗎？它像剛這個 VS Code 你也要設定這個資料夾嘛。但是其實現在都蠻聰明的啦，像我剛剛說的，如果你是用這個 Cline 的話，你直接這邊基本上按這個 Configure MCP Server，或者說你真的去按一次這個 Install 的，它就會自己去創這個資料夾。

[02:11:47] 那 Cursor 也是其實是真的差不多。它就是放在自己這個 User 裡面，然後 AppData/Roaming/Cursor...反正就是 Cursor 的資料夾裡面再放一個這個 config.json 檔。其實就這樣而已。

[02:12:12] 那完全沒有自己...不用自己寫程式嘛。我剛剛說了，除了這個 MCP Server 之外，這這裡面這個內容是什麼？這裡面這個內容就是我剛剛講的嘛，這邊這個 JSON 檔的 Windows 版，你就是直接這樣...誒它這個有一個字...然後你到時候你就進來，你就直接這樣，然後貼上就好。OK，我已經貼過了。

[02:12:46] 然後如果你想要做兩個怎麼辦？因為剛剛範例是一個嘛。那如果是兩個的話，你不用分開檔案，其實你就寫在同一個檔案裡面。你就第一個是從這裡開始到這裡嘛，那你就用一個逗號分隔，那你就可以寫下一個這個你的 MCP Server。

[02:13:05] 那你一樣寫完之後，你就按 Ctrl+S。你按 Ctrl+S 之後，它就會自己讀進去，然後讀完之後它就可以進來了。那有時候他這邊會讀不到。因為你看下這邊是有兩個 Tools 嘛，這兩個 Tools 是怎麼來的？不是我們去 Set 兩個東西嘛，是這個 MCP Server 他這個人在 Create 的時候他就寫兩個 Tools 進來，所以你當然就如果真的是 Load 的成功的時候，你就有兩個 Tools。

[02:13:39] 有時候它會失效，你就可以旁邊這個把它關掉 (Refresh)，再打開讓它重讀，它基本上就都可以成功。

[02:13:44] 那載完之後呢，你現在有這個東西嘛。所以比如說我可以先問...然後你要注意的是這個 Cursor 的話，現在如果你這大部分都是要付錢的。那你不想付錢的話，你就官方就說你就是...反正我也沒有把那個信用卡寫進去啊，他扣不要我錢。那你就按這個 Composer (或 Chat)。

[02:14:14] 你可以請問說，比如説：
「現在有什麼 MCP Server 可以使用？」

[02:14:44] 好啊，通常就是要跑...沒有讀進來是嗎？這有說壞...因為基本上應該是這邊亮綠燈就要有。

[02:15:10] （助教遇到技術小狀況，Cursor 未成功讀取 MCP）
小尷尬。好，那我先繼續使用剛剛的這個（VS Code Cline）好了。然後我按 Ctrl+S。

[02:15:53] 等他一下。就是基本上多一個架構就是像這樣。我剛是還還成功...好。那 Cursor 的話就基本上跟我是一樣，只是這個路徑教一下。

[02:16:11] 那我們還是那就在現在回來 Cline 這邊。那你看現在這邊它就有三個 Server。那你看比如說他有這個 Weather Server，我們就可以去查美國的天氣。比如説：
「現在洛杉磯天氣？」

[02:16:46] 好，然後他就...它就會自動去問、去讓 LLM 跟這個 Server 做連線。那可能要等他再跑一下。通常我覺得通常都會跑得比直接去問 GPT 還是久一點。不過就是如果你想要接這個 MCP 的話，還是可以現在...就是目前的方法就...

[02:17:20] 好，那你看像你問現在洛杉磯的話，那它就會去跟你...它其實就是去，它就會自動去解讀。比如說裡面這個 Weather 在幹嘛？其實這個 Weather Function 它就是去輸入兩個，就是你的經度跟緯度，然後它就會去調用那個我剛才說美國那個氣象局裡面的 API。

[02:17:42] 然後你就按 Approve。所以它是把這個「洛杉磯天氣」，然後我知道這個 Server 可以使用。那這個 Server 要什麼？它要兩個這個經度跟緯度。然後我就再把它丟進去，然後就去看。

[02:17:58] 你看它就出來了。所以它這個不是 LLM 自己想出來的，是他真的去調用那些工具。

總結 MCP 運作流程

[02:18:26] 好，那我們就從上面看到下面。我再講一次，剛剛我們那個問天氣大概到底是怎麼講的。

[02:18:32] 那其實就是說我現在把這個 MCP Server 載進來了嘛。那我不事有用 Cline 這個程式嗎？所以在啟動的時候 Client 就會跟 Server 之間有一個連接。然後他就會說：「哦，去看 MCP Server 你有幾個 Server 啊？然後這些 Server 裡面有什麼 Function？」好，這個是一開始這個 Client 跟 MCP Server 它會有一個這樣大概是這樣子互動。

[02:19:00] 那你現在用戶就說 "Weather of New York tomorrow" 這樣子嘛。那你可能就說問這個問題完之後，我們是在 Cline 這個介面裡面輸入嘛。所以 Cline 他就會去跟 Model（比如説接一個 GPT），我就跟 GPT 說明天得天（氣）怎麼樣。順帶一提，我這個 MCP Server 裡面有什麼工具，那他們可以幹嘛啊什麼的。

[02:19:30] 那模型就知道說：「我的問題是紐約明天的天氣，那我要調用的是 Forecast 這個函數，那參數是什麼」這樣子。好，所以就會再去跟 MCP Server 說我要調用這個函數，那參數就輸進（剛剛講的經度跟緯度）。

[02:19:58] 那 MCP Server 收到這個經緯度之後，它去調用那些 Function 算出來之後，它就知道結果是什麼吧。那結果再傳回這個 Client，再把結果傳回 Model。那 Model 就知道說他調用這些...利用這個 Server 去網路上...不從不算直接搜尋，是利用 MCP Server 它寫的 Function。你可以你有一個那模型有一個 Input 之後，然後你 Server 就可以知道說它可以給你個 Output 這樣子。

[02:20:31] 好，那最後模型就知道說紐約的天氣是什麼啦，那他就可以回傳給 Client，然後最後 Client 會再回傳給你這個用戶。好，大概這個流程是大概是長這樣子啊。

交易競賽回顧与結語

[02:20:51] 好，那 MCP 的部分就到這邊。然後記得我記得上個月的時候，我不是說有一個交易競賽還蠻有趣的嗎？然後那個時候我不是說有兩個模型很高，就是本金是 1 萬嘛，他賺了 2 萬。

[02:21:02] 然後但是我那時候有跟你們說，像這種 AI 的話，如果你不知道你在做什麼，其實很恐怖。那後來在這個區段快結束的時候，就是 11 月 4 號（他從 10 月 18 號），結果其實到最後的話變成...因為那一段時間有一點那個比特幣的大盤的走勢沒有這麼好，那你看他們下回撤的幅度就大。

[02:21:28] 那這就是我講的你們可能會遇到的問題啊。就是如果想做金融 AI 的話，最容易會遇到的問題。然後如果你們期末有人想要做金融相關的的那個 Project 的話，是可以來跟我討論。我可能可以給你一些可能會遇到的盲點，你們應該要注意什麼這樣。

[02:21:54] 好，那今天我的課就到上到這邊。那可以看一下大家有沒有問題。

[02:22:11] 好，那如果沒有問題的話，那就下課。好，謝謝大家。
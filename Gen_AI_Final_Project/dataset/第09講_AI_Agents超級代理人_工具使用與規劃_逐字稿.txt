【生成式 AI】09. 為什麼大家說 2025 年是 AI Agents 元年？（逐字稿 Part 1）

時間範圍：00:00:00 - 00:40:50

[00:00:00] 這是我們大型語言模型，也就是生成式 AI 的第一段文字生成的最後一節。為什麼叫暫定？因為會發現有一些細節會放在後面，那些細節比較偏技術方面，所以我們之後再慢慢了解。我們今天其實會說到其中的一個細節。

[00:00:30] 今天是一個總結的部分，也要為大家介紹最近、特別是今年相當紅的一個主題，叫做 AI Agent。可能很多同學都有聽過 AI Agent 這個詞，我們今天就是要為大家介紹到底什麼是 AI Agent。

[00:00:55] 我們來看 AI Agent 到底是什麼。你去外面查的時候，都會發現看了一堆好像很有道理，但是完全不知道他在說什麼的東西。大家覺得 AI Agent 好像是很神奇的東西，可能會告訴你說：這個 AI 能夠有能力自動感測外面的世界，同時自動做出一些反應判斷等等。或者你也可以聽到更實用的說法，例如可以自動幫你訂票、自動做什麼事情。反正你聽到一堆很神奇的東西，但是你還是不知道到底什麼東西才叫做 AI Agent。如果我們真的要自己做 AI Agent，要怎麼樣做？這就是我們今天的主題。

[00:01:55] 如果大家有聽說，今年 2025 年是 Agent 的元年，所以我們這一次介紹 Agent 真的是最好的時機。明年再介紹的時候就已經不是元年了。總而言之，2025 年就是 AI Agent 的元年。如果你上網搜尋，會發現真的非常多的地方在討論 AI Agent，我很推薦大家進去看，很多地方都會解釋到底什麼是 Agent。要注意，這跟上次 RAG 很接近，很多地方都有 Agent 相關的報導，不一定是在技術型的地方，你會發現查任何語言都會有 AI Agent 的介紹。

[00:03:00] 在 AI Agent 裡面，如果想要花個 25 分鐘左右去了解什麼是 AI Agent，非常推薦吳恩達老師（Andrew Ng）的一場很短的演講，大概 25 分鐘左右。我們今天會稍微說一下他演講的內容重點，今天的課程結束之後，你再回去聽那個演講，你會更懂到底什麼是 Agent。這大概是去年的演講，吳恩達老師是推動 AI Agent 非常重要的推手之一，他是其中的關鍵人物。去年大家開始準備要出現 AI Agent 的概念還有實際上的實作，所以今年大概是實際上實作非常重要的一年。

[00:03:55] 我們來解釋一下什麼是 AI Agent。第一點，我們先來解釋在使用大型模型的時候，「沒有 AI Agent 的世界」是長什麼樣子。我們心目中想像大型語言模型的世界，就是我下一個很好的 Prompt（提示詞），Prompt 要很清楚地告訴它正確的資訊，我們要很清楚地指引它做什麼事情。這是大型模型在下 Prompt 最重要的事情。

[00:04:26] 我們以前下 Prompt 的時候，其實都是下完了就請大型語言模型回答我們。以前就是這樣做的。我們也知道原理了，它其實就是在我們下完 Prompt 之後，看下一個字應該要接什麼字才叫做合理，它就會開始接下一個字。如果我們是一個問題，它就會開始回答我們的問題。這就是本來用大型模型的方式。

[00:05:00] 但是，吳恩達老師說了一件很重要的事情。想像一下，如果我們寫文章或是要說一段話的時候，每一次我都只想下一個字要說什麼，都不去稍微想一想我今天到底要講什麼。我每次都只是看前面的東西——我已經說出來的話或是已經寫出來的字，我就考慮下一個字要寫什麼，然後就這樣子一路寫下去。這就像是你今天用電腦打一篇文章，或是考試需要寫作文，規定你打出來的字就不能再改了，這相當殘忍。完全不能塗改，你也不能往前修改，甚至按 Delete 鍵都不行，你什麼都不能做，你只能再生下一個字、再生下一個字。萬一你有一個字下得不好就死了，因為不能改，抱歉，然後你就要一直生下去。你就會發現這樣子生出一個好的文章好像有點困難。

[00:06:00] 我們人也是這樣，語言模型也是這樣。因為它基本上根本沒有考慮比較宏觀的角度去看它要寫的東西，它基本上就只是準備去回下一個字是什麼，這樣一路回下去。所以這是第一個困難點，跟我們正常的習慣不太一樣。我們正常習慣喜歡一邊寫一邊想、一邊修改，可能還要先打一個草稿。我們在 Colab 裡面寫程式也是這個樣子，我們很多同學其實以前沒有太多寫程式的經驗，今天跟你講說你要寫程式，請你就從第一個字開始寫，然後不能改、不能塗改，要一次寫完全部，有錯也不能修改，這實在太困難。

[00:07:05] 所以這樣的情境，我們發現這樣去用大型語言模型，跟我們人類的習慣方式不一樣，對大型語言模型來說也可能是一個很大的挑戰。所以我們再回來看，要讓大型語言模型回答得比較好的時候，我們到底要做什麼事情？

[00:07:33] 第一類型，我們可能要查很多的資訊給它。因為要有正確的資訊嘛。這些正確的資訊包括什麼呢？如果今天有計算的時候，千萬不要相信它的計算，這我們已經很懂了。看到有計算的時候，麻煩自己幫它按計算機，不然它算的一定會不準。請你直接幫它算，算完再告訴它說這個算出來答案是這樣，所以你也要放在資訊裡面。

[00:08:08] 第二個，它有可能要去特定的檔案裡面查特定的資訊。所以請大家記得要做這樣的事情，你可能要到某一個特別的網頁，比方說你要知道天氣，你可能會說要到中央氣象局裡面去查未來多少天的天氣。

[00:08:31] 也有可能有一些資訊是要上網去搜尋的。特別是在語言模型剛出來的時候，語言模型本身其實是沒有搜尋能力的。它有搜尋能力是我們今天講的 AI Agent，但以前是沒有搜尋的。標準訓練的時候就是前面一堆字去預測下一個字，中間根本沒有搜尋的時間，所以你需要自己上網搜尋去告訴它。大家就會覺得這樣真的有點蠢，電腦就是一個很會算數學的、很會計算，為什麼不叫電腦算？查資訊不叫電腦自己去查嗎？上網搜尋也是叫電腦去上網搜尋嘛。

[00:09:15] 所以說，AI Agent 最容易的理解方式，事實上也是最正確的理解方式，就是：我們觀察我們用大型語言模型，如果一個 Prompt 直接下去沒辦法完全達到我們想要的結果，我們會想要做的事情——例如幫它收集資訊、幫它做計算等等——我們把這些我們想要幫它做的事情，讓電腦自己去完成，把它自動化。這就叫做 AI Agent。它是我們的 AI 代理人，本來我們就是這樣做的，只是今天我們教電腦說你可以這樣做，然後讓它自己去做。

[00:10:10] 再說一次，AI Agent 其實就是模擬我們用大型語言模型的方式，嘗試著讓電腦自己去做。再來，也有很可能如果你有經驗讓 AI 去寫作業——這個我沒有反對，但是後面的事情是反對，你不要不檢查就給它交上去了，你一定要看。這個不會十全十美完全符合你的需求。光是臉書的 PO 文都不會。今天你想要寫個 PO 文，但你想寫得再更有趣一點點，所以你叫 ChatGPT 幫你改，這個可以，它很擅長改。但是改完了相信你還是不太滿意，所以你這個時候有可能又要再跟它講說這個地方要這樣子做、那個地方要這樣子改。

[00:11:05]那我們就想說，這樣的事情可不可能讓 AI 自己去做？意思就是說，今天 AI 寫完了一篇文章之後，我們再叫一個 AI（或是它自己）去改一下。改完了以後，它再根據修改意見再去修改，然後再出下一篇文章，再看看要不要再改一下。這樣子來回。這樣子的動作可不可能就是讓 AI 自動的去做？這就是我們想要想的事情。

[00:11:40] 所以說，不管是什麼樣子的情境，就是我們本來要做的事情，讓 AI 或電腦自動的幫我們做完，就叫做 AI Agent。它代理我們去做這樣的事情。所以 AI Agent 的真正意思就是這樣子。我們要知道真正的意思，我們才能夠真的去想說我到底要怎麼樣去設計我自己的 AI Agent。所以今天我們的重點就是：我如果要自己設計 Agent，我到底要怎麼做？

[00:12:15] 我們回想其實 RAG（Retrieval-Augmented Generation）它本身也是一個 AI Agent。上次 RAG 做的事情，就是我們需要查資料，我們先丟給它所有的資訊，可能放在一個資料夾裡面。我們在做的時候，就請 AI 去搜尋，用語意去搜尋說這個裡面哪一段話、哪一段我們切出來的資訊是跟它要回答問題是有關係的，然後它就抽出來，把參考資料自動找出來。

[00:12:46] 這個自動的動作其實我們自己也可以做啊，我們人也可以做，就是比較笨而已。我看著那一堆資訊，然後我說「喔，答案應該在這邊」，我就把那個圈起來送給它。但是這個聽起來很笨，我們還要自己去做。所以我們在做 RAG 的時候，就是自動的完成我們剛剛說的事情，自動的從資料庫裡面完成我們自己去做的事情。所以再說一次，Agent 其實就是想辦法去模擬我們想要做的事情，嘗試讓電腦自動幫我們完成這件事情。

[00:13:40] 比方說，即使是在找資料這個地方，我們現在也會發現很多步驟。我可能要用 RAG 來找內部相關的資訊，然後可能需要去幫忙收集正確的資訊、要上網去收集，然後可能要按計算機去檢查一下數字有沒有算對，然後可能寫出來的東西要檢查一下文句通不通順、有沒有錯別字、架構是不是符合我們的想法等等。在這樣的情境之下，AI Agent 其實就是要自動幫我們做這些事。

[00:14:20] 現在 AI Agent 的世界，我們就好像是一個大老闆一樣。我們一樣是下 Prompt 告訴它要做什麼，我們可能可以不用像以前說得這麼清楚了。因為要找資料的那些事情，可能電腦就自己去做。所以說，有可能有一個 AI 先開始去計畫一下、打一下草稿，說我們現在使用者問的這個問題，我到底要怎麼樣回應比較好。

[00:14:58] 我們可能有 RAG 一號、RAG 二號。為什麼有 RAG 一號、二號？因為有一個可能是針對法規，有一個可能是針對我們會辦的活動或是我們公司的產品，所以它可能是不同的資料庫。所以有時候事實上是經常的，我們會做成不同的 RAG，然後讓我們的 AI 知道說這個時候應該去看第一個 RAG，這個時候應該看ˋ第二個 RAG。

[00:15:25] 剛剛說到有些時候我們需要去呼叫我們想要的工具，比方說按計算機啦、上網搜尋啊等等，那個就是工具的呼叫。然後最後有可能要看從我們過去的對話紀錄去用一些記憶的技術。你當然可能會想說，為什麼要有記憶的技術呢？我們不是把全部的文字都給它放進去就結了嗎？答案其實意思也是這樣子。特別是未來的語言模型，它可以讀的範圍是越來越大。

[00:16:05] 我順便說一下，在未來的世界裡面，有可能我也不用做 RAG。因為我們要做 RAG 的一個很重要原因，就是因為語言模型它都有一個限制。像 GPT 剛出來的時候，它最多只能放 2000 多個字。萬一你今天參考資料就有 2 萬個字，那怎麼辦？根本塞不進去，所以我們就要做 RAG。但未來的世界裡面，我們也可能不用再做 RAG，因為現在要看完兩萬字的語言模型已經出現了，甚至 Google 他們一直覺得他們未來可以做到上下文是無限長度的。所以在那樣的情境，我們可能真的不用做 RAG。好，這是另外一個補充的事情。

[00:16:48] Memory（記憶）就是我們之前有說過，在大語言模型跟它交談的時候，我們當然可以把所有的交談過程都把它放進去。可是如果這個語言模型的上下文長度沒有那麼長的話，我們就需要有一些技巧。其中的一個技巧就是我們可能把過去的交談紀錄做成 RAG，所以它就可以在需要的時候搜尋到以前的資訊。

[00:17:23] 比方說我們今天是咖啡店的客服系統，我們把客戶跟我們所有的交談內容都記下來。所以下次人家再來、他常常來的時候，因為每一次可能會多知道一些資訊，比方說他在哪裡工作、他平常都點什麼咖啡等等。所以在進來的時候，你就可以很快速地找到說：「喔，這客戶他平常都是點什麼咖啡」，或是「今天下雨天的時候他都點什麼，大晴天的時候他都點什麼」之類。所以我們就可以把這個記憶有可能做 RAG 系統或是用其他的記錄把它做起來。

[00:18:05] 總而言之，就好像說我們有好幾個員工在下面，它其實是要模擬我們做的方法，但是它就好像很多個員工。現在就變成是說以前我們自己要做的事情，我們有好多個員工去幫我們一起完成。然後目標就是要找到足夠多資訊，目標就是要能夠產生一個非常清楚的指引，告訴最後負責總結的語言模型。然後我們就把這個新產生出來的 Prompt 丟給我們可愛的語言模型，再出來做輸出。所以在我們下完 Prompt 中間，其實經過了很多的事情。這個就是 AI Agent 的想法，跟我們原來直接下一個 Prompt 就出來答案的不一樣。

[00:19:00] 好了，我們現在正式進入我們今天的核心主題。剛剛還是比較抽象的想法，我們今天跟要大家討論的是：常見的 AI Agent 的設計模式到底有哪些？我要再度強調一次，AI Agent 基本上就是我們平常使用大型語言模型、可以讓它回答得比較好的方式，我們想辦法教電腦讓它會這個方式，這個就叫做 AI Agent。所以它不一定有固定的設計方式。但是如果今天告訴你們說：「各位同學，AI Agent 沒有固定的設計方式，我們今天的作業就是大家要設計一個 AI Agent」，那大家就會覺得很抽象，不知道怎麼做。

[00:20:00] 所以我們現在要為大家介紹常見的 AI Agent 的設計到底有哪些形式。再說一次，不一定要這樣設計。這幾個形式是吳恩達老師說的，他在我們剛剛說到的 YouTube 影片（投影片網址在下面）介紹 AI Agent 時說到了這四種模式。

[00:20:25] 第一個模式就是反思（Reflection）。
第二個模式叫做 Tool Use，就是工具的使用。
第三種模式是 Planning，就計畫。
第四種模式是 Multi-agent Collaboration，就是多個 AI Agent 一起合作。

其實第四個有說跟沒說一樣，就是大家一起做啊。其實你很容易想像會有第四個。等一下我們一個一個介紹了以後，你就會知道第四個其實很自然。

[00:20:51] 然後我們今天一樣要用我們之前就用過的 AI Suite。因為它的好處就是它是很統一的使用方式，它可以很輕鬆的切換模型，而且可以使用不同供應商的模型。有一些點你覺得不用太好的模型，用免費的就可以了；有一些點你覺得不行，我要用那些付費的，例如說 OpenAI 的，那你就可以混用。因為特別是在 AI Agent 的世代裡面，有可能有很多個不同的 Agent，所以你可能要設計不同的 AI Agent 出來。每一個 Agent 可能都是一個語言模型，所以你可能要用不同的語言模型。最後還有一個總結的語言模型。它安裝很容易，我們已經裝過了。

[00:21:48] 第一種設計模式：反思 (Reflection)

它其實很簡單。第一步，第一個大型語言模型先寫一個稿、一個初稿。不管我們想要它做什麼，可能請它寫一個臉書的 PO 文、寫一個報告，反正它會寫一個初稿。就跟我們平常下 Prompt 一樣，想辦法讓它產生一個還不錯的初稿。但是通常這些初稿你自己去看的時候，都會發現有很多需要改的地方。

[00:22:30] 所以我們就再請第二號語言模型去看那個初稿，然後說：「你這個邊要改、那邊要改」。它怎麼知道要改？因為我們教它的。所以我們在 Prompt 就會告訴它說你要看哪一些重點，然後要確實的注意，或是如果是跟事實有關的，你要想辦法去查核事實等等。第二個語言模型就是想辦法提供建議，然後再交給第一個語言模型（寫稿的那個）。寫稿語言模型就會根據修改建議再度的修改，修改完了以後再送到第二個語言模型，然後第二個語言模型再去看有什麼問題，然後再告訴第一個語言模型。如此依此類推，一直這樣下去。

[00:23:13] 看你要做多少次，這當然要試驗之後比較會知道。也許做個三次就不錯了，或是做個一次、第一次搞不好就已經最好都有可能。這都是要試驗以後才會知道的。不管怎麼樣，這就真的模擬我們人類的樣子。如同我們剛剛說的，我們下了一個 Prompt 之後，今天第一號「作者機器人」開始寫稿了。寫完了以後，「評論員」去評論他的稿。評論完了以後，他就再根據這個評論員說的去繼續寫稿。就這樣來回反覆。

[00:24:00] 如果都用一樣的大型模型的話也是這個樣子，只是我們下的 System Prompt 是不一樣的。所以它雖然都是用一樣的大型語言模型，但是感覺就是兩個機器人。事實上就是兩個機器人，因為它完全不知道另外一個機器人下的 System Prompt 是什麼。它只看得到它出來的稿，然後作者機器人也只看得到評論員幫它改的文章建議。所以這可以是兩個一樣的大型模型，當然也可以是不一樣的大型模型，看我們高興自己去設計就好。這就是 Reflection，其實很簡單。

[00:24:47] 第二種設計模式：Tool Use (工具使用)

這個概念也很簡單。簡單的說，就是大型模型不太擅長的事情，拜託不要再逼它做了。今天如果要算數學的時候，不要逼它去算加減乘除，它就是不會。但是我們想到說這件事情電腦很會啊，那我們為什麼不叫電腦寫個程式去解就好了？算得準確的。

[00:25:34] 第二個它也不太會做的，就是特定的資料。比方說你要買東西，你當然就要上某一個你常用的購物平台搜尋，在購物平台裡面搜尋出來的才算嘛。如果你叫語言模型自己說，它會在邊胡說八道。比方說你叫它幫我找某一台相機最便宜的價格，它自己會吹、它什麼都會吹，它就是接龍接著下去說出來，那當然不準啊。所以你真的要去某一個地方去把它搜尋出來。有些東西是需要上網去搜尋的，你就要告訴它去上網搜尋。

[00:26:26] 以前 ChatGPT 剛出來的時候，它真的沒有搜尋能力。之後慢慢的它就開始有一個搜尋的按鈕。演進到現在，因為競爭實在太強了，它發現每一家都會搜尋了，它再不搜尋真的會死，所以它就偷偷的覺得有必要的時候它就幫你搜尋。這就是 Agent 做的事情，就是我不太會的時候，我就去呼叫工具來幫我做這些事情。

[00:27:20] 當然大家會覺得說，我們的大型語言模型在訓練的時候，根本沒有教它要有 Agent 的這種 Tool Use 的概念，那到底怎麼辦？非常簡單。今天我可能就是一開始就跟它講說你會有一些工具——其實這個用 Prompt 去下就可以。就是直接告訴它說，你發現你要做加法的時候，你就意識到要做加法，然後要去呼叫加減乘除的函式。

[00:28:12] 例如說股票的資訊或是新聞，你要告訴它說有這個功能。所以只要是語言模型意識到現在是要去搜尋股票的資訊，那你就去呼叫股票搜尋資訊的工具，或是去呼叫股票相關的新聞工具。舉一個例子來說，比方說「94 加上 87 是多少？」，它就會開始想辦法做成它可以去呼叫工具的樣子。這件事它怎麼知道？是我們先告訴它的。我們跟大型語言模型說：「如果需要加的時候，你就要呼叫這個加法工具。這個加法工具的函式名稱叫做 add，那要加的就是 X 的值還有 Y 的值」。然後它就會去呼叫這個 add 的函數，把 94 加 87 加起來。

[00:29:40] 第三種設計模式：Planning (規劃)

Planning 大概是現在最受重視的語言模型設計方式。而且 Planning 有另外一個大家在外面會聽到感覺有點誇張、好像很厲害的事情，就是相信大家都聽過說「語言模型有思考的能力」。大家都在比嘛，說我這個思考能力比較強，或是我的推理能力比較強。

[00:30:15] 推理能力事實上就是現在的 Planning。Planning 其實我們用最白話文的方法想，就是：看到一個問題的時候，請大型模型不要直接回答問題。 就好像我們人今天看到一個問題的時候，我不要馬上就開始準備回答，我先想一下我要怎麼回答，我就先產生那些「想一下」的動作。所以它事實上也沒很神奇，它就是先產生這個想法。這個想法其實說穿了就是「打草稿」。

[00:30:51] 所以我變成說我要下 Prompt 的時候，我的 System Prompt 就是想要好好的教我們的可愛語言模型要怎麼去打草稿。所以今天我會教我的語言模型：看到一個問題的時候，我先要切出來說：一要做什麼、二要做什麼、三要做什麼。所以外面吹得很厲害說這個就是「推理過程」，推個頭啦，是我們教它的。我們教它說：「你要回答這個問題的時候，第一個你要幹嘛，第二個要怎麼樣，第三個要怎麼樣」。你就是想辦法下一個 System Prompt，想辦法產生這個 1、2、3 這種推理的過程，或是它的計畫的過程，或是打草稿。

[00:31:39] 所以你的專注就是跟我們以前不太一樣。我們通常以前 Prompt 就是要專注說語言模型要一次就把我的問題回答得很好；現在沒有，現在要專注讓我產生一個很好的草稿出來。在 Planning 裡面，最有名的方大概叫做 CoT。這個我們一定要記得，因為這個很適合出去外面騙人。我們千萬不要把那個原來的詞說出來——Chain of Thought，叫做「思維鏈」。就是叫它 CoT，這樣人家才覺得我們高級嘛。

[00:32:17] CoT 也是在外面大家說語言模型有思考能力的原因。再說一次，它不是真的思考，它只是你教它說：「你要很有系統的拆解複雜的議題」。看到一個問題，想辦法教它怎麼拆解，然後再請它找出議題跟議題之間的關聯性，然後你要想辦法教它說它應該要嘗試涵蓋各個面向，然後看出現的順序等等。

[00:32:52] 一個最有名的例子，事實上是大家知道 CoT 跟 DeepSeek 非常有關係。因為 DeepSeek 出來了以後，其實 CoT 不是 DeepSeek 他們發明的，但是 DeepSeek 做出來了以後，大家很多人才從 DeepSeek 裡面知道 CoT。因為 DeepSeek 有一個好處，就是它的思考的過程它會寫出來，就像這灰色的部分就是它的思考。

[00:33:14] 它思考過程其實寫得很好。如果大家有機會用到，特別是你覺得說在外面這個開源型的版本裡面，像 Perplexity 他們也有提供 DeepSeek 的模型，你就可以試著去用用看。它的好處就是會把思考的過程寫得很清楚。其實 GPT 的思考也是這樣思考的，ChatGPT 通常是不告訴你它怎麼想的，你覺得它突然變得很慢、它就在想，但它其實只是產生這些思考的過程。它沒有很神奇，它就是產生思考的過程。

[00:33:57] DeepSeek 就會把思考的過程用 <think> 兩個 think 標籤把它框起來。所以在回應的時候，它就會說「那根據我們剛剛的思考的過程，再去回答使用者的問題」。

[00:34:13] 吳恩達老師一開始就發現了一件事情，就是你用 Agent 去設計的回應（比方說用 CoT 這樣去設計回應），其實是同樣的語言模型，甚至是比較小的語言模型，你都會讓它的回答品質大幅度的提升，會非常接近、甚至有時候可以比大的語言模型還要厲害。所以這就是大家為什麼覺得 Agent 真的很好，因為它其實基本上就是取代我們原來要做的事情。

[00:34:50] 我們人很厲害的時候，其實你也會發現，我們的語言模型雖然有可能比隔壁的那一位——他的語言模型可能比較厲害，因為他可能交了非常多的保護費給 ChatGPT 或是 Claude 或是 Gemini 等等。順便一提，現在 Perplexity 我們可以直接使用，所以我們也不用交保護費就可以使用。總而言之，它可能交了很多的錢去用那個很大型的語言模型。但是，你雖然用的是比較簡單或者比較小的語言模型，但是因為你很會用、你很會問它問題，在這樣的情境之下，你會發現你出來的結果比較好。

[00:35:40] 簡單說，就是用設計了 Agent 之後，你可以讓比較小的語言模型或者沒有那麼強的語言模型，看起來變得很強的樣子。好，那我們為什麼要特別去學這件事情？有一個很大的重點，就是 DeepSeek 雖然它真的是那個 think 的過程做得非常好、CoT 過程做得非常好，但畢竟不管或是其他的像 ChatGPT 啦，你會發現它有思維、有去想一下的時候，它會做得比較好，沒有錯。

[00:36:20] 但是，你要知道，它們這些語言模型要做的事情都是包山包海。人家問什麼問題，它都要產生還不錯的思考結果。不管問什麼問題哦！那誰知道你要問什麼問題？反過來說，我們今天去設計的時候，通常只針對一個任務。比方說我今天就是要常常生出來一些特定技術報告。我們就想辦法讓我們可愛的 AI 打草稿的時候，就想辦法是要讓技術報告寫好的那個草稿。

[00:37:28] 因為我們做的是單一件的任務，所以我們非常有機會比那些通用模型做得好。因為他們要包山包海，我們不用，我們只要做單一的任務。所以我們自己設計的 Agent 真的非常有機會可以超越那些外面現成的語言模型。即使它的底層的模型是用一樣的模型，我們也很有可能可以超越。大家去做做看的時候就會發現真的是這個樣子。

[00:38:03] 最後一種的設計模式叫做 Multi-agent（多代理人）。我順便說一下，如果你常常去用的時候，現在非常推薦如果用 Mac 的話，大家可以去嘗試 ChatGPT 出了它的 app，或是 AI 的瀏覽器。像是 Perplexity 也有出。這些 AI 瀏覽器都號稱它可以幫你買東西，就是你可以再請它自己幫你下單。當然你不用真的很擔心啦，因為它在最後一刻它會問你。你不用擔心，因為有一些可怕的購物網站會記得你的信用卡，所以你下單的時候連信用卡都不用掏，它就幫你下單了。但是你在用 ChatGPT 或是 Perplexity 出的這些 AI 瀏覽器的時候，它雖然會幫你下單，但它通常不會做最後一個動作，最後一個動作還是要讓你確認，但是它前面的該填什麼它會幫你填好。

[00:39:14] 你去使用的時候就會發現，如果你真的對那個東西很熟——除非你對那個東西真的不熟，你想買東西然後你對它不太熟，有些同學可能是衝動消費的。如果你對它不太熟的話，你比較適合用這個。如果你真的很熟，你知道哪邊才有可能比較便宜的，然後你也知道有一些網站特別喜歡在深夜的時候降價等等，那個 AI 不可能知道，它沒有你厲害。所以如果你今天要自己去設計一個專門為你的某一個特定的產品然後要做的 AI Agent，你一定可以做得比它好，比它預設的做得好。因為你才是真正的專家。

[00:40:09] 所以再一次，我們發現一件很重要的事情，即使在買東西這種小事上，我們都會發現說如果你真的是專家的話，你就可以指導 AI，然後做的比 AI 預設的還要好。所以我們在很多地方就是要做到這件事情。你多做幾次，你就會有點信心啦。因為你真的做得比 AI 好的事情還蠻多的，它要打過你其實還有一點點困難度，除非是你親自手把手教它怎麼做。

[00:40:45] 好，所以這個是前面的三種 AI Agent 的設計方式。

【生成式 AI】09. 為什麼大家說 2025 年是 AI Agents 元年？（逐字稿 Part 2）

時間範圍：00:40:53 - 01:17:06

$$00:40:53$$

 第四種設計模式：Multi-agent Collaboration (多代理人合作)

第四種就很容理解，就是多代理人合作。其實多代理人是非常常見的。你要寫文章，你今天有一個 Reviewer（審稿人），就是我們第一種「反思（Reflection）」的那種 AI 設計模式。你會發現它第一次要寫文章的時候，它還是需要有資料。它要有那個資料，它需不需要去做 Tool Use（使用工具）、使用一些相關的工具？當然很可能要。它需不需要先做一下好的打草稿？很可能要。

$$00:41:27$$

 所以它雖然是 Reflection，但是它有可能把我們剛剛介紹過的 AI Agent 的模式都用上去了。通常在大部分的情境，真正的情境都是有一個多工的模式。這個多工的模式有一個很像人類 PM（專案經理）的角色，也是 Planning，但他的 Planning 是比較計畫說：「我要回答這個問題的時候，拿幾個、你們下面哪幾個要來做事情了？」因為有時候有一些 AI 在某些問題是不需要動作的。比方說我們是要有一個特別的資料庫，這個特別的資料庫可能在這個問題裡面根本沒有關係，所以負責特別資料庫的那個 RAG 系統它就不用動作了，那就其他人要做。

$$00:42:15$$

 所以他就會把工作拆解，讓底下的幾個 AI 一起去完成這個工作。這聽起來很複雜，但是事實上你真的在使用的時候，你會發現這很自然。你去真的做一個 AI Agent 的系統，你就會發現這個真的很自然。

$$00:42:38$$

 2025 年：AI Agent 元年

這就是 AI Agent 的基本介紹。我們再強調一次，2025 年是 AI Agent 的元年。在英文裡面有時候會說 2025 年是 "The Year of AI Agent"，講得好像說明年就不是 AI Agent 的年了。我們這次翻得很不錯，叫做「元年」。元年就是這個是只是第一年，還有後面的日子。

$$00:43:09$$

 我們現在應該大家已經很熟悉的 Andrej Karpathy（前 Tesla AI 總監、OpenAI 研究員）又出現了。他就說，現在其實我們真正的重點，是應該要花幾年才能真正的理解要怎麼樣好好的去做 AI Agent。這是 Karpathy 說的非常重要的事情。所以他覺得未來這幾年，我們可能要花個幾年的時間才能真正的知道 AI Agent 的設計。上面可能不只是這四種，可能有更多種，或是就是這四大類，但是這中間的應用技巧要怎麼樣去使用，然後我們怎麼樣打造出最符合我們需求的 AI Agent。

$$00:43:50$$

 可能我們現在開始、這個元年開始，的確已經有很多的 AI Agent 出來，但是你去使用的時候，你就會發現還有很多很多可以努力的空間。這給我們一個很大的提醒，就是 AI Agent 其實是未來很重要的趨勢。我們現在還沒有完全的熟悉，所以大家現在學是剛剛好的時間，就等於是你就可以當成 AI Agent 的先驅了。

$$00:44:34$$

 AI Suite 與 Gradio 介紹

我們先把投影片講完，可能下一節課的時候，我們就專心的來開始寫我們的程式。我們在這個生成式 AI，特別是文字生成的 AI，有很多值得去準備的工具。如果你未來要去學習，這看起來有點複雜，我講一些簡單一點的。

$$00:45:00$$

 就是我們已經使用到的工具：AI Suite。它其實真的是很適合。用 AI Suite 有很多的好處，第一個是因為它沒有特別多什麼神奇的功能。有一些框架，它其實可以幫我們很容易的做出我們想要做的，不管是 RAG 或是特定的 AI Agent，現在因為都有很多的框架可以做。但這些框架做的時候，有時候你會不知道它到底做了什麼。它其實也不是做了什麼神奇的事，比方說我們說的 AI Agent 基本上就是剛剛的那四類的設計方式，所以它也不是做一些很神奇的事情。但是因為它被框架包得好好的，你要做這件事情的時候，你只要告訴它這個還有這一個，然後它就會幫你做了。所以有時候你會比較不知道中間發生的事情。所以我們在開始練習的時候，用非常單純、簡單、簡潔的 AI Suite 其實是一個還蠻好的方式。

$$00:46:08$$

 第二個我們已經用過很多次，就 Gradio。Gradio 這本身不是做任何深度學習開發的工具，但是它是可以很容易讓我們想做出來的成品、我們想要 Demo 給人家看的時候，可以用圖形化介面來做出來。它其實最大的目標就是要 Demo 給人家看，它不是真正的目標不是要真正做產品的。真正做產品還是要自己好好的去寫啊，那詳細的怎麼樣去互動，你要自己去寫網頁啊等等。所以它的目標是我們做出來一個模型，我要怎麼樣 Demo 給人家看，很快的可以用那個圖形介面 Demo 給人家看，然後也可以讓我們的親朋好友去炫耀一下，說你看我做出來這麼厲害的東西。

$$00:47:02$$

 我們課程其實也有用到，在 RAG 的地方，但是沒有非常非常的強調給大家。大家就記得說，AI Suite 就是在寫程式的時候，有很多只要跟大型語言模型有關係的東西，它很多都包在裡面了。所以 AI Suite 可以去研究一下。

$$00:47:36$$

 工具呼叫 (Tool Use) 與 MCP

另外還有其他的像 AutoGen，它其實就專門是為了要打造 AI Agent 去設計的。這是給大家參考的。

$$00:47:57$$

 第二個，在工具的呼叫上面，我們就會發現說其實大型模型我們已經說過嘛，大型模型它基本上是沒有一個固定的、它本來是不會的，所以你要教它工具的呼叫。你要在 System Prompt 想辦法教它，然後想辦法讓它產生的東西是知道要呼叫誰。就是我們有辦法去真的使用這個工具，我就是要告訴它那個「工具使用說明書」。

$$00:48:26$$

 通常因為這些函式可能是我們自己寫的，所以我們自己寫的當然知道怎麼樣讓語言模型弄出來，然後弄出來的時候我怎麼知道說我現在要呼叫哪個函式、函式裡面要的引數是什麼東西。但是你久了以後你就會發現這真的有一點點麻煩。很重要的原因就是因為，你今天要做的事情也不是那麼特別的事情。比方說你要去按電子計算機，你當然可以自己寫函式，但是一定要自己寫函式嗎？說不定有別人寫好了更高級的、可以讓語言模型去用的電子計算機。或是要做網路的搜尋，這又不是一個很特別的事情。

$$00:49:23$$

 可不可以想辦法大家有一個共通的標準？只要用這個共通標準之下的，我要用這個工具就可以用這個工具，我要用那個工具就可以用那個工具。那個就是非常有名的 MCP (Model Context Protocol) 就是要做這件事。MCP 是 Anthropic 他們出的，就是出 Claude 的那家公司。它其實目標就是要打造一個統一的規格，讓我們可以在用大型模型的工具的時候，大家使用的方是固定的，因此我們就可以用很多很多別人已經開發好的工具。

$$00:50:06$$

 （休息後繼續）剛剛說到在因為每一個人如果都自己設計工具的話，那每個人呼叫的方式不一樣。一方面是明明就是你覺得這個應該很多人都會想要用的這種工具，那照說應該有人做好了，那可不可以就直接拿它的工具來用？或是相反的，我們設計了一個我們覺得不錯的工具，那我們也可以直接就給人家使用。但是變成別人還要知道說我們這個工具要怎麼樣去呼叫等等。每一個呼叫方式又不一樣的話，那就會變得非常的複雜。

$$00:50:49$$

 所以說就有 MCP 這種的概念出來。Google 也有出一個很接近的，那比較不一樣的是 MCP 它的主要主攻的方面是說「單一的 Agent」。就是我今天有一個 Agent，我怎麼樣去呼叫外面的工具。那 Google 比較是那個「兩個 Agent 的中間」，他們要互相溝通的時候，有沒有一些共通的協定。就是我今天是即使是你設計的這個、即使那一個 Agent 完全是別人設計的（根本不是你這寫的），你也知道怎麼樣跟那個 Agent 去做溝通。所以這個就是 AI Agent 出來了以後大家想到的需要解決的事情，現在的確開始在解決這些事情。

$$00:51:39$$

 Perplexity, Felo 與 SearchGPT

事實上因為之前就有介紹過 Felo，那還有一個叫做 Perplexity，他們的特色很接近，他們都是一個在開始的時候都是一個很會搜尋、就是搜尋能力很強。然後 Felo 他們其實最早起家，當然後來大家也發現說比方說 Felo 就又做了很多特別的事情，會幫你畫心智圖（一開始就會了），然後後來又開始會幫你做簡報等等。那 Perplexity 也開始做會做很多的事情，因為他們也有競爭上的壓力。因為他知道別家的語言模型——因為他們不是出基礎語言模型的公司，他們就是用別人的基礎語言模型來使用。

$$00:52:42$$

 它其實就可以想成是一個 AI Agent，那特別強調的就是搜尋的功能。所以你就會發現說它在搜尋的時候，並不是直接拿要問的問題直接去比對哪一個網頁跟我比較接近我就去搜尋它。它是先把問題做拆解，所以它意思就是說它先做我們那個 Planning 的動作。它就是先把問題做拆解，然後說：「我要搜尋這個人家在問這件事情的時候，我應該要考慮到哪一些東西？我可能要搜尋這些東西。」然後它可能才一一拆解，然後一一拆解的再去一一做搜尋，搜尋完了以後再把它合併，合併以後再把它送回來。所以就會發現說它的搜尋真的做得比較好，特別是比開始的時候 ChatGPT 即使是有了搜尋的功能之後，還是做得比它還要好。

$$00:53:33$$

 所以 Perplexity 跟 Felo 他們本身都可以想成是一種類型的 AI Agent 的形式。那事實上 OpenAI 也出了好幾個 AI Agent。第一個 Operator（註：此處講者口誤或指 Connector/Actions，後文修正為 Operator 概念），事實上就是那個要去使用工具的，那就是一個 AI Agent。只要符合 GPT 的那個 Connector 的規格的話，那你就可以把那個工具給 ChatGPT 去使用它。

$$00:54:03$$

 然後當然還有一個就是 Search。雖然大家現在都覺得是一個很自然的，但那個 Search 就是去搜尋網頁。在很早期、兩三年前你開始使用 ChatGPT 的時候，你知道剛剛開始的時候 ChatGPT 其實是不會搜尋的。那在後來出來了之後，你就可以按 Search，就是告訴它說你要去搜尋，那它就會去做。其實這個也是使用工具，就會去開始去搜尋。

$$00:54:37$$

 然後 o1-preview (Research) 我們沒有完全知道它中間到底做了什麼，但是可以想見也是一個 AI Agent 的設計。它可能很有可能是很多個 AI Agent 一起去做，他也需要去搜尋說有相關的文獻，然後他也要去確認核對那個資料是不是正確，然後他要有一個 Planning 先教他說：「你要寫一個好的、要回答好的回答這個問題，你應該要回答到哪一些的面向」等等。所以它應該也是一個 AI Agent 的設計，只是他沒有公開它是怎麼設計的。

$$00:55:19$$

 GPT-5 與未來的 AI Agent

ChatGPT 其實在一度沒有多久前，就是 GPT-4o 時代，你就會發現 GPT 打開的時候，如果你特別是付費了之後，你會發現這個版本超級多。多到很少很少人真的知道說他們的每一個版本到底有什麼不一樣。所以說這個其實是非常的讓人很困惑。那一個困惑就是因為他的那個可以使用的 AI Agent 很多，那你還要決定說我現在要做這件事情，我應該要哪一個 AI Agent。

$$00:55:50$$

 所以 GPT-5 出來的時候（或指 OpenAI 未來方向），最重要想要做的事情，也是它可能以後 ChatGPT 會走的方向，就是他希望今天下了一個 Prompt，就是像以前一樣下了一個 Prompt，它需要呼叫哪一個 AI 它就自己去呼叫了，他也不用你再去按 Search。事實上它現在也是這樣了，其實你不要按 Search 它也會偷偷的找了，不然那很多的問題答案真的會很可怕。

$$00:56:25$$

 所以它就會判斷說它自己要不要使用哪一個 AI Agent。再來就是把語言模型合一，簡單的說它就是自動的去找到它覺得最適合的模型、最需要、最適合使用的 AI Agent，它要用的就是它要做的事情就是這樣。所以這個就是 GPT-5 要做的事情。雖然剛剛開始的時候其實沒有很成功，因為剛剛開始有一點點混亂，特別是它的 AI Agent 它自動去切 AI Agent 它沒有切得很好，所以很多地方大家抱怨連連。

$$00:57:03$$

 但是這應該是未來的方向，不管哪一家可能都是往這個方向走，就是讓語言模型比較單一化。我們覺得單一化，它事實上可能不單一啊，它可能還是有好多個語言模型，但是它可以決定在這個時候不用想太多的情境，你就找一個...它只是要快速答案，然後這個讓我們用輕巧型模型去回答它。那這個時候應該要想一想的時候，你就讓它去想一想；這個時候要去上網搜尋，就讓它去上網搜尋。它自動去決定這件事情。

$$00:57:43$$

 這自動去決定當然會有一些風險，沒錯了。就是因為有時候 ChatGPT 在有搜尋的功能的時候，它還是會很有自信，它就自己回答、它不去搜尋。偶爾還是會發生這種事情。但是久而久之相信這種事情會越來越少。那現在有一個比較相反的事情，你明明覺得這個問題有夠簡單的，你不要想那麼多，它就會開始去思考、好好的思考。它怕你罵它，它就好好的去思考，然後就花了很多的時間。但是這些事情都是在更長久之後應該都會有一些新的改善。

$$00:58:28$$

 Andrej Karpathy 訪談：AGI 與人類學習

我們要特別介紹兩部影片，如果對 AI 有興趣，真的可以好好的花時間去看一下。雖然我們會做一些簡單的濃縮版的說明，但是其實你要知道裡面的意思，其實你真的要看它整個的論述是怎麼論述的。

$$00:59:01$$

 第一個是 Andrej Karpathy 最近接受的一個訪問。那它最重要的一句話就是——這個其實是他們拿來做標題的一句話——就是 AGI 大概還有 10 年。他要再一次的強調，Agent 的這種設計方式，我們大概還要花 10 年去理解它。那第一個我們今天介紹的 Agent，當然是不是這種 AI Agent 要花十年再去理解他？其實當然包括。但是其實 Karpathy 他覺得這個整個的語言模型這件事情還要去重新的思考。

$$00:59:35$$

 所以他覺得簡單的說，他覺得 AGI 的出現——就 AGI 就是像 Data（星際爭霸戰角色）那種新型 AI，就是像真人一樣的那種 AI，或是比真人還要厲害的那種 AI——那個其實應該就是在 Agent 的世代能夠完成的。只是那個 Agent 可能不是我們今天說的這個單純的這四種設計方式，可能甚至原來的那個訓練模式或者等等的模型架構等等，可能還要會有很多的要去重新考慮的地方。

$$01:00:19$$

 他的幾個重要的想法：
第一個就是我們剛剛說的，AGI 基本上現在還沒有出現，或是現在 Agent 還沒有很成熟。也就是說現在的 AI 還不能像真正的「人類的實習生」。這讓大家同學們也放心了，所以你去做實習其實還是非常有價值的，因為現在的 AI 還沒有辦法取代你。你還是可以做得比 AI 更好。

$$01:00:53$$

 第二個很重要就是 「人類學習不等於強化學習（RL）」。為什麼他突然說的這種沒頭沒腦、突然說的強化學習這件事情呢？這個我們在後面會為大家介紹。AI 在訓練它的時候有幾個階段，其實我們等一下就要幫大家說明了。

$$01:01:32$$

 大型語言模型訓練的三個階段

那在第二個階段裡面常常用的是強化學習。就是 AI 在我們前面介紹的說前面的一堆字去預測下一個字，這種模型其實它要做的事情就是預測下一個 Token。這種模型我們常常把它叫做 基礎模型 (Base Model)。基礎模型就單純就是我們前面介紹，它就是生成的模型這樣子。

$$01:02:01$$

 但是如果基礎的模型直接拿來做對話機器人的時候，常常用起來沒有我們心裡想的這麼好。就是因為我們其實需要做一個 對齊 (Alignment) 的動作。Alignment 就是對齊我們人類的價值觀或是對齊我們的需求。比方說那個傷風敗俗的話它不可以說。我們在訓練生成模型的時候，我們可能沒有教它這種事情，基礎模型並沒有這樣。它看過那麼多文字，傷風敗俗的文字它可能也看了不少，所以你要讓它說出一些傷風敗俗的話來，它應該也會說。但是我們不希望它說出來，所以要跟人類的價值觀要對齊。

$$01:02:42$$

 第二件事情，需要跟你的需求要對齊。簡單的說就是它的回答應該是要比較像我們希望它的回答方式，不要再那麼胡說八道啦，然後希望它的回答是友善的（例如啦），然後希望它的回答是要比較條理分明的等等。那個就是要對齊。

$$01:03:06$$

 所以第二步驟其實是對齊。那對齊常常用的技術叫做 強化學習 (Reinforcement Learning)。那簡單的說，強化學習就是...強化學習其實通常是看結果的。比方說我怎麼樣做強化學習？就是我今天通常不看那個過程。

$$01:03:32$$

 比方說啦，現在為什麼很多 AI 比方說 o1 (Strawberry) 很會解數學問題？它其實最引以為傲的就是它的做解那種競賽型的數學問題的時候，它好像比很多的大型模型、其他的那種更大的語言模型還要做得成功。那它其實都是用強化學習去訓練的。但是那是怎麼做的呢？我們剛剛已經說過，它是先產生一些思考，就是推理的過程。推理過程完了以後才說，根據這個推理過程，你就一步一步做，然後再去回答他的問題。做數學問題也是這個樣子做。

$$01:04:11$$

 但是你就會覺得：「喔，這樣好複雜，那還要我們人類去監督它說它的思考過程有沒有錯？」沒有。它的訓練方式就是說，我今天產生一個思考方式，那我怎麼知道這個思考方式好還是不好呢？我只要答對了、它最後的數學問題答對了，我就說想得不錯，弄得到正確答案。那答案錯就是想的不好，你要好好的調整一下下。雖然你沒有教它要怎麼調整，但是你的目標就是最後要答對正確答案。

$$01:04:41$$

 大家都懂數學，就是你知道數學的話，你就知道說有時候那個思考的過程是錯誤的，也有可能得到正確答案，沒錯。問題就出現在這裡。所以說 Karpathy 說那個強化學習——人類的學習其實並不是真的就是只是強化學習這樣。所以他覺得說 AI 現在的 AI 的訓練方式、學習方式還是不太像人類的學習方式。如果要模仿人類的那個學習方式的話，它會有種種的一些障礙。這個大家可以去看，他裡面會講解的更清楚一點點。

$$01:05:25$$

 他有很多很有趣的想法，比方說他會說「忘記」在這個人類的學習當中其實是一件很重要的事情，或是說我們的記憶沒有那麼好，其實對我們的學習是一個很重要的事情。在心理學方面也有一些很接近的一些想法。所以如果特別是你是念心理學等等的，那你也會發現說，現在其實下一次的 AI 的目標其實就是要學我們人類本來是怎麼學習的，然後希望那個 AI 的學習方式越接近人類的學習方式越好。

$$01:06:02$$

 我們剛剛要說的是，在大型模型訓練的三個階段：

Pre-train (預訓練)：基礎模型的訓練。

Alignment (對齊) / Post-train：對齊我們人類的價值觀或是對齊我們的需求。

Alignment 的動作其實想起來有點複雜的原因，是因為第一個我們會發現我們現在很多都是用強化學習去做 Alignment。就是希望它在解數學問題可以給我們正確答案，這個是我們想要的事情沒錯。但是其實這事情可能比我們想像中複雜，因為我們其實更希望的是它的思考過程是正確的。那這件事情很難做到，用強化學習很難做到。所以有沒有可能用其他的方式可以做得更好一點點？

Fine-tuning (微調)：為了我們要讓它訓練的更符合我們的需求，我們還可以做 Fine-tuning。

$$01:07:28$$

 做完了以後，我們的語言模型就會開始發布了。就 GPT 就會出來，ChatGPT 啦、然後是 Llama 啦或什麼什麼就開始發布了。他們都會先經過前面的 Pre-train 跟那個 Alignment 的這兩個動作。交到我們手上，為了我們要讓它訓練的更符合我們的需求，我們還可以做 Fine-tuning。但這個還是真的要訓練。所以這三種都是訓練，只是 Fine-tuning 的階段是我們使用者自己去訓練。

$$01:07:47$$

 那跟大家講一個好消息，就是我們現在目前說到的 AI Agent，基本上我們都還是不用做 Fine-tuning。所以我們在開始的時候、開始使用（大家當然都開始使用來修我們這個可能是你的生成式 AI 的第一堂課），在這堂課裡面，絕大部分的情境之下，我們大概都用 AI Agent 的設計去讓它做得更好就可以。所以通常我們是完全不用再去做任何的訓練。

$$01:08:43$$

 Vibes Coding (感覺編碼/氛圍編碼)

這一次的訪談呢，其實也就是很多很多的（至少看到了三四個）也有 AI 的專家。這不是說大家這個...因為有一些反正我沒有指名道姓沒關係，反正有一些人你就知道說他就是那種「資料型」的 KOL，因為他其實他沒有那麼懂，但是他就是看外面的資料，然後他就可以開始寫文章、寫 AI 的文章這樣，甚至有人可以去講課。然後也有對 AI 真的很大懂的。但是他們普遍的很多，特別在台灣很多人都把 Vibes Coding 跟 AI 輔助寫程式是混用的。

$$01:09:33$$

 要再強調一次，而且他說到這一次的訪談，其實這次訪談真正重點是 Karpathy 想要討論說未來 AI 還應該要去做什麼樣的事情。但是很多人引用的時候是引用他的一段，而且引用的時候都說錯了。他都說「喔，這個 Vibes Coding 或是 AI 輔助寫程式沒有那麼好用」。不是！特別很多人說 Vibes Coding 沒有那麼好用，還說 Karpathy 改了。沒有，Karpathy 沒有改，他是原創者，他是在今年的 2 月才說的話，他怎麼會在現在 10 月他就把它改過來？

$$01:10:28$$

 所以再一次，就是 AI 時代有三種寫程式的方法。這個是他裡面說到的確他這樣說到的。

完全不用大型模型：Karpathy 認為它是不合理的。現在這個時代你還完全不要用 AI 去寫程式是有點不合理，有點太過堅持某些事情了。因為 AI 明明很方便可以幫你做很多的事情，所以他絕對不是反對要用 AI 來寫程式。

AI 輔助寫程式 (Copilot)：就是現在比較多人在使用的，特別是程式設計師。就是比較合理的使用方式，就是人類還是主導的，但是 AI 可以幫我們把那個我們覺得很麻煩、我們還要寫這個，那可不可以叫 AI 幫我們寫？AI 可以幫我們寫。

Vibes Coding：我們上次花了很多的時間去介紹，它基本上就是要我們完全沒有要幫他看程式的那種才叫 Vibes Coding。

$$01:11:56$$

 Geoffrey Hinton 諾貝爾獎演講：AI 比人類聰明？

第二個很想要為大家介紹的是 Geoffrey Hinton 的這次的演講。他一開始就說了一個非常嚇人的話。他也蠻適合當記者，他說呢，如果你今天看完這一個演講、聽完這一場演講的話，你還可以睡得好的話，就是你沒有了解這一個演講。這當然有點誇大，大家不用太擔心，你看完了你真的懂了，其實你應該還是可以睡得好。

$$01:12:36$$

 反正他的重點是要說什麼呢？AI 的世代其實在開始的時候是所謂的「符號式」，簡單的說就是我要告訴所有 AI、那個電腦所有的規則，要我們人告訴它這個基本上要做這樣的事情，然後人要告訴它要怎麼推論。對所有的事情是我們要告訴它的。這是符號式邏輯學派。

$$01:12:59$$

 那到神經網絡的時候，就比較像是所謂的「生物學啟發的 AI」。他這邊翻得很好，是生物學啟發的 AI。就是我們很想要學人類的學習方式。那神經網絡就是下面這一種，現在我們的主流的方法就是下面這一種。他也覺得這些（其實 Karpathy 也是這樣覺得），他覺得未來的年代還是神經網絡的年代，但是它有可能跟我們現在看到的神經網絡的架構不一定是完全一樣的。

$$01:13:38$$

 Hinton 說的最令人擔心的話，他就是說 AI 將會比人類還要聰明。他是非常確切的覺得是這個樣子。因為很多人、特別是還有很多的 AI 的大佬們（像有一位在 Meta 的那一位 LeCun），他就常常覺得大型模型跟人類的想法是完全不一樣，大型語言模型那個預測下一個字根本不是真正的智慧。

$$01:14:13$$

 但 Hinton 是持相反的看法。他覺得說這個大型模型它其實這種也叫做理解。就是我預測下一個字，它怎麼能預測下一個字？它其實也是理解了，它才可以預測。所以他也覺得、他還是覺得這個是一個理解。意思就是說他覺得現在的 AI 已經跟我們人類的理解世界的方法已經很接近了。這是他說的第一件事情。

$$01:14:37$$

 第二件事情，就是 AI 將來一定會比我們聰明。這個他是非常肯定的事情。現在可能還沒有，但是未來一定會比我們聰明。這也是他擔心的事情。所以他才從 Google 退出來，因為他就是發現語言模型出來了以後，他就很擔心說我們再不好好的去想辦法防止一些有可能的危害發生。在這樣的情境之下，所以我們應該要趕快要找一些專家們一起來想辦法，所以他就專心的去做這件事情。

$$01:15:11$$

 再來就是 AI 可能有主觀經驗 (Subjective Experience)。這也是少數...應該說現在的 AI 就是我們已經介紹過了，就是神經網絡基本上都算出來的，所以它基本上是沒有我們人類想像的意識。意思就是說它今天做什麼並不是因為它很爽所以它做了這件事，不是像我們人類這樣子。我們人類做一些事情總是有一些目標，比方說為什麼要賺大錢？那當然是我因為覺得賺大錢很爽快，我可以買很多喜歡的東西等等。那 AI 沒有這個。

$$01:15:49$$

 但是我們也有說過說 AI 有沒有可能做出對我們人類是有危害的事情？這是很有可能的。意思就是說它雖然沒有覺得很開心，它也沒有覺得那個佔領地球、把地球整個把它弄爆它覺得很開心，沒有沒有。但是它還是有可能做到說出這樣的事情。所以說他覺得說很需要、他甚至覺得說未來的 AI 可能會有它自己的主觀經驗或主觀意識。他未來的也可能真的會有這樣的事情發生。所以在那個時候呢、在那個之前，我們可能就要考慮要怎麼樣做這件事情。

$$01:16:27$$

 他倒是覺得 AI 比我們人類聰明，他沒有非常擔心這一點。他在其他的地方有說過為什麼沒有很擔心呢？因為他就說你就像那些大老闆們，其實他下面都有好幾個博士啊什麼，他可能他的學歷是最差的，但是他是大老闆。所以你不需要太擔心，因為我們人類就已經常常在跟很聰明、比我們還要聰明的人一起在工作了。所以我們其實沒有真的需要很擔心說 AI 未來真的比我們厲害，只要我們知道說我們要怎麼樣跟 AI 一起來工作這樣。

$$01:17:06$$

 實作時間：作業說明

好，所以這個就是我們今天的投影片的部分。我們現在要進入實作時間。我們今天的實作時間，我們來試試看哦。我們本來要解釋兩個實作。就是今天的實作呢、今天的作業也是這樣，我們現在說作業。

作業第一個： 你就是去設計一個 AI Agent。但是當然今天是我們的一次性的作業，你也不用設計一個很複雜的 AI Agent。所以呢，你就可以去設計這個 Planning 型的 AI Agent，就是先去打草稿。我們等下先來做這個，可以嗎？我們先來做這個。

作業第二個： 就是我們也可以去做一個 Reflection 型的，也就是兩個 AI 在那邊互相...一個 AI 寫文章，第二個 AI 就跟它講說哪邊沒寫對啦、你怎麼這樣子寫啊、亂七八糟的在搞什麼東西啊...等等。就是這樣子在那邊互相踢皮球的 AI。

我們的作業就是你可以找兩個中間的一個，你覺得你比較有興趣的，然後你就去修改一下，就改成你要的應用。千萬不要看得出來是這個哦，這個不就是我們的上課範例的應用只改了一點點。那等一下我為什麼要解釋第二個？因為第二個非常容易改成那個 Planning 的樣子。

所以兩個都可以，兩個都有示範的範本給大家。而且那個示範的範本已經是標準的就是用 Groq，而且我現在全部改成 Llama 那個發布的這個開源型的版本。就是你自己要在你自己的機器上裝也不是很容易的，除非你去買那個 NVIDIA 最新出來的那個小型的那種 AI 的電腦。
【生成式 AI】09. 為什麼大家說 2025 年是 AI Agents 元年？（逐字稿 Part 3）

時間範圍：01:19:23 - 02:01:45

[01:19:23] 另外我們都是選這個版本，用 Groq，然後免費就可以使用。當然我不知道它的限制是多少了，反正大家萬一發現說它真的擋了我們，再跟我說一下。我們也沒什麼辦法可以想，應該就是停幾天，然後你再試試看它會不會在下次再給你使用。

[01:19:47] 好，那我們先看第二個 Planning 這個階段。Planning 這個其實很簡單，就是第一步你先出現草稿，第二步再根據草稿來做回應。這樣簡單吧？

[01:20:01] 那我們要示範的呢，就是 AI07_Planning（AI07C）。我要再說一次，如果你對寫程式沒有那麼有信心或是沒有那麼熟悉的同學，你只要改 A 或 B 就可以了。A 是我們要做反思（Reflection）的 AI，就是一個是寫稿的、另外一個是評論員的那個 AI。然後 C 就是要去做所謂的思考的 AI，你等下就會發現說它思考真的是騙人的，這個明明就是我們教它要怎麼樣去想的。寫哪一個都可以。那如果你對寫程式很有經驗，你想要挑戰一下，你還是可以做這兩個，因為這兩個其實是很標準的、很重要的範例。

[01:21:04] 那如果你今天覺得說我們比較沒有說到的是 Tool Use（工具使用）這個地方，如果你對程式覺得...特別是你很會問 AI 說你要寫什麼樣程式、要怎麼樣寫的話，也可以，你就去用第二種，就是去做 Tool Use，就是呼叫工具。所以三種都可以。但是因為我們有範例只有兩種，那第三種稍微的有一點挑戰性，雖然也沒有真的很難，但是就是留給說如果你想要挑戰，你也可以挑戰第二種。

[01:21:44] 好的，那我們就開始介紹了。就是這個 AI07C，請助家來幫忙貼一下網址。然後 AI 大寫，然後就會看到熟悉的字眼了，因為又開始又是「正向思考生成器」。然後它有一個很酷炫的名字叫做 Two-Stage CoT，這就是我們出去外面騙人的時候可以說我們用了高級的方法去做了我們的 AI。

[01:22:12] 你可能會想說：「這個正向思考生成器很單純啊，就是要正向思考啊，那這個要 CoT 幹嘛呢？」所以我們這邊 CoT 的部分是這樣想的。我要再強調，你 CoT 的部分拜託你就自己去想。因為有一次我在介紹完了這種的 CoT 的方式之後，大家的 CoT 都是很類似的。我先講一下我們這邊的想法。

[01:22:39] 我們這邊的想法就是，你會發現說正向思考生成器有時候你會覺得他說的還蠻好笑的、蠻有趣的，有時候你就覺得有點點無聊，他到底在說什麼啊？就是因為他每一次都只是接下一個字，他沒有先規劃、計畫，他沒有先好好的想一想。那你想要怎麼樣讓他好好的想一想呢？很簡單嘛，你就先讓他想五種理由，說為什麼這是一個超幸運的事。因為我們要正向思考，所以我們要想那件倒霉事情為什麼超幸運的理由。所以你讓他想五個，可以吧？先讓他想一想。

[01:23:13] 然後呢，你再請他第二階段的時候，再請他說：「喔，你就從那個前面想的那五個中間挑一個最有趣的，然後來把它寫成那個社群的發文。」簡單說就是這樣。所以這個就是我們把正向思考改成兩個階段。

[01:23:37] 那我現在要說的事情是，拜託大家、懇求大家，不要每一個...因為我們上次就是介紹完了以後，大家做 CoT 都來個五個理由。每一次大家都說先想五個理由。CoT 沒有一定要先想五個理由啊！先想五個理由不是 CoT 重點啊！你愛怎麼讓他做、你愛讓他怎麼想，你就讓他怎麼想啊。不一定要有五個理由。大家不管做什麼應用都是一定要五個理由，然後從裡面挑出一個最好的這樣子...不一定。

[01:24:10] 好，我再強調一次，不一定。你完全覺得你要打草稿，你應該要怎麼打，你就怎麼打就好了。所以我們現在就開始喔，就 opening Colab。所以再一次，就是寫 AI 其實很簡單，重點是你的想法。你要先有一些想法說到底要怎麼做，然後才開始真的去做這件事。

[01:25:03] 那我們發現萬聖節快到了，所以我們的可愛的貓跟狗、螃蟹為什麼沒變好？這個時候就是你的了。好，你完全可以改成你的作業名稱。你當然不是正向思考，千萬別再正向思考了，你就做你的 CoT 的例子。好，再度的強調，懇求大家不要大家都想五種。CoT 的階段都是思考階段，不一定都是想五個理由。你有很多可以產生思考的方式，反正你就指引他產生你覺得要回答問題的適合的思考方式。

[01:26:01] 第一段就是要先讀進要的套件。那我們會發現執行的比較好。再一次，我們預設這邊是用 Groq。再一次就是因為它是少見的、不需要付錢就可以開始使用的。那我們這次比較特別的是 Model 我們是選用 OpenAI 的 Model 格式（指 API 相容性），但是是用 Groq 的模型。而且它比我們在介紹那個大到可怕的 GPT-2 還要大很多。GPT-2 是 1.5B（15億），但它現在是很大。我們就來用，然後它就說要授權存取那個金鑰，那我們就讓它授權存取。這都是我們之前做過的，所以應該都可以。

[01:27:06] 那再來就是要讀入這個 AI Suite，就是我們一直用的這個很簡單的 AI Suite 的套件。它就開始安裝。那我們應該已經很習慣了，都會看到可怕的紅字，他只是要跟我們炫耀說他在很認真的工作，我們不要理他。他雖然這邊會出現錯誤，但是我們這邊也可以不要理他。

[01:27:41] 然後呢，這裡我可以選兩個 Model。你會發現這很有趣，因為我們的寫法呢，就可以讓你即使...就是我們等一下會有一個是「寫草稿」的機器人，有一個是真的要根據草稿然後去寫出最後的文章的那個機器人。你可以把這兩個機器人都用同一個模型，也可以用不同的模型。甚至呢，因為 AI Suite 方便的地方，所以你可以設定：比方說第一個是用 Groq，第二個在真的寫的是要用 OpenAI 的模型也可以。就是 Provider（供應商）是不一樣的是可以的。我們這邊只是設定是一樣的，所以你可以設定不一樣的，這個是用 AI Suite 的好處。

[01:28:45] 然後這邊再一次，就只是這一次就只是打開一個空白的對話機器人。然後空白對話機器人裡面呢，我們就是在 Message 這邊就是把過去的歷史的對話記錄放進去。一開始的時候只有兩個對話記錄，第一個就是 System，第二個就是 Prompt。

[01:28:56] 我現在順便教大家有一個很奸詐的小方法。就是我們之前在教大家去使用、要設計這個自己的對話機器人的時候，會跟大家講說你可以先在 ChatGPT 或是其他的網頁上先去用用看。然後你可以先讓他自己說明一下他自己的功能、就介紹自己，然後你再看介紹自己的內容，你就會知道說他有沒有真的懂、他有沒有真的懂那個你希望他要做的事情。

[01:29:30] 那我要介紹一個更奸詐的事情。就是那個介紹的內容，你覺得他真的懂了，那你可以在這邊加上說一個 Role: Assistant。就是你假裝他有說這句話——不是假裝，他真的有說這句話，但是他在這邊因為沒有讓他地方呈現，就是他說的那些話。那這邊還有一個另外一個好處是什麼呢？就是假設你對他說的話、第一次他自己真的說的那些話不滿意，你還可以把他改成你想要他說的。他會以為他真的有說過這句話，雖然他沒有。好，這是一個奸詐的小應用實作技巧，大家可以試試看。你做的時候，你就會發現說這個更穩定，因為他又由前面的再次的強化他自己的角色是什麼嘛，所以他會更知道他應該要做什麼樣的事。

[01:30:45] 然後後面的就是我們呼叫這個標準的、要想辦法去呼叫這個語言模型。那首先告訴他 Model 是什麼，那就是因為這個只是 AI Suite 它規定的 Model 要這樣子寫。要告訴他供應商是誰、我們選擇的 Model 是誰，然後告訴他歷史資料，它其實就會回應了。

[01:31:17] 在執行的時候呢，我們現在就兩個 Agent。第一個是要產生思考，第二個是才是真正的回應（要根據前面的思考來做回應）。

[01:31:29] 所以第一個產生思考的呢，Prompt 是：「請用台灣習慣的中文回應。你是一個正向的思考導師...（中略）...請寫出五種為什麼這個是超幸運的事。」所以他就會寫出來，他就會寫五個理由。這樣可以吧？我們叫他要寫五個理由。

[01:32:25] 那第二段呢，就是說：「想到的五個理由是這個（剛剛前面那五個理由）。然後請從裡面挑一個最有趣的理由，然後根據它寫一段正向思考的發文。」就是這樣。可以嗎？可以喔。

[01:34:31] 其實你也可以把這個剛剛 System Writer 這邊的 Prompt 再把它放到這邊也可以啦。就是因為他是根據這個 Prompt 他才要最後產生最後的結果，所以大家你會發現說其實你有很多地方有可能是可以去做改善的地方。好，第一個就是請大家不要大家都做五個理由。你不要說老師說不可以五個理由、我改三個。不可以！不一定要想理由啊。不一定要想理由然後挑一個最好的，不一定要啊。有一些問題不需要做這件事。

[01:35:11] 所以我們就做完了以後，我們就開始準備要去上架了。上架其實很標準。那我們在實務上用的時候，其實我們不會把五個理由寫出來。但是因為我們現在是在練習，因為我們通常設好了一個 AI Agent，比方說我要做一個正向思考的 AI Agent，做出來我只要把最後的正向思考的結果輸出來就好了。那使用者可能只會覺得說：「喔，你的正向思考好像進步了，好像更好笑一點點」之類。但是他不會知道你中間怎麼想的。就是我們通常思考的過程是不列出來的。

[01:35:53] 但是因為現在我們是在練習，所以非常的強烈的建議大家把思考的過程（即使你不是五個超幸運的理由）你把思考的過程寫出來，然後再根據那個思考的過程，然後去寫出它最後的產生的結果。這樣可以喔。

[01:36:15] 好，然後就執行了。好，執行了以後呢，我們就可以去欣賞我們自己的結果。來喔，大家倒霉的事來...「今天下大雨從教室出來的時候，發現我的傘被偷了」。好，順便對不太熟悉政大的，這是我們政大常常會發生的事情。

[01:37:43] 然後我們就會看到他會先說出五個理由。

因為沒帶傘，路過的陌生人熱情的遞上雨衣或是雨傘，讓我感到人性的溫暖。

防範意識提升...（這啥東西啊）

讓自己學著順著雨天的節奏...

傘壞了以後好高興我可以買新的...（這又不是電腦）

激發創意應變能力...

好，不管了，反正他就想了五個理由了。我們應該還要跟他說那個要選比較合理的理由了，不能只有有趣。好，那他就產生了。所以我們就來看喔，他選了什麼？「傘被偷讓我啟動了雨天求生模式」。他就是說要有創意的解決的方式。喔，好，就是這樣子了。如果你覺得說他打的好像沒有那麼好，那你就可以再去做一些調整。

[01:39:19] 好，所以這個就是我們今天樣請大家做的作業。再說一次，我們有兩個可以選擇。如果你並不是這麼熟悉這個寫程式，你可能會覺得「喔，這個第一個老師連講都沒有講就要我們做，這個實在太過分了」，那你就可以選這個後面的 CoT。我們為什麼選 CoT？因為 CoT 一方面是它也是現在非常重要的應用，第二方面是如果你真的會做 CoT 的時候，你也會對你自己的信心會爆棚啊，因為你就會覺得我自己真的很厲害，我居然可以教 AI 怎麼去思考。然後唯一要注意的地方是請大家不要都用五個理由去做 CoT。有很多做 CoT 你只是要引導他去寫一個比較好的草稿出來。就這樣子。

[01:40:08] 好，我們就休息十分鐘。我們等一下就是進入我們今天的助教時間。

助教時間：Agentic AI 與醫療

[01:40:12] （助教 1：中央大學 呂佩真同學）大家好，我今天要講的是關於...他們推出了一個很長的模型的名字叫做 Checkpoint Inhibitor（免疫檢查點抑制劑），去了解細胞的各種運作，然後去做比對。實驗證實發現說它可以提高抗原呈現 50%，也就是說它可以讓那個腫瘤看起來更顯眼，然後讓它更好去攻擊，提升了免疫療法的成功率。那這個模型它還不太算我今天要講的 Agentic AI，但是算是也是一個醫療的新突破。

[01:41:55] 在過往的藥物開發的世界裡面，就是要花費非常多的金錢跟時間。從實驗室裡面開始研發，然後再到病患手中的真的可以吃到藥物，最少最少平均都需要花費 8 年甚至 10 年以上，那花費更是可能要動用到 10 億美元、上億美元的這個數字。那根據一個美國一家健康科技公司的執行長他的研究，他說將近 45% 的研究時間其實並不是用在科學研究上面，而是他可能要等待上層的決策、要等待各部門的資料整合，或者是跨部門要去進行協調。所以其實有很多有潛力的藥物，他們可能因為受到時間還有資金的限制，他們就永遠都沒辦法進入真正的臨床階段。

[01:42:56] 那現在呢，就是 AI 出現了以後，就是可以讓「助理式 AI（Agentic AI）」來解決這些問題。也就是要解決我剛剛講的這些資訊延遲、或決策延遲，或者是因為成本很高的問題。

[01:43:18] Agentic AI 它就是有三個主要的關鍵：就是它可以自主性，然後它可以自己規劃，然後自己執行任務。就它可以協助很龐大的一個自動化流程。那再來就是它可以有各種的臨床資料、然後實驗數據、還有病歷資料，都會分散在不同的平台，它可以把它們全部整合在一起，然後做出即時的推理。然後再來還有適應學習，就是它可以從這些推理當中找出修正的方式。

[01:43:53] 其實有點像是說，以前的航海系統它是要被動執行指令；那生成式 AI 的話就有點像是 GPS 導航，它提供你建議的路線；那代理式 AI 的話就比較像是現在自動駕駛，它可以自己寫什麼。

[01:44:38] 那 臨床試驗自動化 呢，以一個簡單的例子來說。在臨床試驗中，假如說發生了一個病人發生了心臟病這這種嚴重的不良事件的話，那法規會要求必須在 24 小時內上報到安全系統上面。那助理式 AI 這時候就很有用，就是它可以不間斷、不休息的不停的去偵測病人的狀況，然後自動核對可能安全它的系統庫裡面這個心臟病這個東西，然後跟電子數據採集系統對裝。它裡面可能會偵測到說「喔，有心肌梗塞這個病歷」，那心臟病跟心肌梗塞它就可以把它對應在一起，然後向醫療人員提出建議。

[01:45:28] 那還有更多更困難的任務，可能需要人力花費好幾天以上，那現在的話如果透過助理式 AI 可能幾小時。所以它可以模擬不同的臨床方案的成功率，或者是去預測他們的監管風險，然後還可以挑選出可能這些藥在哪些病患的組合裡面是最合適成功的。那根據估計的話，這樣應用可以讓臨床的週期縮短 25% 以上。

[01:46:03] 那助理式 AI 它不只是會影響到研究的階段，那它也可以延伸到整個藥物的上市流程。但是它可以從策略制定，它可以去整合市場上的所有資料跟研究成果來識別說喔有哪些醫療需求是沒有被滿足的。那再來它可以進行到營運優化上面，就是它可以跟醫師去溝通或者是去規劃要怎麼銷售才可以提升效率。那最後的話它也可以透過個人化的治療溝通，了解病患的狀況來提升這些藥效。

[01:46:51] 所以說，但是 AI 功能雖然強大，但是就是像我們現在在做 AI 一樣，它不是完全取代我們嘛。所以他們有一個名詞叫做 Human-in-the-loop（人類參與循環），就是它只是負責處理這些重複性或者是比較可替代性的任務。那最終可能還有一些關於醫療上面的倫理責任啊，或者是最終的策略的決策，還是透過人去進行的。所以他還是會有完整的問責制度，然後他也是要遵循可能國際上的醫療法規或者是 AI 法案之類的。

[01:47:30] 那根據預測的話，助理式 AI 是由潛力將新藥從剛剛開始的研究到上市階段可以從八年縮短至更短。那每提早一個月上市就當然會有更數以千的生命可以被拯救，所以是一個很好的發展。謝謝大家。

助教時間：VLA 模型與機器人

[01:48:04] （助教 2：台灣大學 黃柳涵同學）大家看得到嗎？好，那我開始我今天的分享了。我來自台大，然後在這邊交換，然後在慕尼黑工業大學讀研究所。然後今天想分享的是 VLA 模型，也就是一種可以生成動作的生成式 AI。

[01:48:49] 不知道大家有沒有對機器人有興趣？因為我本身自己是學機器人在德國。然後其實非常有意思，就是能夠想在未來機器學能像人一樣看懂或者聽得懂指令，然後並且自己能夠決定怎麼行。那麼這個 VLA 模型呢，可以說是實現這個想法的一個方法之一吧。

[01:49:12] VLA 模型的全名是 Vision-Language-Action Model，然後它也就是結合了三種能力：一個是視覺、語言以及行動。然後它是通過影像來理解環境，通過語言來理解指令，並且輸出合作。這個模型是 2022 年由 Google 的 DeepMind Research 等團隊提出（指 RT-2 等模型），然後它的核心思想就是從文字能夠生成一個行動出來。然後也是最近非常火台 AI 的進階形式。這個模型可以不只會看、說，並且可以實際上有個基座可以讓它實現做的步驟。

[01:49:54] 這裡可以看到一個大概的 VLA 的一個模型的框架。也就是通過這個 Text Instruction（比如下面寫「把一個企鵝放到碗裡面」）以及 Image Observation（就是在機械手臂上或者機器人上的這個影像系統，可以是相機或者是 Lidar 或者是 Reader 各種都可），然後通過這些語言和視覺的 Encoder，然後在這個模型裡面可以生成出一個動作的 Action Token。然後可以用這個 Token 直接控制一個機械手。

[01:50:41] 那為什麼這個對於機器人來說非常重要呢？因為傳統的機器人大家可以看到它是重複的單一工作，然後並且大部分的時間是它不可以移動的，它是固定在那裡的。然後並且要在編程或者調試的時候明確的告訴這個機器我們要得做什麼，然後這個時間可能是數天到數月不等。也就 VLA 模型呢，可以非常好的解決這個問題，通過理解來完成這個任務。然後通過它的傳感器、通過影像、通過文字、通過語音來理解這個世界，然後並且變成一個可以自己推理的智能體。

[01:51:21] 下面是一些目前在這個用 VLA 模型做的比較好的一些公司。比如說美國的 Figure AI 和加拿大的 Sanctuary AI 這些公司，他們都是考慮在人形機器人上做一個通用的，比如說在家裡或者說在實驗室拿起來這個實驗的樣本。以及日本的在便利商店，或者說德國的也有在傳統的機械上實現來使用這個模型。然後在倉儲領域也可以做到一些很好的實例。

[01:51:59] 最後給大家講一下它未來為什麼...就是它還沒有大量的實現，也就是說它有一些自己的困難。

真實機器人的示範非常的少（資料稀缺）。

有一些強力的模型，比如說各大公司雖然出來的也相對偏封閉。

缺少對於新任務高效率的微調方法。

根據不一樣機器人的外形啊、傳感器他們也是差異巨大的，所以從一個機器人學到的技能轉移到另個機器人也是非常難的。

多步驟的長任務也是目前來說比較容易失效的。

以及大家常說的這個在推論推理的延遲啊，然後還有能耗。

最後就是一些不安全行為可能通過這個模型導致的。

好，這就是想給大家分享的，謝謝大家。

助教時間：作業講解 & Speculative Decoding

[01:53:39] （助教 3）好，那今天助教課第一個問題還是就是再回復一下上週的作業。有很多同學反應說他們在 Colab 做作業的時候沒有辦法連上 Hugging Face。就是你已經產生金鑰了，然後你你也登錄了，然後你 Google 那邊你也登進去了，但是他沒有權限。

[01:54:23] 那可能的原因就是像第一個解法的話，就是說你在開這個 Hugging Face 的這個金鑰（Token）的時候，你不要開 Fine-grained，你就直接開 Read。那這樣的話你當然就是權限就會...你就會把那個權限給他，那他就可以讀。

[01:55:01] 就是你在這邊 Create new token 的時候你可以開這個 Read。那這樣的話你就可以直接開權限給那個你的 Colab 讓他去使用。那如果你不想要開這麼多權限給他的話，你只想開部分權限的話，那至少你要...我試的話是就是至少要開這個 Repositories 裡面這個第二個。那就是你只有你要授權它才同意的話，那你記得在這邊至少要開第二個這一行，那它才可以去讀這個東西。

[01:56:06] 那今天我還會分享兩件事情。一個是我覺得一個蠻有趣的、在 LLM 裡面的一個技巧 Speculative Decoding（投機採樣/投機解碼）。然後我這個分享完之後，我等下會分享一個就是我最近在 AI 裡面看到一些有趣的事情。

[01:56:41] 這個 Speculative Decoding 它到底是要做什麼？它其實算是 Google 跟 DeepMind 幾乎算是在同一個（就前後相差大概幾個月），那他們提出了一個生成加速的技術。那它主要就是用在...不僅限於但是可以主要是可以用在就是這個 LLM 的這個我們推理的過程。

[01:57:49] 那它的核心目標是什麼呢？它的核心目標就是說，我們今天一個 LLM 在生成我們這些回答這些 Tokens 的時候，我們有沒有辦法去提升它生成的速度。不是說提升它生成品質的內容，就是它的正確性或是他回答的語氣都跟這些無關。我們單純的在乎說如果我們想要讓我們這些 LLM 在跟我們對話的時候生成更快，那有沒有辦法？

[01:58:33] 那這個 Speculative Decoding 的話，它就是先簡介說它就是想要用一個草稿模型（Draft Model） 跟一個比較大的 Target Model 去組成像是一組的概念，然後讓這個 Draft Model 去輔助 Target Model。那它在 LLM 的加速效果，如果你設定得好的話，通常可以讓它快到兩倍或是到 2.5 倍的這種生成的這個推理速度的提升。

[01:59:13] 所以再講一次說為什麼需要這個 Decoding 的技術？因為像我們傳統的這種 LLM 或或者是 Auto-regressive generation（自回歸生成），那它這種 Model 的話，它就是用前面的這些字，然後我們去做一個預測嘛，去預測說下接下來出現什麼字的情況是最高。那我們就是用像這種的方式去一個字一個字去做生成。

[02:00:00] 那如果你這個 Model 很大怎麼辦？第二是說如果你 Model 很大的話，你的計算量就很高。那再來就是說你的 Model 越大的話，你去在做這種 Token 的生成的時候其實就會比較是耗資源的。那其實還有一些原因，比如說像是如果你是問很簡單的問題的話，你也不一定需要這麼大量大型的 Model 才能回答到你一些的問題。如果你問題就是很簡單的，就是翻譯成英文或者是做一些簡單的簡介啊或什麼，它不一定需要用到所這麼大的模型。

[02:00:44] 那這個 Decoding 目的就是說：不僅剛剛說的說很簡單，就是困難問題也可以。那就是讓大模型一次生成很多、要生成很多 Token 的這件事情，我們去提升它的速度。如果你做得好的話，差別可能就會像是生成速度看起來就會有差別。左邊這個就是一般的 LLM 或是自回歸模型在生成文字的速度，那右邊很明顯就快非常多嘛。這篇文章也非常有趣，就是大家如果有額外興趣就都可以參考。

[02:01:40] 所以再回來基本的想法。那我們剛剛講這個 Decoding 我們是說有一個是 Draft Model 跟一個 Target Model。那這個 Draft Model 很明顯的就要比這個 Target Model 還要小嘛，因為我們剛剛是說什麼？我們剛剛是說這個大模型在生成的時候很貴，或者是說我們的計算量很高，所以我們想要用的概念就是：用一個小的 Model 去做輸入，先根據當前的輸入去預測很多個 Token（可能就三個、五個、八個，這邊就是可以自行去設定）。
【生成式 AI】09. 為什麼大家說 2025 年是 AI Agents 元年？（逐字稿 Part 4）

時間範圍：02:01:45 - 影片結束

[02:01:45] 那它去算這個，它簡單來說就是去算你這個 Draft Model 生出來的字跟較大的模型生出來的這個字是不是一樣。但我們不可能直接去決定說「就是這個字」，因為如果你都已經把那個字做出來的話，你就不需要再一個小 Model 去幫你生成一個字嘛。比如你現在小 Model 生成一個字，結果你大模型你也生成一個字，然後你又說去檢查大模型有沒有這個字，然後說有、大模型就生成這個字，那你就再把字拿起來。這樣不是很奇怪嗎？

[02:02:11] 所以它其實是去算這個機率分佈的合理性。那如果算出來結果是合理的話，那我們就接受。那如果是不同意的話，那我們就是...比如說我們小模型生成兩個 Token 之後，在第三個 Token 跟大模型就不一樣了，那我們就在這個地方切掉，讓下一個字直接由大模型去做生成。然後生成完之後，我們再重新讓小模型進來繼續去預測接下來的幾個 Token。那就是一直重複這個動作，直到你這個整個模型、你整個這個對話結束。

[02:02:53] 所以圖示的話就是這樣子，這個就可以大家看一下。比如第二行好了，這個綠色的地方就是由這個 Draft Model 生成的。然後我們剛剛不是說去算嗎？去算說是不是應該要是這個字。算出來是，那就是讓它放在這邊。那假如說像這個算出來這邊（第五行這邊）算出來是 0，但其實如果應該要是 1 的話，那我就不要這個 Draft Model 的結果，我把這個 Target Model 從拉進來開始做生成這個字。接下來這幾個字我再使用 Draft Model 去做生成。

[02:03:52] 好，那你就就是這樣疊疊疊，有點像是這種跌跌撞撞的感覺，就是 Small Model 一直進來幫你做生成。那這樣到底有什麼效果？因為我們剛剛講的就是說 Draft Model 你要挑的 Model 嘛，所以做這件事情可以...如果很順利的話，就比如說這邊十個字可能有八個字啊、六個（至少可能六個）相同的話，那當然很棒，它就可以去壓縮我整的整個這個 Token 生成的時間。

[02:04:25] 當然說如果你的這個技術你實作出來就發現說你的這個接受率幾乎是 0，那這個效果可能就很有限。反正我們現在就是先試試著去做這個事情、先有這個概念。就是說如果你這個 Draft Model 是可以很好的去輔助你的這個 Target Model 的話，你的 Token 的生成的速度就會很快。對，下面這個就是剛剛的這個解讀嘛，綠色就是讓這個 Draft Model 去生成這些 Token，然後讓大模型去檢查。

[02:05:05] 這邊還有一個就是其他教授的 YouTube 也有講解一個關於這個 Decoding 的這個技術，那大家有興趣可以去參考。那我這邊截一些我覺得很有解讀意義的地方給大家參考。這個「預言家」就是我們剛剛講的這個 Draft Model。那比如說我們現在讓預言家輸出這兩個字（就是這個淺紅色），反正就是第一個框框跟第二個框框好了。

[02:06:42] 那假設這個都是對的，那其實它可以同時做什麼事情？就是這第一條線、第二條線、第三條線它可以同時都去做。我們剛剛講的去做那個算「它是不是應該要是這個字」嘛。如果是這樣的話，我原本是不是只可以做一行？但是如果我們可以做這個同時做這個運算，其實就是那個平行運算的概念。如果我們可以同時去做這件事情、同時去檢驗這件事情的話，那其實這個模型它的速度是幾倍？就是三倍嘛，Almost 三倍。因為我們就說這個 Draft Model 很小，所以它預測的速度比較快，幾乎是三倍。

[02:07:37] 那假如說一個是錯的呢？一個是錯的話會變成說你原本就有一個第一個字嘛，第一個字跟這個第一個字的結果是對的。那第三個，因為你開始有錯誤資訊了對不對，所以我同時驗證這三件事情，第三條不行嘛，所以第三條最後出來的是不能用的。但是還有前面這兩條啊！所以如果假如說你生成兩個字裡面有一個字是對的，那會怎麼樣？那就會讓這個模型大約是兩倍的這個提升嘛。因為我說的就是它生成的這個速度、這個 Draft Model 它生成速度很快（我們先忽略），那其實就是我們賺了一個 Token 對不對？因為我生成一般的這個比如說 GPT-4 生成一個這個做到這邊的時候，我就已經做出兩個 Token 了，所以就兩倍。

[02:08:32] 那假如說都錯呢？都錯的話就是都沒什麼好講的嘛，那就只有這第一行這個還沒有把這個錯誤的東西放進來的時候會是對的，那這個東西就是確實它就會慢一點。好，但是我們的目標就是還是要讓這個正確率高一點嘛，因為如果你的這個正確率高的話，那這至少下面這個最後一張圖這個發生的機率就會低很多。那如果這個東西只是偶爾發生的話，其實不影響。因為我說過 Draft Model 的生成速度是比較快的，那其實你前面比如說像這種兩倍啊、三倍，你如果大部分時間都賺到...就是都做到這件事情的話，你就賺到了很多時間嘛。所以就是可以比如說就可以讓你 Token 生成速度快兩三倍。

[02:09:25] 小 Note 與實作

那這是一個小 Note，就是說如果今天在實作的時候，如果你是直接接這種主流 API，就是比如說你想把這個 OpenAI 或是把那個 Claude 什麼直接接進來的話，你沒有辦法直接去在這些 API 去開這個 Decoding 的參數。那至於他們內部裡面有沒有開、要不要開，這個就是那些公司自己決定的。這項技術如果我們是在落地做的話，我們就是盡量用去載 Hugging Face 的這些套件來做。

[02:10:03] 那參考實作的話，這一篇這是一個 Google 的裡面的工作人員他寫的文章，那我有參考這個文章去做實作，然後有做一些修改（因為都要跑很久，所以我先跑一部分）。那你就是跟著我的這個步驟做：登錄 Hugging Face。那如果你這個可以開 GPU 的話，因為我是有買這個 GPU 的，那如果你可以開的話，你就盡量開。因為這種模型它畢竟還是你如果載到本地端的話，然後有載比較大的模型，它還是不是會對你的算力有稍微有點要求。沒有的話就是 Google 還是會開一些免費的資源，但是因為 Google 對於免費就相對來說沒有那麼穩定。對，就可可以盡量能開的話還是可以開一下。沒有強迫大家要一定要花錢。

[02:11:10] 然後接下來我們就是用兩個：Gemma-2B 跟 Gemma-9B 的當做 Draft Model 跟 Target Model。那這邊就是去下載啦，我已經下載完了。好，是說像這種載比較大的話，好像有開那個 Pro 的話速度也差蠻多的，我實測下來的效果是這樣。

[02:11:39] 然後我們就做一些這邊就是模型的實作，就是我們怎麼做這個剛剛講的這個 Draft Model、Token Model。不過我們並不是去刻我剛剛講的這件事情，因為這個如果我們自己單純去刻的話，確實可能出來效果沒有這麼好。那它有一些套件在這裡，所以最前面有載一個這個 Transformers 這個版本之後的話，它裡面有一個套件叫 generate 的這個 function。那你用進去之後，你就可以去決定說你要不要去做這個效果。

[02:12:21] 好，那我們就對，因為我們就直接跑好了。那就先試一個 Prompt 好了，那這個是英文的 Prompt，我們先試英文，然後我們就生成 100 個 Token。那會花一點時間啊，因為這個這些 Model 還是...就是這個 9B 跟 2B 還是離這個現在的這個最新的 Model 有一些時間了。那我們跑跑看看是什麼結果。好，就等他一下。那下面還有一個是中文的，那就一起先讓它等待，我們就等一下吧。

[02:13:12] GPT-4o 寫貪吃蛇遊戲

然後在等待的時間我就插一個小播好了。就是因為上一堂課是有關 Vibes Coding 的事情嘛，那我就有用那個 GPT-4o 看了一下。我覺得我發現其實 GPT 做這種小遊戲好像還蠻不錯的。因為我只有寫一行而已，那他就跑了。大概的功能還有有講嘛，那我覺得它的優點是它有這個預覽的這個功能。貪食蛇應該是一個最經典的啦，你看其實就蠻正常的，都你看上下左右，然後還可以暫存。這個都是很經典的這個功能嘛。對啊我現在只有一手拿麥克風就沒辦法秀給你們看。好，那我們就還有暫停，真的蠻不錯的，可以停止。而且它還有一個這個我覺得也蠻不錯的，就是他下載的話直接就把我們 HTML 檔打好了，我們也不用再去找一個這個記事本或什麼的去把它存起來。所以我覺得你們也可以，如果真的想要做這種小遊戲的話，GPT-4o 是一個不錯的嘗試。

[02:14:42] Speculative Decoding 實作結果

好，那這個就跑好了。那因為我有開就是讓它會生成一樣的這個 Token，所以這個 Token 會一模一樣。那可以看一下就是說 Normal 的這個 Inference Latency（推理延遲）是他說生成這些 Token 花了這麼多秒（約 62 秒）。那如果做這個 Speculative Decoding 的話，它就是 22 秒多。所以大概就是三倍嘛，接近三倍。

[02:15:09] 然後中文的話我也有試過啦，是跑得出來的。Gemma 的話它支持英文跟中文。OK 他在做了。下面這些補充就是一些細節。你看這個就是一樣嘛，而且其實秒數沒有對吧？你看這邊英文 62 點...對，所以它其實也沒有不會做中文就會特別吃力。那一樣的就是做中文跟英文這個效果都有。

[02:15:40] 那我們補充一的話就是一些比較數學的地方，這個有興趣的人再再看一下好了。那簡單來說就是白話翻譯就是說我們到底是怎麼去接受那個模型的。那假設說我們就是我們這個大模型跟小模型，它其實是會輸出一個分佈。那我們就隨機從 Uniform (0, 1) 挑一個數字出來。那如果 (0, 1) 小於等於下面是小模型、上面是大模型的話，那我們就接受那個 Draft Model 生出來的 Token。那其實就是什麼意思呢？就是說如果 $P(x)$ 就是 Big Model 嘛，如果你大模型的機率比小模型的機率還要大的話，那大模型就是會更同意小模型，那我們就會提高它的接受率。那相反的話就是我們大模型認為小模型生出來的東西太樂觀了，或是覺得它是錯的，所以我們拒絕的機率就上升。當然我們就他你看它還是有一點點這個 Random 性質的，他並不是說小於就想要完全拒絕，大於就完全接受這樣子。這個是當初他們在設定這個 Decoding 這個技術的時候，他們做到的事。

[02:17:05] 好，那最後再講一下說，就是我們實際上是不是讓這個大大的模型一直去做 Token 的這個 Generation？我們其實大部分時間是希望 Token 去...就是大模型去做這個 Token 的 Verification (驗證)。就是如果一直去做這個 Verification，然後可以把 Draft Model 的 Token 他覺得是 OK 的就一直拿進來的話，我就會大模型就可以不用一直做 Token 的 Generation。那所以這兩個東西是有差別的。那 Generation 的話它就是我講的嘛，根據上下文或是說根據上文...那反正就是根據這段文字你要去生成下一個 Token 對不對？那因為它是有一個分佈的嘛，所以你就要去算，算的事情就很多。那如果你只要做 Verification 的話，你就去做那個機率，然後去看那個機率跟那個大模型產生這個機率分佈跟小模型產生機率分佈有沒有差太多這樣子。

[02:18:17] 好，那應該也跑完了吧？這邊又跑完了。對，剛剛就跑完了。好，所以這就是一個...就是算是一個技術分享。那跟你們說就是說除了就是把 AI 做的做得很精準，然後或是做的可以回答很多問題的話，其實還有很多面向可以去鑽研。就是說就是怎麼把你的這個 AI 的 Token 做得又快然後又不犧牲它的品質。像這個技術它做出來就是它對文章生成的品質是完全沒有影響的。OK，好，那麼我的那個...

[02:18:59] AI 加密貨幣交易競賽案例分析

好，那最後的話我想分享的是一個 Case Study，就是最近看到的一個 AI 炒股大賽（加密貨幣交易競賽）。我覺得蠻有趣的。那他是這個 NOF1（主辦方叫 NOF1 啦，但好像沒有到真的超有名），但他這個 Project 在 Twitter 跟在 Threads 都蠻紅的。那他大概是 10 月 17 號開始，結束日期是 10 月 13 號（應為口誤，可能指為期一段時間）。這大概兩週的時間。

[02:19:33] 那他放了他把這個 DeepSeek、Qwen、GPT-4、Gemini 1.5 Pro、Claude 這幾個模型都拿去做比特幣加密貨幣的交易，那想要去看說它的結果是什麼。我覺得這個蠻有趣的，分享給大家。那他就說他就是都給 1 萬美金，而且這個是真實的那個現金賬戶喔，他說他標榜是他背後是真實的現金賬戶。那他說給的這個 User Prompt 都是一樣的。然後它就會有像這種 Position 啊或是 Model 或是 Model Chat 這裡它就會說那那回應是什麼。

[02:20:25] OK 那看起來就是 DeepSeek 跟 Qwen 是贏的嘛。那有些人會說 DeepSeek 是磨坊那個量化嘛，他們是做量化公司的很強。然後 GPT 跟 J（Claude/Gemini）好像非常非常弱。好，但是但是...我不是我跟你們分享這個很有趣，但是我不是叫你們回去就趕快去跟那個 DeepSeek 聊天，然後開始炒股哦。就是如果你們炒股的話有輸錢不要來找我，對千萬不要來找我。我只是想跟你們分享說，你看現在這個已經翻翻倍了。

[02:21:06] 那我只是想要藉這個專案跟你說這個東西看似有兩件事情嘛。第一個是說怎麼 GPT 跟 J 這麼這麼大間的這種公司，他做這種股票預測的效果這麼...（差）。那還有一件是說怎麼這個 DeepSeek 跟 Qwen 就做得這麼好？那當然可能就跟他們可能因為比如說像這個 DeepSeek 它是他們是一間量化公司做出來的模型嘛，就他們的母公司就是一間量化公司（幻方量化），所以可能他真的為了特別多的這種財務資料。

[02:21:46] 但是以我們以我財務的觀點來看，你去看一下，就是說像 Qwen 這件事情他做了什麼事情？他做了一個 20 倍的槓桿。那如果你們沒有...你們有如果你們有概念的話，就是說你投一塊等於你漲一塊就等於漲 20（漲 1% 就等於漲 20%）。如果你輸 1%、原本輸 1%，你也會變成是輸 20%。那這個是很劇烈的這個漲幅。所以這個會有一個問題就是說，你們相信這就是像現在這些軟體他有辦法真的做到風險控管嗎？

[02:22:26] 那為什麼特別提這件事情？是因為我們要去看我們現在到底在做什麼。比如說主要的這個市場是跟隨比特幣波動的啦。那我們看比特幣，比特幣的這個走勢它原則上是一個正向的嘛，當然有一些波折什麼。所以這其實代表說你如果今天你這些模型他選擇去做多的話，基本上都會贏。那但是如果今天市場翻轉了，你看今天從這邊到這邊的時候，那如果他今天是跌破呢？那你這些機器人是有辦法去做道止損的這件事情嗎？這沒有，我沒有辦法跟你保證。

[02:23:08] 所以我要提醒各位的是說，不要太...你們在做這種尤其是在做這種交易的話，你們還是要小心一點。對對，但這個這個真的蠻有趣的，那你們可以再有興趣的話就去關注一下。

[02:23:27] 好，那好像就是看一下同學有沒有發問。剩五分鐘的時間。對，好，好，那就再回到...誒，我是把它打叉了嗎？這個其實我也做失敗蠻多次啊。

[02:23:52] 主要就是說如果你想要去接直接去接 API 的話，我先跟你們講說如果直接去接 AP、想要直接去接那種現成 API 可能會有什麼問題。就是說他可能不讓你去做，就是不讓你做這個。所以其實你根本就那個 Draft Model 就插不進去手。那反而你跟 AI 聊天他試著幫你做這種 Draft Model 的時候，你會發現它的速度...你把做這個技術的話可能是沒有做比沒有做還糟。你可能做了還有兩三倍的時間，因為他可能會變成是大量的時間去一直去叫呼叫那些 API。但你們這種一來一往的事情就會讓你們這個模型的速度可能會變很慢。

[02:24:37] 所以我是說你們如果想要在本地端自己就是實作一次這個的話，我的建議就是去 Hugging Face 找兩個模型。那如果這兩是 2B 跟 9B，那如果你們還是覺得太大的話，可能就這個 Target Model 稍微再找小一點。那我的建議是不要至少比較小於三倍的那個的那個參數的數量，不然你們兩個模型的效果可能就沒有差異可能沒有那麼大。當然現在好像有一些小模型但是非常強勁的嘛，那就是我覺得都供大家去做一個發想。對，就是這件事情，蠻有趣的。

[02:25:22] 好，那誒，那個點名導單在那個直播的 FB 直播那邊哦，政大生一定要去點名哦。好，那如果沒有問題的話，我們今天就就上課到這邊。謝謝大家。Yeah。
[00:00] 另外一個重要的模型，我們先稍微看一下整個大綱。如果畫面傳出去是正常的就好，我看那邊有點被拉長，確定一下。好，正常就好。目前我們已經學會或提過的技術：第一個是目前比較少用的「生成對抗網路 (GAN)」，這是我們在開始時就介紹過的，也是大家覺得生成式 AI 最早期的技術。雖然上一次大家覺得 GAN 最有希望，但沒想到現在比較少人提起了。不過知道這件事也很好，至少我們了解了比較完整的生成式 AI 狀況。

[01:13] 第二個就是我們要預測下一個 Token 的標準大型語言模型，這是我們花了相當多時間介紹，也是目前最紅的一種生成式 AI 的樣貌。第三種我們要介紹的是圖像生成。前面主軸還是在文字上，從今天開始我們要介紹圖像的生成，特別是一個新的模型叫做「擴散模型 (Diffusion Model)」。

[01:41] 順便說一下，如果這三個技術你都至少知道概念，不一定要寫出數學公式，但至少知道這三種的大概樣貌，其實你就比外面很多自稱懂生成式 AI，甚至在外面開課的人還要清楚生成式 AI 的完整面貌。這就是我們今天要介紹的擴散模型。介紹完之後，大概現在主流的生成式 AI 模型大家就都知道了。

[02:21] 我為什麼前面加個「本來」呢？因為擴散模型本來是把生成對抗網路 (GAN) 打趴了。說打趴有點過分，但也差不多啦。反正擴散模型出來以後，大家為什麼覺得 GAN 沒有那麼好用，就是因為擴散模型。那我們今天開始要介紹擴散模型。那為什麼前面又加了一個「本來」？難道後面又發生了一件事情？沒錯，後面又發生了一件事情，而且這是在我們這學期開課之後比較明顯的變化，我們後面會為大家介紹。但是擴散模型本身在介紹生成式 AI，特別是要介紹現在完整的面貌時，它還是一個非常重要的模型，所以我們從今天開始來介紹一下。

[03:19] 我們前面已經提到過 Embedding (嵌入) 這件事情。雖然有提到過，但在大家剛開始還沒有接觸很多的時候，可能沒有這麼強烈的覺得 Embedding 是什麼重要的事情。所以我們現在花一點時間來更深入了解一下。Embedding 簡單說就是：我輸入一筆資料，想要找到它的特徵代表向量。

[03:56] 我們現在其實已經有一點經驗了。如果要用神經網絡做這件事情，以前所有的 AI 都是我們需要很清楚知道輸入是什麼、輸出是什麼的「判別式 AI」。比方說我們要訓練它辨識八哥，輸入就是一張八哥照片，輸出就是它的答案。我們告訴它的方法比較特別一點，例如要辨識三種八哥，我們就給它三個分數，最後訓練好後看哪個分數比較高，就覺得是它了。這就是我要很清楚知道輸入是什麼、輸出是什麼。

[04:41] 但是 Embedding 這件事情有一點點特別。Embedding 是說，我輸入一張八哥的照片，但我今天不一定是為了辨識八哥，或者說辨識也可以，總而言之我想要找的是這隻八哥的「特徵代表向量」。如果用人話說，就是我想要找到 AI 對這張照片的想法與理解。實際上 Embedding 在很多地方有不同的名詞，有時候會叫它 Latent Vector (潛在向量) 或是 Latent Tensor，不管怎麼樣，它就是要找到輸入的東西（例如一張照片）最適合代表它的特徵代表向量。

[05:34] 用人話的說法就是：我們希望找到 AI 對這張照片的理解。那個理解是一個抽象的理解，因為拿出來的時候，我們並不知道它到底在想什麼。因為我們不知道它在想什麼，所以這個訓練資料基本上是沒有辦法準備的。我們不知道這張八哥照片對這個 AI 來說，應該要放哪一個特徵代表向量才是最適合的。

[06:04] 所以在這樣的情境之下，我們沒辦法找訓練資料。那怎麼辦？事實上我們前面有碰過，但是沒有很正式的介紹，所以希望花一點時間正式介紹 Embedding。因為這件事情是我們在高階 AI 應用中非常重要的概念。如果你真的懂了，你會發現對 AI 的認識會更深入。

[06:40] 所以不只是圖像（八哥辨識），人物也可以訓練一個神經網路找到很好的特徵代表向量。文字當然可以，這是我們前面做過的。數據也可以，比方說一支股票過去 20 天的數據，如果我可以充分理解這支股票，找到一個很適合的 Embedding，那我們用這個 Embedding 去訓練讓它分辨這支股票會漲還是會跌，可能比直接輸入原始數據還要容易一點。因為這是 AI 經過理解、思考之後的結果，可能會比較容易訓練得好。聲音也是，我們可以想辦法把它變成適合的 Embedding，然後看是要做語音辨識或其他任務。

[07:55] 當然重大的困擾就是我們前面說過的：我不知道什麼是適當的特徵代表向量，因此我沒辦法準備訓練資料。但我們前面說過了，在神經網絡裡面，比方說全連接層，我只要決定這一層有幾個神經元放進去就好了。所以輸入是一張照片，輸出的時候就是一個向量了。其實這個向量就可以當成我們的 Embedding。當然不是說隨便做一個它就會自動輸出好的 Embedding，我們需要給它一個小任務去訓練它。

[08:54] 這個現在粉紅色的層我們可以當成 Embedding，下一層紫色的也可以當成它的 Embedding。嚴格來說，任何一個隱藏層的輸出，我們都可以當成 AI 對這個輸入資料的理解，也就是特徵代表向量。

[09:41] 但我們不能亂做。不能只是設計好一個神經網絡，輸入照片，輸出 128 個數字，它不會自動找到適合的 Embedding。我們需要給它一個小任務去訓練。這個小任務是我們很容易準備訓練資料的。比方說「八哥辨識」就是一個小任務，輸出可能是三種八哥的機率。當這個模型訓練得很好的時候，我真正要的並不是最後的輸出結果，而是中間某一隱藏層的輸出。

[10:55] 大家一定會有一個問題：為什麼這樣輸出來的向量可以當成很好的特徵代表向量？因為出了這個特徵代表向量之後，經過後面的隱藏層，它真的可以把八哥辨識得很精確。那我們就可以知道，它前面那個向量應該對八哥的理解很正確。想法就是這樣子。第一次聽到可能會覺得抽象，但久了以後你會發現這是很自然的想法。

[11:42] 總而言之，我們去設置一個小任務。這個小任務是我們覺得它需要理解輸入的部分才能完成的。比方說要做八哥辨識，它當然要理解這張照片是哪一種八哥才能順利完成。如果訓練成功了，我們就把中間某一個隱藏層的輸出當成它的理解，也就是 Embedding。這種小任務叫做「代理任務 (Pretext Task)」。意思就是我們並不是真的要執行這個任務，我們真正要做的是訓練中間的這個 Embedding。如果順利解決問題，我們就知道這 128 個數字真的很適合當作輸入的特徵代表向量。

[13:20] 訓練完之後，前面這一段模型我們通常叫做 Encoder (編碼器)。不管是照片、文字或數據，輸進去以後我就找到它的特徵代表向量。這段就是 Encoder，負責找到 Embedding。後面那一段通常叫做 Decoder (解碼器)，不一定會使用到，因為有時候不一定想要把它還原。如果我們只是要找 Embedding，通常就是用 Encoder 這一段。

[14:36] 我們先舉幾個例子，像 Word2Vec，這是以前在自然語言處理 (NLP) 非常有名的一個模型。簡單說就是要透過代理任務把文字變成向量，而不是只用 One-hot Encoding。One-hot Encoding 只是編號，本身沒有意義。Word2Vec 設計了兩個小任務。其實你不一定要照它的設計，只要任何你覺得需要看懂文字才能做出來的任務都可以。

[16:01] 事實上 Google 他們做 Word2Vec 時，很喜歡把第一個隱藏層輸出來的就當成特徵代表向量，而且第一層的 Activation Function 是線性的，也就是輸入什麼就輸出什麼，完全是線性設計。到現在你會發現很多地方做 Embedding 還是很喜歡這樣做。

[16:53] Word2Vec 設計了哪兩種小任務？第一個是先看旁邊的字，預測中間的字 (CBOW)。這很明顯需要知道文字的意思才能填，其實就是克漏字測驗。第二個任務比較複雜一點，給你中間的字，要預測前後旁邊的字 (Skip-gram)。這兩個小任務都取了很酷炫的名字，但其實就是設計兩個小任務來訓練。

[18:37] 代理任務真的可以自己設計。比方說用前面的字去預測下一個字。我們前面說過，如果用 RNN 設計，隱藏層可以想成是 AI 的記憶。我們也可以把中間這部分當成 Embedding。目標是預測下一個字，這當然要懂文字的意思才能做。現在的 Transformer 大型語言模型，通常是把第一層就當作 Embedding 層。它一邊做我們訓練它的東西，第一層就自然而然做出了應該要有的 Embedding。

[20:50] 回到生成式 AI。我們要做的事情是找到很好的特徵代表向量 (Embedding 或 Latent Vector)，然後去生成一個應該代表這個意思的東西。但我們會遇到一個困難點：我們剛剛說過 Embedding 通常是一些抽象的特徵代表向量，我們不知道它的意思。如果要讓它創作，我們到底應該怎麼樣放進這個潛在向量？

[22:13] 第一種方法是亂放。如果我們把每一個數字都假設是標準常態分佈（平均值是 0，標準差是 1），每次都從標準常態分佈抽一個數字放進去。這樣的好處是可以天馬行空地想像，在很多生成任務裡是不錯的想法。 第二種方法是先訓練 Encoder，找到代理任務訓練出來。訓練好後，我想生成貓的照片，我就找一隻真正的貓的照片輸入 Encoder，得到 Embedding。然後我把這個 Embedding 稍微修改一點點，再放進 Decoder。我希望因為只改了一點點，生出來的應該還是貓的照片，只是不一樣的貓。

[24:13] 我們現在要介紹一個很難被大家想像可以用來應用的模型，叫做「自編碼器 (Autoencoder)」。在教擴散模型時，很多人稍微忽略了這個概念，其實很簡單。Autoencoder 也是要找到圖像的特徵代表向量。它的代理任務非常容易準備訓練資料：輸入哪一張照片，輸出就是原來的那張照片。

[26:28] 你當然會問這有什麼用？輸入是它，輸出是它自己。這模型本身沒什麼作用，但是中間有一個隱藏層，我們設定向量維度（比方說 512 維），這當然希望比原來的像素還要低很多，不然乾脆直接把照片輸進去就好了。中間這一層輸出叫做 Latent Vector，就是我們要找的 Embedding。因為這個向量最後可以順利還原這張照片，所以它真的掌握了這張照片所有重要的資訊。

[28:31] 訓練完以後，我們就可以用 Encoder 輸出一張照片的特徵代表向量。但我們是生成式 AI 課程，重點在後面。我們希望今天放一隻兔子的特徵代表向量進去，稍微變動一點點，不要太多，放到 Decoder 裡面，應該還是一隻兔子，只是顏色或動作不太一樣。也就是說，我們希望如果基本上是同一種類型的照片，它找出來的 Embedding 在高維度空間裡面應該很接近。

[30:29] 但很遺憾的告訴大家，結果是不太可以。你動一下之後發現完全不一樣了。甚至一張兔子的照片找到 Embedding 後稍微改變一下，還原之後可能根本不像一張照片，變成亂七八糟的東西。這表示中間的 Embedding 是用一個很特殊的記憶方式把它記起來。它基本上就是強記前面那張圖的樣子，所以它可以順利還原。但是你改動一點點的時候，它並不會生出一個很類似的圖。所以單純的 Autoencoder 沒有辦法滿足我們的需求。

[31:36] 那怎麼辦？有沒有可能訓練一個可以生出不同圖像的模型？答案是可以，就是我們要介紹的這個名字聽起來很高級的「變分自編碼器 (Variational Autoencoder, VAE)」。

[32:03] Autoencoder 沒辦法做到的原因，是因為中間的 Latent Vector 每一個數字其實沒有符合任何的常態分佈，基本上是亂數出來的。所以它就用很神奇的記憶方法把它記起來。那我們到底要怎麼樣讓它變成常態分佈呢？大家想到一個簡單的解決方案：既然我希望它是常態分佈，我就「要求」它是常態分佈，結束。這就是改進版的 Autoencoder，也就是 VAE。

[33:11] 順便說一下，VAE 雖然是很簡單的模型，但在圖像生成裡佔了非常重要的地位。重要到很多人在使用 Diffusion Model 的時候，都沒有發現 VAE 其實是一個很關鍵的點。 VAE 的做法是，我希望中間的 Latent Vector (z1, z2...zk) 每一個都符合一個常態分佈。我們只要決定兩個數字：平均值和標準差，就可以決定這個常態分佈。

[34:11] 那我怎麼知道它是符合什麼常態分佈？沒關係，我們在前一層先學。比方說 128 維，就在前一層學 128 個平均值和 128 個標準差。學完以後，就用這個平均值跟標準差去抽樣出一個數字來。重點來了，所以中間這個向量其實是從那個常態分佈裡面抽樣出來的。即使是同一張圖進來，它的特徵代表向量其實是有可能不太一樣，因為它是抽樣出來的。

[35:19] 但是因為我們這樣訓練（目標是要還原照片），所以它會想辦法在還原的時候都還原出一漾的照片。意思是說，如果只變一點點的時候，它會知道不應該變太多。所以答案很簡單，既然希望它是常態分佈，我們就要求它變成常態分佈。一張照片進來，它會學到每個位置的平均值和標準差，然後從這邊抽樣，產生一張很適合的照片。在這樣的情形下，你真的動一點點的時候，它就真的只會變一點點。這就是 VAE。

[36:18] VAE 最有名的應用，雖然是不太好的應用，就是 Deepfake (深偽技術)。Deepfake 就是把你的臉貼到那個影像上面去。這怎麼做的？原理也很簡單。我們用兩套 VAE。一套是學 A 這個人 (Encoder A, Decoder A)；另一套學 B 這個人 (Encoder B, Decoder B)。如果用 Encoder A 把 A 的照片放進去，Decoder A 就會把這個人的照片還原回來。

[38:03] 那到底怎麼樣做到換臉？其實影片只是很多張照片。我們把有 A 的照片放到 Encoder A 找到 Embedding。但是我們很奇怪的是，不交給 A 的 Decoder，我們交給 B 的 Decoder。但 B 根本就不會畫 A，它只會畫 B，因為它只學過 B 要怎麼還原。所以我們送到 Decoder B，它生出來的人就變成只有 B 這個人在裡面了，所以就把 A 這個人換成 B 這個人。這是 Deepfake 早期用的技術。

[39:04] 一開始的時候大家覺得 Autoencoder (不管是 VAE 還是 AE) 好像沒有想像中那麼好。第一個是品質好像沒有真的很精細；第二個是它的變化真的沒有很大。因為要符合常態分佈，所以變化不會很大。最有名的例子是用手寫辨識數字去訓練，看能不能寫出沒出現過的數字。結果發現最多就是稍微歪一點，變化好像沒有很大。所以大家覺得 Autoencoder 不太適合做生成模型，生成對抗網路 (GAN) 一度獨大。

[40:31] 後來發現一件事情，跟大型模型有點像。大型模型原理很簡單，就是預測下一個字。只看一本書的時候好像有點意思但沒什麼用，但看了非常多文字後突然變得很厲害。所以大家就想，如果 VAE 這種形式的 Autoencoder，如果讓它看真的很多照片，大量的照片，會不會也可以生出相當好的東西？事實上後來發生的事情就是這樣。

[41:30] 所以我們就要來介紹現在開始的一個主角叫做 Diffusion Models (擴散模型)。
[00:41:30] 我們就要來介紹現在開始的一個主角，叫做「擴散模型 (Diffusion Models)」。我們把話說在前面，Diffusion Models 從上一次開課（上學期）到這學期又發生了很多變化，世界又發生很多變動。變動到一度我們都覺得是不是應該不要教 Diffusion Models 了。但是因為又有新的變化，反正這個世界變化實在太多了，我們發現 Diffusion Models 雖然可能最後我們應用的方式不是現在介紹的方式，但它可能還是一個重要的模型，是我們學生程式 AI 需要知道的東西。[00:42:25] 總而言之，先記得一件事情：我們現在介紹 Diffusion Models 就是讓它出去生圖的。雖然現在最主流的模型可能不完全是單純的 Diffusion Models，但我後面會解釋，Diffusion Models 有可能用在其他奇特的地方。所以不管怎麼樣，Diffusion Models 還是一個重要的模型。[00:42:55] 屬於 Diffusion Models 的時期，特別是在 2022 年差不多 ChatGPT 起來之後，大家發現有另外一個系統，就是生成圖像的生成式 AI。比較有名的就是 DALL-E，然後還有 Stable Diffusion。Stable Diffusion 的特色就是它是開源型的，如果你的電腦夠好，可以裝在自己的電腦上整天在那邊生圖。還有一個就是收費型的 Midjourney。大概最有名的就是這幾個。那時候 2022 年吹起了電腦創作的風氣。[00:43:37] 它基本要做的事情，就是只要輸入了一段文字，而且你真的可以很天馬行空。比如說「一隻兔子穿著帶著兔耳朵的帽子」，這聽起來怪怪的，它已經是兔子了幹嘛要帶兔耳朵的帽子？但總而言之它就可以幫你生出來。這是 Midjourney 生出來的，生出來的還相當好，所以大家就真的嚇到了。這跟以前用 GAN 做的控制，其實沒辦法真的控制那麼精確。以前是你跟它下一段文字，它就幫你把那段文字代表的圖生出來。[00:44:19] 漸漸的，開始的時候生的圖真的都比較噁心一點。現在其實還是會喔，最有名的就是一個人常常會出現六隻手指頭，然後手腳都怪怪的，有時候會斷手斷腳之類的，有時候生出來的圖真的有夠噁心有夠醜。到了 2025 年初的時候，其實就是今年初，要強調年初，因為現在已經接近年末，世界又有點不一樣。你會發現每一個都生得相當好了，不管是 Midjourney、Stable Diffusion 的 XL 版本，或是 Bing Create（通常是用 DALL-E 的系統），說真的做得比較好。[00:45:25] 後面的原理就是有一種叫做 Diffusion Model。比較起來，我覺得 Diffusion Model 跟前面的 ChatGPT 那種大型語言模型相比，Diffusion Model 其實更像是「橫空出世」的感覺。因為 GPT 那種模型我們很早就已經這樣用了，只是以前的效果還沒有那麼好，但 Diffusion Model 真的比較像橫空出世。所以大家有了 Diffusion Model 就開始畫圖了。[00:45:51] 現在有些收費型的 Diffusion Model 還是很重要，因為 Midjourney 現在還存在著，他們也一直在改版，還是有它的好處，唯一的壞處就是它一定要收費，沒有免費試用的版本。所以大概現在現存的 Diffusion Model base 最好的可能就是 Midjourney。[00:46:25] 我們今天要做的練習，其實應該現在都不是 Diffusion Model 的圖像生成，但好處是因為現在理解中文的能力比較強，基本上你都用中文去創造圖、打造你的作品就好。下一次我們會介紹有一些，特別是 Stable Diffusion 中文能力其實一直沒有很好，雖然也有人改過，但基本上就是讓 AI 自己把它翻成英文。不然既然要這樣做，我們之後會教大家，不如就直接放入你最信任的大型語言模型，請它翻譯就好了。[00:47:42] 我們今天先來做的練習是用 Microsoft 的 Bing Create。我一直在猶豫的原因是因為，它其實嚴格來說已經不是純粹的 Stable Diffusion Model，已經不是我們這幾週要介紹的擴散模型。因為我已經找不太到真的可以免費、效果又真的還不錯的 Diffusion Model 給大家使用。所以我們稍微退而求其次，就是用一個稍微接近一點點的，先讓大家感受文字生圖的世界大概長什麼樣子。再強調一次，它可能事實上已經不是 Diffusion Model 了，但是我們先來用這個 Microsoft Create，至少它可以免費使用。[00:48:56] 今天的作業是這樣：請大家用 Microsoft Bing Create（大家用一樣的比較不會有落差）。裡面應該是用 DALL-E 3（OpenAI 出的）。雖然 DALL-E 2 的時候還是純粹的擴散模型版本，DALL-E 3 已經不是了，但我們就假裝是好了，其實我們也沒有真的要假裝，我們現在只是使用一下文字生圖的 AI。[00:50:22] 為什麼推薦 Microsoft Bing 的原因就是因為它可以免費使用。雖然你說 ChatGPT 也可以免費生圖，但 Microsoft 的額度稍微好一點。比方說現在完全可以用中文形容：「五位台灣的大學生在咖啡店裡用筆電討論東西的照片」，它就會生成那個咖啡店的樣子，這次還不錯，至少沒有五個人有十杯咖啡在桌上。[00:51:11] 我們要試驗的是：請你去想辦法找一個「風格」出來。你要想辦法找一個風格，當然這個風格一定要你喜歡的，然後這個風格不要是大家已經在用的，比方說 Pixiv 風格。最好是你看到一個圖像的風格、插畫的風格，你很喜歡那種風格，你想辦法看能不能做得出來。[00:52:16] 以前的作業是針對 DALL-E 2，那時候畫出來有點不一樣。現在我們請大家做的，比方說風格是「比較奇幻型的水彩畫風格」，然後後面描述要畫什麼內容，它就會畫出一個內容來。Microsoft Bing Create 的好處是它都會生成四張圖，你可以找其中你比較喜歡的留下來。[00:53:07] 再舉個例子：「黏土動畫」。就是用黏土去做成動畫，大家就是手捏的造型。當然我覺得它這一次的黏土動畫做得沒有那麼像黏土做出來的東西。比如說「一隻熊貓戴著眼鏡坐在沙發上用他的 MacBook」。我們要去找一個風格，最好是那個風格稍微再複雜一點點。再強調一次，你一定要喜歡這個風格。如果你真的找到了這個風格，其實你以後不要亂跳。以後就可以固定這個風格，你的投影片就可以都用這樣的風格，整個投影片風格就會非常一致。[00:54:36] 所以今天的作業其實就是做這件事情，應該是一個還蠻好玩的作業。你可以去 Google 搜尋人家都用了什麼風格，也可以自己去想辦法找到那個風格。像這個 Pixar 的風格，我覺得也不是這麼像 Pixar 風格，管它，總而言之你可能想像中它應該是很像那個風格，但畫出來不是，可是重點不是說你真的要找到非常符合你本來想像的那個風格。如果你嘗試了很多方式，最後雖然不像原本想的，可是畫出來你覺得還蠻喜歡的，你就可以固定那個風格當成作業交出來。[00:55:35] 還有一種是「向量型繪圖」。雖然畫出來不是真的向量圖，它還是點陣圖，但是因為向量型繪圖通常筆觸比較簡單。比如說要 minimalist (極簡風格)，細節少一點點。反正這個都是你去想辦法嘗試形容你想要的風格。如果畫出來以後覺得還不錯，讓它多試幾張，發現真的可以用這樣的指令畫出來風格很穩定的圖，這就達成了我們作業的目標。[00:57:29] 還有一個很有趣的事情，就是你可以指定一個藝術家。畫得太像反而會有一點點版權上的問題，雖然大家也在爭論風格到底可不可以有著作權。但有一個小技巧，你可以打入這個藝術家，但是你用很多其他的形容詞。因為你去查這位藝術家，其實他畫出來的風格根本不像這張圖的樣子，所以你可以再加很多的形容。甚至從 Midjourney 那邊學到了一個小技巧：我們可以放很多個藝術家的風格，所以它不會單一的像某一個藝術家。你把它混合起來，看 AI 怎麼理解，有時候會畫出來你覺得真的還蠻特別、蠻有趣，而且可能是獨一無二的風格。[00:58:59] 所以今天的作業就是請大家使用 Bing Create，麻煩大家盡量都選 DALL-E 3，不要用預設的 GPT-4。找到一個喜歡的風格，決定了這個風格後，再產生幾張不同主題的圖。最後證明說你這個風格真的穩定的找到了，所以它真的會畫出來看起來就是同樣的風格。這個風格是你喜歡的，不要在外面隨便抄兩個字然後就交作業。希望大家多去找，也可以寫清楚一點你是怎麼找到的。[01:01:31] 如果我們要用真的是 Stable Diffusion 的話，我們上一個學期其實還有教 diffusers 這個套件，就是真的用 code 跑 Diffusion Model。但是這個學期可能不教大家這件事情了，我們不再教大家寫程式，所以很怕寫程式的同學就沒有寫程式這個動作了。[01:02:44] 在外面，直到今年（2025）年初的時候，大家非常喜歡用一個叫做 Automatic1111 的 WebUI。這是一個把 Stable Diffusion 打包好的一個介面，用的是 Gradio。它的功能很多，幾乎所有 Stable Diffusion 上面大家會做的事情它上面全部都有。所以很多人是用 Automatic1111。但是問題是它比較大，要裝在 Colab 上面現在有非常嚴重的問題。因為 Google 發現太多人拿 Colab 去生圖了，所以決定要把 Automatic1111 這件事情擋掉。[01:03:59] 所以為了讓大家還是可以在 Colab 上生圖（這是後面的課程），我們會用一個其實也是非常簡單的，特別是它很接近 Midjourney 使用體驗的介面，叫做 Fooocus。這個非常簡單使用，如果你自己的電腦上有適當的 GPU，要自己安裝其實也算是容易。我們後面會介紹這一個，讓大家實際使用 Fooocus，也會教大家外面常用的一些技巧。[01:04:53] 我們現在要介紹的主角就是 Diffusion Models。其實如果我們懂 VAE (Variational Autoencoder) 的話，Diffusion Models 就非常容易懂。因為它基本上在訓練的時候，它就像是一個 Autoencoder，或者更嚴格說就是像 VAE 那種東西。[01:05:21] 也就是說，它訓練的時候就是輸入一張圖，輸出就是一張圖。只是中間的 Latent Vector 我們需要做一些特別的處理。所謂特別的處理，也就是希望中間那一個 Latent Vector 是我們可以很容易自己生出來的。[01:05:46] 因為前面介紹的 Autoencoder 或 VAE，如果我想要生成一隻貓的照片，我可能真的要去找一隻貓的照片放進去，找到它的 Embedding，稍微修改一下，再還原。但如果我今天就是要畫一個天馬行空的東西，我到底要從哪邊找到適合的照片呢？那可不可能就是我可以很容易生成那個 Latent Vector，比方說它就是符合某一個我們知道的常態分佈？這樣我就能輸入天馬行空的想法，輸出就會是一張不錯的圖。所以 Diffusion Model 基本的想法就是這樣，整個架構看起來還是一個 Autoencoder 的樣子。[01:07:51] 比較起來，我會說 Diffusion Model 比較像是橫空出世，但說它是真的橫空出世其實也不太對，因為 2015 年的時候就有 Diffusion Model（擴散模型）。擴散模型本來就是講物理上的擴散，像把方糖丟到水裡讓它擴散。但在用在生圖這件事情上，2015 年就有用在生圖上面。所以它也十年了。[01:08:34] 但是在 2015 年出來的那篇擴散模型文章，還沒有引發太多的討論。原因是：第一，它生出來的圖沒有真的很厲害；第二，它的計算非常多步，生成速度沒有很快。所以大家沒有太理它。[01:09:16] 到了 2021 年，OpenAI 寫了一篇文章，非常白話地說：Diffusion Model 在圖像生成可以比 GAN 還要厲害。所以就是這篇文章引發了蓬勃的發展，讓 Diffusion Model 幾乎完全取代了 GAN 的地位。[01:10:04] Diffusion Model 基本上就是一個很呆的代工機器人，輸入什麼輸出就是什麼。但是大型模型告訴我們，如果讓它看比較多的東西，也許真的會有一些很神奇的變化。所以 Diffusion Model 基本上就是做一個 Autoencoder，只是要讓它看很多很多的圖。[01:10:50] 它跟以前的 Autoencoder 有一點點不一樣。以前的 Autoencoder 的 Encoder 跟 Decoder 都是學出來的。大家突然發現說，我要做 Embedding 的時候，我可不可以要求中間的 Latent Vector 基本上就是符合某一個常態分佈，例如標準常態分佈？[01:11:36] 我要怎麼樣做到標準常態分佈呢？大家突然發現：我可以每一次加一點點 Noise (雜訊) 下去。當然不能一次加太多，一次加太多你會發現變得跟原來的圖一點關係都沒有。所以每一次加一點 Noise，一直加很多很多步，最後它就會很接近我們希望有的常態分佈。[01:12:01] 所以前面基本上算出來的（所謂算出來就是我們自己把 Noise 加上去），一次加一點、一次加一點。加到最後，中間那就是完全符合我們想要的常態分佈。既然我們知道它是符合某一個常態分佈（例如標準常態分佈），所以我以後只要給一個標準常態分佈，它應該就會對應出一張很像樣的圖出來，特別是我們讓它看了非常多圖之後。[01:12:39] 所以在這個 Encoder 的部分，就是算出來的，加上了高斯雜訊 (Gaussian Noise)。比方說這是一張鳥（台灣特有種小彎嘴畫眉），我們就開始加一點點 Noise，一直加加加。你看到後面的時候，你會發現真的變得很亂了，完全看起來就像雜訊的雜訊了。[01:13:03] 為什麼需要做到後面真的看起來很像雜訊呢？原因是因為，比方說第 50 步那邊，其實我們加 Noise 沒有加那麼快，真正加的時候大概要加到 1000 步的時候，我們才可以看到像現在顯示第 150 步的那樣。你會發現第 50 步如果直接要生出這樣的圖還是有點困難，因為它基本上還要有個鳥樣子；要生出第 100 步的時候也是有點困難。但我生出第 150 步那個完全就是雜訊的樣子，那個就簡單了，我只要符合那個常態分佈，我就可以抽樣出這種東西來。[01:14:15] 所以每一個步驟，我們的 $x_0$ 就是原來的圖，清晰的圖。然後我們會一步一步的加這個 Noise 上去。公式基本上就是加 Noise 上去。這個感覺上就像擴散的樣子：第一個步驟（比方說螺旋狀的圖）加個 Noise，它就開始變得有一點點擴散，再加一點又擴散一點，最後就擴散出來，真的像每一個點都符合常態分佈的樣子。所以 Encoder 這個部分的概念就是這樣，Encoder 是算出來的。[01:15:23] 但是我們剛說過要加得夠多步才可以讓它夠亂。那這個 Encoder 的部分是不是真的需要做 1000 步？答案是當然可以不用。我們會有一個公式，你要加到第 1000 步，我可以直接從第 0 步計算一下就可以跳到第 1000 步。這個算得出來。所以在 Encoder 部分，我們可以一次就算到我們要加 1000 步的樣子。[01:16:15] 接下來是 Decoder 的部分。Decoder 的目標當然是：我看到這個雜訊，我就要還原這個圖。因為是 Autoencoder 嘛，我應該就要還原出這一隻小彎嘴畫眉出來。但是相信大家看了以後都會知道，這對我們的 AI 真的太殘忍了。它看到左邊的那一個雜訊，就要知道右邊原來圖長這樣，真的太殘忍了。[01:17:07] 大家就發現說，可能我們不能直接學這件事情。直接輸入一個加好 Noise 的樣子，然後目標是還原原來的圖，直接讓模型去學這件事情可能真的會有困難度。所以我們到底要它去學什麼呢？[01:17:40] 為什麼 Decoder 不能直接算回來？因為在加噪的過程中其實有加入隨機性，第一件事是它不太容易算出來；第二件事是就像回歸一樣，我們看到的是加上 Noise 的樣子，要回歸成原本漂亮的線，其實是要稍微計算的，因為我們不知道加了多少 Noise。如果要從完全亂的雜訊還原成原本的圖，這個困難度很大。[01:18:40] 照理說，我們應該要做的事情就是讓它還原「一步」就好了。就是從 $x_t$ 到 $x_{t-1}$ 還原一步就好了。那還原一步可能簡單一點點。但大家又突然發現，這還是有點殘忍。要從左邊變成右邊（上一步），到底差什麼東西？[01:19:23] 所以大家突然發現說，會不會去學 Noise，就是從 $x_{t-1}$ 到 $x_t$ 這一步的時候到底加了多少 Noise，會比較簡單？答案是會。所以我就去學只學 Noise 的部分。這對我們人來看好像有點過分，但是至少你知道右邊的是符合某一個常態分佈的，在學的時候你已經知道它是常態分佈的；那左邊的還不是。所以在這個時候去學 Noise 這件事情感覺比較合理，比較簡單一點點。[01:20:11] 如果我們真的把這個 Noise 學出來了，全部的 Noise 學出來了，那我們當然就可以算出來說這個到底差了多少，那我們就可以算出來 $x_0$ 是多少。大概就是這樣子。所以我們做的事情就是這樣。[01:20:27] 今天在訓練這個模型的時候，它其實基本上就是隨便從某一個步驟（比方說 150 步），它就會去預測說從 0 到 150 步它到底加了多少 Noise。所以理論上它也可以直接從 150 步就直接跳到 $x_0$ 還原。[01:21:24] 雖然理論上可以還原，但是如果我真的去學了這件事情，原則上我就可以還原它了。所以我們今天如果要生圖的話，我們就可以做這樣的事情：我今天先隨機的去生出一張 Latent Tensor 出來，因為它是符合常態分佈的，所以要生這件事情非常的簡單。那我們這個 Decoder 就專門去學它的 Noise 加了多少。當 Noise 真的學出來以後，我就把這個隨機生出來的 Latent Tensor 減掉這個 Noise，然後我們就會還原出它的圖出來。這就是它整個生圖的原理。[01:22:11] 只是說，雖然我們訓練的時候的確是這樣訓練（預測總共的 Noise），但是一次就生（從第 1000 步直接跳回第 0 步）雖然理論上可以，但結果通常都是超級噁心。可以做，但超噁心。所以我們現在的做法是：想辦法雖然會預測總共的 Noise 多少，但是在太遠的地方預測通常不是很準，只能預測出大概來。[01:23:25] 所以我們就用這個預測先做，先進個 10 步。然後進了 10 步之後，我們再預測一次，再進個 10 步。這樣慢慢地把它做到最後，也就是慢慢地穿回我們的圖出來。所以我們的重點就是在 Decoder 的時候，我們要學的就是放入了一個加噪之後（不管是多少步）的結果，然後要預測它的 Noise，預測到底加了多少才會變成這副德性。[01:23:53] 所以我們在還原的過程中間，就可以開始慢慢去雜訊 (Denoise)。雖然在第一張最左邊出來的時候，我其實估計的就是估計到最後要加多少、減掉多少雜訊才會變成那個原來的圖。可是因為太遠的地方，它去預測的時候通常比較不準，所以我們在整個進程的過程中間，其實我們會慢慢的進。如果最精確的當然就是每次真的只進一步，但是那個會做時間太久，所以我們通常也不會做這麼噁心殘忍的事情，所以我們就慢慢的進一點，最後就會達成我們要的東西。[01:24:32] 好，這 Diffusion Model 不知道大家有沒有什麼問題？
[01:24:32] Diffusion Model 不知道大家有沒有什麼問題？再說一次，Diffusion Model 其實它基本上就是 Autoencoder，更明確說它應該就是某種 VAE。但是它今天在處理中間這個 Embedding 或是 Latent Tensor 的時候，它是不斷經過加噪 (Add Noise) 上去的。所以你會發現有一件特點，就是它整個過程當中跟傳統的 VAE 或是 Autoencoder 又不一樣，它的維度並沒有變喔。[01:25:09] 它本來是多少維的，進來就是多少。因為它就是原來的照片的大小，因為它就是不斷加噪而已，它其實就是原來照片的大小。所以它的大小完全沒變，如果是 256x256x3 的時候，它還是 256x256x3，它完全沒有變。所以它就是加噪的結果，大小還是一樣的。[01:25:37] 然後還原的時候就是去學它的 Noise。我們的 Decoder 其實去學的是它的 Noise，就是加了多少的 Noise 它才會變成那個這麼亂的樣子。所以就想辦法慢慢的減、慢慢的減，慢慢的 Denoise (去噪)。所以它的 Encoder 的過程，我們通常把它叫做「加噪的過程 (Forward Process)」；Decoder 的過程叫做「去噪的過程 (Reverse Process)」。然後 Encoder 是算出來的，Decoder 是我們的 AI 模型學出來的。這樣可以嗎？[01:26:06] 大致的情境就是這樣子。然後開始的時候為什麼大家沒有那麼重視？因為剛剛提到了一個很重大的重點：它的大小其實是一樣大的。這以前沒有發生過。我們做了一個很像 Autoencoder 的東西，我們本來 Autoencoder 就是想要找到它的特徵代表向量，通常要維度比較小嘛。但它本來的圖多大，生出來就多大。[01:26:35] 那比方說 512x512 的圖，中間的 Latent Tensor 就真的是 512x512x3 (RGB 三原色)。然後它要在還原的每一個步驟都是這麼大，所以計算量很大，大家也發現這個真的不太好用。[01:26:55] 所以怎麼辦呢？所以在最近有一些改良，事實上基本上包括 Stable Diffusion 等等的 Diffusion Model 應該都是用這樣的方法做。這種方法做叫做 Latent 版的 Diffusion Model，也叫做 LDM (Latent Diffusion Models)。這就是 LDM 原始的 Paper，Stable Diffusion 基本上就是照著這個概念去把它做出來的。[01:27:26] 那 LDM 是做什麼呢？剛剛我們說過 Diffusion Model 的幾個問題當中，一個問題就是要去噪，所以它要一步一步去噪。也不是真的一步一步啦，我們當然有各種加速的方法，我們把它叫做排程器 (Scheduler) 或是 Sampler。就是在中間我們可以跳幾步的一些技巧。所有的 Diffusion Model 在使用的時候，我們基本上都用了一些技巧，讓它不用像我們加噪假設是加 1000 步，還原的時候其實沒有真的算 1000 步，不然大家就會瘋掉。你要生一張圖，好不容易做完了一步，然後說還有 999 步，你真的會瘋掉。所以它都有一些技巧讓它可以比較快的就把那張圖生出來。所以有很多人在研究哪一種排程器是比較適合的。[01:28:21] 但是它還是一個問題，因為從頭到尾它的大小是一樣的。比方說最標準的 Stable Diffusion 一開始最標準生出來的圖的大小就是 512x512。那你從頭到尾就算加快的一些步驟，你只做 20 步好了，那你每一次的動作都還是 512x512x3 這麼多的數據在那邊做計算。所以大家就發現說這個速度真的會很緩慢。[01:28:50] 然後呢，就想到說：那為什麼我們不要讓它縮小一點點呢？所以大家就想說好，我們就來縮小。縮小的方法呢，就找了以前我們熟悉的、可愛的 VAE (Variational Autoencoder) 來。[01:29:09] VAE 做的事情就是：我今天有一張圖，然後中間那個 Latent 該長什麼樣就長什麼樣子。我就把這張圖找到它的特徵代表向量，就縮小這張圖，然後它就可以去還原這個原來的這張圖。這就是 VAE 本來做的嘛。[01:29:33] 所以 Latent 版的 Diffusion Model 就是 LDM，它做的很有趣的事情就是：它讓 Diffusion Model 去學的並不是在生真正的圖，它去學的所有圖都是我們用 VAE 訓練好的 Encoder 之後的那個結果（Embedding 或 Latent Tensor）。然後它拿去做訓練。也就是說，它會生出來的圖是我們也看不懂的圖，人類是看不懂的圖。真正它去用 Diffusion Model 去學生成的圖，其實是人類看不懂的圖，它就只是生出來這個中間的 Latent Tensor 的部分。[01:30:26] 在標準的 Stable Diffusion 是這樣，它本來是 512x512 的圖，它就把它縮成 64x64 的圖。就是這樣子。然後我們希望它還是圖的樣子，它就圖的樣子嘛。所以在最後，所以我以後只要找到中間的這一個 Latent Tensor（事實上是 64x64x3），我就可以放進去，就可以還原成 512x512 的圖。[01:30:56] 我要再說一次，這個中間的 Latent Tensor 我們拿出來看當然看不懂，不知道是什麼圖，完全看不懂。但是我們的 Diffusion Model 反正不計較這個，它看不懂它也會學。所以它最後這個模型很有趣的是，它生出來的就只是中間的 Latent Vector/Tensor，然後我們需要再靠我們的 VAE 的 Decoder 還原，我們才會變成那個人類看得懂的圖。這樣可以嗎？[01:31:29] 但是因為它拿去訓練 Diffusion Model 的那邊真的小了很多，所以在這樣的情境之下就會比較容易訓練，整個的計算也會比較快。因為主要要算的時候都是在中間那個 64x64x3 的那個大小一直在那邊做計算。[01:31:53] 所以我們這個學會了。至少現在當然因為老實說世界有點改變，不過我們在開始的時候，就是在外面 Diffusion Model 在流行的時候，你會發現很多人都會用一些你聽不懂它是什麼的技巧。其中一個就是因為剛剛 VAE 就是它真正的生圖其實是用 VAE 生圖的，所以我們也有機會去把 VAE 改得讓它生圖的能力更好一點點，例如更精細一點點。我們有可能去訓練一個更好的 VAE，是不是有可能？當然是有可能的。[01:32:35] 所以你會發現說在 Stable Diffusion 這種 Diffusion Model 裡面，它還有一個技巧：就是你可以換最後要生圖的 VAE。然後你就會發現有一些模型如果用那個 VAE 生圖的話，它會生得比較好。所以外面有一些人就會說，那我們可以去用 VAE 這個比較外行的話。因為所有的 Latent Diffusion / Stable Diffusion 它一定要有 VAE 啊，只是你要用標準的 VAE，還是你要用那個修改過、改良版的，或是特別符合某些模型的 VAE。[01:33:20] 所以 Stable Diffusion 比較常見的調整是三種 VAE：一個是預設版的，一個 EMA，一個 MSE。總而言之它有訓練不同的 VAE，各由不同的效果。那有一些模型它也會自己訓練自己的 VAE，讓它那個生成的效果更好一點點。不管怎麼樣，Stable Diffusion 這種 LDM 版本是一定要有 VAE 去做的。[01:34:01] 剛剛說的就是，中間的 Latent Tensor 因為很容易生成，因為它就是完全符合某一個常態分佈的，所以那個完全是我們自己可以生的東西，很容易生成。但是呢，我們就會有一個疑問：什麼叫「文生圖」？我們明明現在做的事情是要把一個文字放進去之後，讓它把圖生出來。這件事情到底是怎麼做到的？我到底怎麼告訴這個我們可愛的 Diffusion Model 說這張圖到底要生什麼樣的圖？[01:34:46] 我再說一次，剛剛我們訓練完以後，我們可以想見，今天所有的 Diffusion Model 只要看過很多的圖，所以以後只要找到那個從某個常態分佈抽樣出來那些點，然後它就會生出一個對應的圖。因為它看過真的很多圖，所以它生出來的圖應該都是品質還不錯的。[01:35:13] 可是現在的問題不是這個，現在的問題是說，那我們今天要把某一個意思（Prompt）放進去。我們剛剛前面有看過例子，就是我們要下 Prompt 的時候，那個 Prompt 到底怎麼告訴它的？[01:35:32] 基本上也蠻簡單的。就是說，前面那一個很像 Noise 的，它是完全符合例如標準常態分佈的資訊，完全我們可以抽樣出來，因為那個真的是隨機的。那就可以想像就是我們的天馬行空的一些想法、創意的想法。然後後面呢，就是我們把代表文字意思的 Embedding 找出來，把那個意思找出來，然後把這個文字意思的 Embedding 把它加進去。[01:36:22] 這裡不一定是真的「加」，通常直接加的效果不好。這個超過了我們課程的範圍所以不說。不就是把它混進去。混進去有一個技巧，就是我們前面的 Transformer 的時候做過的 QKV (Query, Key, Value)。就是我用 Transformer 的技巧把它混進去。今天的 Q 就是我們那個隨機生成的 Latent Tensor，然後我們再把它混到我們前面那個要生出來的那個字義（文字 Embedding）的部分。[01:37:06] 所以整個的樣貌是這樣子：有經過一個叫做 CLIP 的模型。這 CLIP 的模型是找到這個文字的 Embedding，可以嗎？這個模型其實如果我們用以前大型模型想的話，我也可以用大型模型裡面，比方說我們在 NLP 裡面用過的這個 Embedding 的那個模型去做。總而言之，這個東西其實就是我們文字的 Embedding。[01:37:46] CLIP 後面我們會再介紹一次。它現在基本上訓練得很巧妙的地方是，因為我們就是要文生圖，那我們就想說，我們要理解一個概念的時候，我也可以看到一張圖。比方說這張圖是有幾位同學坐在咖啡店裡面拿著筆電來討論東西。那我可不可能把這段文字的 Embedding 跟這張圖的 Embedding 做得很像、拉得很近？也就是說我代表同樣的意念。如果放到這張圖裡面去，就代表這張圖的意念；如果是文字的話，我就把這段文字放到裡面，就當做這段文字的意念。[01:38:42] 所以稍微做一個比較總結：CLIP 就是做文字的 Embedding。然後呢，我們就想辦法把它加進去。因為我們剛剛說過了，它真的去做那個 Diffusion Model 的時候，它真的去預測它加了多少雜訊的時候，它事實上是經過 VAE 縮小的這張圖（Latent），然後去做這件事情。然後做完了以後，它會預測出雜訊，然後它就會把原來的圖生出來。但是那個圖其實我們對我們來說看起來還是很像雜訊啦，所以我們要經過 VAE 的 Decoder，它才會生出原來的圖。這就是完整的 Stable Diffusion 或是這個 Latent 版的 Diffusion Model 的架構。[01:39:26] 那我們最後說一句話，我們就可以結束今天的（理論部分）。就是我們在剛剛說我們把我們的意思加進去的時候，事實上是我們每一次在還原的過程中間，都耳提面命的告訴它：「我要畫這張圖，你要記得」。然後再下一步又告訴它我要畫這張圖，再下一步又告訴它。反正每一步都有耳提面命。真的有點囉唆啦，反正總而言之就這樣慢慢還原的時候，才會確保說它真的有還原成功。[01:40:02] 那這邊就會有一些小的技巧，我們這個之後再來介紹好了。這個就是我們今天要為大家介紹的 Diffusion Model。那事實上至少在直到 2025 年初的時候，它還是那個文生圖最紅的一個 Model。那我們後面的課程還會為大家介紹實際上使用的時候還有什麼相關的技巧，然後再來就是到底後來又發生了什麼樣的變化，為什麼我說了剛剛那些話。[01:40:30] 好，我們先休息十分鐘，我們等下再回來進行我們今天的 TA 時間。[01:40:35] （課堂公告）趁這位同學還在處理設備問題，我們先來公告一下。就是政大的點名表單已經放在置頂連結了，請同學趕快去點名。然後還有明天助教的 Office Hour 要取消，就是正課表下午 12 點到 2 點取消，請同學注意。然後同學們可以開始想自己期末專案要做什麼。主題不限，那就是老師上課教到的東西都可以用在期末專案。然後我們之後會再發公告。[01:42:09] （設備測試與調整聲音，略去重複測試語句）這樣可以嗎？這樣有嗎？我先有看到文字嗎？有。你那個電腦的聲音、電腦的投影片先放出來。有聲音嗎？沒有。我聽到那邊的... 手機看起來是有開的。好，那我們有請台灣大學的劉伯賢同學為我們帶來他的閃電秀。[01:45:07] （學生閃電秀：劉伯賢）那大家好，我是劉伯賢。那我們今天跟大大家介紹的是在自然語言處理 (NLP) 裡面是 Adversarial Attack (對抗式攻擊)。然後本場的重點會聚焦在 Evasion Attack (閃避攻擊) 的攻擊跟防禦上面。[01:45:31] 我們今天主要就是先理解什麼是 Evasion Attack，然後再去看說它的四個組成構件，然後最後去講它的防禦思路。所謂的 Evasion Attack 就是在不改變人類對語言的語意理解之下，然後對輸入做一個很小的修改，卻能夠讓模型做出錯誤的或者是攻擊者指定的輸出。那和語音、影像不同的是，文字它是一個離散的符號，所以在擾動的部分多半是用替換、插入、刪除或者形態變化來去做這個攻擊，而不是說像語音影像一樣是加入一個連續的高斯雜訊。[01:46:20] 在 Adversarial Attack 關鍵的四個元素就是：Goal (目標)：你想達成什麼目標。Transformation (變形)：你有大些可行的文字修改。Constraint (限制)：你要怎麼樣去確保它的可讀性、合理性或者語法的正確性。Search Method (搜尋方法)：就是在限制下怎麼樣去找到真正能夠翻轉預測的這些擾動。[01:46:54] 在 Goal 的部分，攻擊的目標主要可以分成三類：Untargeted：只要把原本正確的分類打掉，那就算成功。Targeted：把原本屬於 A 類的樣本，誘導成指定的某一類型，像是 B 類。Universal 或是 Task Specific：例如說是一段通用的觸發詞，只要附在各種輸入的後面，模型就會傾向於特定的輸出。[01:47:38] 在可行的修改 (Transformation) 部分，大致上可以分成兩個層面：第一種是 Word (詞語) 的方面：首先你可以做的是同義詞的替換。或者是在每一個詞語的 Embedding 裡面，利用 Counter-fitting 的方法把同意義的拉近，然後再去畫出一個可能半徑為 $\epsilon$ 的一個球體，然後在裡面去找一些同義的詞。或者是用 BERT 在 Pre-train 的時候，它可能用 Masked Language Model，去選擇那些高機率的候選詞作為替換或插入。或者是說用詞性的變化，然後去保持核心的語意。或者是用梯度 (Gradient) 的導向，去做一階的近似，然後來找出 Loss 上升的這些替代詞。也可以刪掉低影響的詞。第二種是在 Character (字元) 等級：可以用交換、替換、刪除、插入之類的。像是下面舉的例子，你可以把 T 交換一下，或者是替換掉某一個字母。[01:49:00] 在 Constraint (限制) 的部分，就是我們希望對抗的這樣本對人類來講依然是自然的、可讀的。所以我們會加上這些限制：重疊度和距離的限制：限制編輯距離或者是最大的修改比例。語法的正確性：也要保持詞性的一致，減少偵測的文法錯誤。流暢度的部分：可以用語言模型的 Perplexity 去檢查它是否還是自然的。語意的保持：要求詞向量或者是句向量的相似度必須要是夠高，或者是用 Universal Sentence Encoder 去檢查句向量的相似度，確保語意不會跑掉。[01:49:44] 最後是 Search Method (搜尋) 的部分：最直覺的就是用 Greedy Search 的方式：對每個位置、每個候選的詞語去打分，然後由高到低去替換，直到翻轉你原本的預測。加入 Word Importance Ranking：選取估計詞的重要度。例如用 Leave-one-out 的方式，看拿掉某一個詞了之後機率下降多少；或者是用梯度的大小來排，然後優先去改掉這些關鍵的詞。用 Genetic Algorithm (基因演算法)：把模型信心當適應度，然後透過交配、突變這些方式，在離散空間之中做演化，直到成功或者是達到查詢的上線。[01:50:33] 在 Defense (防禦) 的部分也是分成兩個部分：Training 的部分跟 Inference 的部分。在 Training 的時候：我們可以在 Embedding 的半徑 $\epsilon$ 裡面去找出它最壞的擾動，然後讓它跟乾淨的樣本一起去訓練 (Adversarial Training)。或者是 Synonym Encoding，把看成是同義集合的凸集合 (Convex Set)，然後在凸集合裡面去尋找最壞的 Embedding。或者先用比較不 Robust 的模型去產生這些對抗樣本，然後再去重新的訓練，跟乾淨的樣本混合訓練。在 Inference 的部分：可以用 DISP 的流水線，先去偵測可疑的 Token，然後再估計它應該有的 Embedding 會是什麼，最後再回復合理的詞。或者是利用攻擊者常常會用高頻詞換成低頻詞的方式去做攻擊，所以我們可以偵測哪裡有一些比較不常見的詞語，把它換成是比較常見的同義詞來修復。[01:51:44] 最後是我們為什麼要去研究這些 Adversarial Attack？因為這些對抗的評估是可以讓系統更可靠、更安全、更公平的一些必要的條件。它能夠提前暴露模型在關鍵情況之下的脆弱性，幫助我們強化訓練、改變資料以及架構的設計，最終可以達成一個更值得信任的自然語言模型。大概是這樣子，那今天是我的報告，謝謝大家。[01:52:12] （老師）謝謝伯賢同學的精彩的閃電秀。那接下來我們要開始進行助教課。[01:52:32] （助教課：Google Calendar Agent 實作）那我先講我的助教課，然後下課之後再上傳（檔案）。那我今天的助教課是要講一個小的實作，是一個很簡單的 AI Agent，然後是用 Google Colab 的 API 去實行。然後我們的目的是要透過輸入一句話給 AI，然後 AI 就可以幫我們自動建立一個 Google Calendar (Google 日曆) 的行程。[01:53:41] 我們第一部分會先說明要怎麼申請這個 Google Calendar 的 API，然後第二部分就是 Colab 的實作。(API 申請流程)我們第一步呢，要先建立一個新的專案。隨便幫這個專案取個名字，然後直接按建立。建立之後呢，因為我們要申請的是 Calendar 的 API 嘛，然後我們就在這邊搜尋 Calendar，然後點進 Google Calendar API，然後在這邊按「啟用」。[01:55:54] 那這邊有詳細的說明，如果之後大家要申請的話也可以參考這邊。按啟用之後呢，我們點這個「憑證」，然後點「建立憑證」。我們要申請的是 OAuth 用戶端 這個。[01:56:36] 進入之後呢，先名字選 Calendar，然後選自己的電子郵件。我們先選「內部」，就是不公開。然後再按這個建立用戶端。應用程式類型的話，我們要選「電腦版應用程式」，然後可以直接按建立。名字可取可不取。重要的是這邊有個「下載 JSON」，然後要點它。我們要下載這個 JSON 檔。如果你按確定的話，就下載不了了，就是要重新再跑一遍這個流程。[01:57:50] 下載了之後呢，因為我們之後要在程式裡面跑嘛，所以大家可以先存到自己的雲端硬碟 (Google Drive)，然後再取一個比較簡短的名字。[01:58:11] 然後這個 Google Calendar 有一個使用事項。就是它雖然是免費的，但是它還是有額度。但是如果依照個人使用的話，我覺得這個額度其實是非常非常夠用的。就是每個人在一分鐘內如果你點超過 600 下的話，它才會給你收費。如果是整個專案的話是一分鐘 1 萬次。(Colab 實作)[01:59:15] 好，那我們第一部分就是先引入各種套件。這些套件都是要先引入的。引入之後呢，我們要先連到我們的 Google Drive。連到 Google Drive 呢，我們才可以去存取裡面的 JSON 檔案。這邊路徑就是要自己填一下。[01:59:52] 填完之後呢，我們就會開始設定它的一些授權。設定授權範圍之後，我們要載入它的憑證。然後因為我們是跑在 Colab 虛擬環境嘛，所以我們要先設定一個本地端 (Localhost) 的伺服器 Port，這樣我們才可以接收它下面那個 OAuth 傳來的授權碼。

[02:00:00] 填完之後呢，我們就會開始設定它的一些授權。設定授權範圍之後，我們要載入它的憑證。然後因為我們是跑在 Colab 虛擬環境，就是 Google 端那邊的虛擬環境，所以我們要先設定一個本地端（本機電腦）的伺服器 Port，這樣我們才可以接收它下面那個 OAuth 傳來的授權碼。我們先跑一次。

[02:01:08] （操作停頓）喔，等一下，忘記了。這個檔案還沒上傳放上去。就是我剛剛下載的 JSON 檔。

[02:02:49] 我們讓它跑一下，剛剛沒有上傳，就是讓它重新跑過。然後跑完這邊就會連到我們的 Google Drive，然後再載入一些相關套件。然後這邊就是跑那個載入憑證。之後呢，我們就是要取得 OAuth 的授權。然後我們就點進去。

[02:03:28] 跑出這個是正常的畫面。因為我們是在 Colab，它的授權碼會放在這個連結後面。這連結後面會有一個 code 什麼的，code 等於這邊，它的授權碼會貼在這後面。所以我們要複製這個連結，然後讓程式去幫我們交換那個授權。

[02:04:06] 然後我們就要交換授權碼去取得它的 Access Token，然後再存檔這個憑證。存完憑證之後，我們就可以建立 Google Calendar 的 API 的服務，然後測試有沒有成功存取。那有成功存取的話就會 print 出來。

[02:04:32] 因為我們就是要讓它建立事件嘛。然後他們那邊就是會有一個文件，就是說我們要用一個 JSON 的形式去告訴他我們要建立什麼事件。所以我們要先建立 JSON。詳細的就是建立要有什麼需要建立的規格，會附在後面。

[02:05:01] 它最必要的就是「開始」跟「結束」的時間，然後其他是可有可無的。那我們這邊為了要完整一點點，我們就加入了 Title。然後它的時間規格只接受這個格式：就是年份、月份、時間、日期，然後時間還有時區。所以我們要在後面把我們的格式轉換成正式的格式。

[02:05:38] 然後再把剛剛的規格轉換成一個建立的比較正式的格式 (Request Body)。在這邊轉換格式，然後它就會 return 一個新的 Body 給我們。然後之後呢，在這裡就會真正的呼叫 API 去幫我們建立。

[02:06:05] 然後我們之後來小小測試一下。就是給它一個規格，然後它就會吐出一個連結給我們。然後點進去就會發現它幫我們建立好這個事件了。就是我們剛剛說的，呃，對，今天弄一個期末專案。

[02:06:38] 那我們確認建立之後呢，我們就要開始套用 AI。我們真正生成這個規格，然後再餵給 API 幫我們生成事件。那這邊呢，就是用老師之前的作業範例去呼叫 AI。那我們這邊使用 OpenAI 的 API。

[02:07:09] 那這邊都是之前上作業做過的範圍，那我們就快速帶過。然後這邊 System Prompt 就是跟他說我們要用把行程轉述成這檔案 (JSON)，然後跟它限制規格。不要其他文字，然後用雙引號，也不要加入這個中括號讓它變成 List，然後請他照這個範例。

[02:07:45] 然後因為 AI 輸出的東西還是可能會出錯，所以我們這邊還是要 Double Check 一下它的規格。就是確保它輸出來的是真正的 JSON 檔案，因為它有可能會輸出成自然語言。

[02:08:15] 然後就是建立這個 ChatGPT。然後我們就寫一個 Prompt 試看。各位在寫 Prompt 的時候就是如果你打「明天」、「今天」、「後天」，AI 可能沒有辦法判別說明天、今天、後天是什麼時候，他就會隨便弄一個時間，然後把這個時間塞進去。

[02:08:35] 我有測試過，跟他說「明天下午兩點到三點要逛街」，結果他輸出的是，假如說現在是 11 月 4 號，他就可能會輸出 2023 年的時間。然後建立一個事件給我。所以大家如果想要玩的話，就是也可以用一個準確的時間。比如說 11 月 5 號。然後它就會跑出來。

[02:09:28] 好，然後我們 11 月 5 號下午兩點逛街的這個事件就建立成功了。測試都可以了，之後我們就套用 Gradio 去產生互動介面。

[02:10:33] （操作演示）去吃什麼時候？12 月 12 號幾點？晚上 7 點到 9 點要聚餐。 然後它就會跳出這個連結。我們點進去看看是不是真的成功了。有欸，成功了。好。

[02:11:28] 然後還有這邊這個 Gradio 這邊 output，因為我們想要它直接跳出一個連結，我們可以直接點進去，這樣會是最快最方便的結果。所以我們這邊使用 Markdown (gr.Markdown) 讓它可以直接跳出一個連結給我們。好。

[02:11:47] 這算是一個最基礎的一個建立事件，就是透過 AI 自動排程。那如果同學想要在自己的期末專案做出什麼可能是更完善的 AI 助理的話，也可以參考我這個助教課的內容。

[02:12:12] 那同學還有什麼問題嗎？ （學生提問：那我不填那個 URL 可以嗎？） 那沒有填，可以不用填，對。

[02:12:33] 那如果就是你真的超過它的額度快要超了，你可以去它這邊有一個設定，就是看一下怎麼設定。就是有一個地方可以設定說額度警告。 （學生提問：哪裡可以憑證？） 這邊進去有一個「配額與系統限制」。然後這邊如果你真的有大量需求要使用這個 API 的話，這邊有一個「編輯配額」，然後你可以輸入新的值，就是低於它這個標準的值。那它如果超過的話，它就會在你跑程式的時候警告你說超額了。對，所以如果你有要大量使用的話，也可以在這邊做設定。

[02:13:51] 所以也不用太害怕。一分鐘 600 次是單個使用者，那如果是整個專案的話是一分鐘 1 萬次。對，這邊 1 萬次跟 600 次。那我這個程式設計的是就是只能自己使用。

[02:14:18] 然後如果同學要公佈給大家用的話，就是你要記得你下載的那個 JSON 檔案不要公開。對，就是做其他專案或者是上傳 GitHub 的時候，不要把它設定在公開的地方。

[02:14:38] 然後日曆（Google Calendar）本身也有自己的上限。我有放在那個...在這邊。日曆本身也有現額，現在的話其實也是蠻多額度的啦，就是正常使用的話其實也不會被要求收費。對，自己做是沒有問題的。那如果要像有那種企業規模的話，就會收費也蠻應該的啦。

[02:15:26] 對，就是如果同學要使用 Google Cloud 其他 API 的話，也要去查資料。其實 Google 資料也寫得蠻詳細的，就是這些事件說明跟管理配額他們都有一份文件讓大家去看。就是要看仔細、看明白，然後再去使用。對，做任何專案都要這樣。

[02:16:02] 好，還有什麼問題嗎？ （老師：下課下課） 嗯，那我們今天的助教課就上到這裡囉。

生成式 AI：文字生成圖像原理與 Fooocus 實作（第一部分）

時間範圍： 00:00:00 - 00:38:53
影片來源： 【生成式 AI】11. 文字生成圖像 AI 的原理及用 Fooocus 實作

[00:00:00] 我們現在開始上課。如果大家有發現我們之前的投影片，會發現進程有一點點改變，這是因為最近 AI 的發展太誇張了。我們上次開課也只是上學期而已，但從上學期到這學期發生了很多事情，讓我們覺得進程需要做一些改變。

[00:00:37] Diffusion Model（擴散模型）還是一個非教不可的模型，但我們可能會讓整個進展速度快一點，讓我們有時間去討論一些比較新發生的生成式 AI 相關議題。本來 Fooocus 是要帶大家下一次實作的，這次原本是要帶大家用程式去呼叫這種 AI 模型，有點像我們之前的文字生成 AI 那樣。

[00:01:12] 我相信還是會有同學有這樣的需求（用程式呼叫），這沒有關係，我們在附錄裡面有告訴大家，如果你真的要這樣做的話該怎麼進行。你可以完完整整地去掌控、去做一個自動化的圖像生成系統，這是有可能的。但是在這個課程裡面，我們會先暫時略過那一個部分，相關內容會放在今天的投影片裡面。今天的投影片其實已經上線了，大家可以去參考一下。

[00:01:49] 我們首先要說明一下，上次我們很快地介紹了 Diffusion Model，基本上它就是一個 Autoencoder（自動編碼器），特別是類似 VAE 那種形式的東西。也就是在訓練的時候，整個訓練其實就是一張圖進來，然後一張圖出去。

[00:02:07] 我們會把整個模型拆成兩個部分，一個是 Encoder，就是產生 Latent 的地方；另外一個地方就是 Decoder。Decoder 那邊就是把我們今天輸入變成了一個 Latent Tensor（潛在張量）之後——這個特徵代表不一定是向量，可能是矩陣或 Tensor，這我們之前已經介紹過了——我就想辦法把這一筆數據再還原。

[00:02:49] 這就是我們訓練過程要做的事情，只是跟以往的 Autoencoder 或是 VAE 有一點不一樣的地方，是我們在 Encoder 的部分是用「算」出來的。就是一直加噪（加雜訊），加到最後你覺得每一個點、每一個 Pixel 都符合某個狀態，例如標準常態分佈。

[00:03:10] 這樣子我們就很容易產生任何圖像，我們就可以隨機產生那一個 Latent Tensor。我們可以隨機產生 Latent Tensor，放到 Decoder，然後 Decoder 就會還原成一張圖給我們看。如果我讓它看的圖夠多，理論上每一個我們隨機生成出來的那些點，都應該能還原成一張看起來蠻漂亮的圖。大概想法是這樣子的。

[00:03:49] 這是圖像生成的部分。再說一次，Encoder 雖然是用算出來的，但 Decoder 還是用神經網路學出來的，是一個叫做 U-Net 的神經網絡。這是一個非常標準的 CNN（卷積神經網路）神經網絡。CNN 就是圖形辨識很強的神經網路，大家未來如果還沒有學過深度學習或是神經網絡的基礎課程，歡迎大家去修習。

[00:04:34] CNN 通常就是用在圖形上面，U-Net 的特色是它輸入進來的圖的大小跟輸出的圖的大小是一模一樣的，非常適合我們 Diffusion Model 的形態。因為 Diffusion Model 大家應該還記得，輸入進去的時候所有的過程，資料的大小是一模一樣的，直到最後輸出也是一樣的。所以大家發現 U-Net 很適合做這件事情，就用了 U-Net。

[00:05:28] 雖然我們上次其實已經稍微解釋過了，就是你打了文字，它怎麼知道就要畫成那樣？因為我們剛剛前面說的那種標準訓練方式（Autoencoder），如果它看的圖夠多，的確輸入一堆從標準常態分佈裡生成的點，它應該就會還原出一張漂亮的圖。但在這樣的情境之下，我們沒辦法控制它到底畫出什麼圖。圖很漂亮，但不是我們要的圖，好像沒有什麼用。

[00:06:15] 所以我們現在要想，到底要怎麼樣告訴它我要怎麼畫出來？上次其實我們有稍微說到，那個秘密就是在一個模型上面叫 CLIP。CLIP 是一個很有趣的東西，它是要找文字的 Embedding（嵌入）。找文字的 Embedding 沒有很特別，因為在我們的生成式 AI 裡面我們至少說過兩次。

[00:06:40] 第一次就是我要產生下一個字，所以我在最後一次吐出來的那一個 Hidden State，也可以當成是前面那一串的 Embedding，然後我就能產生下一個字。另外就是我們在做 RAG（檢索增強生成）的時候，要把一堆文字找到它很適合的特徵代表向量，我們才能去做比對，那個也叫做文字的 Embedding。

[00:07:03] 現在我們也是要做文字的 Embedding，但是有一點點不一樣的地方是，它的概念很有趣。因為我們可以想像，比方說這邊有一段文字說「一個女孩子在一間咖啡店裡面」，右邊是圖像的樣子。一張圖它會代表一個意涵，一段文字當然也可以代表一個意涵。所以我們是不是可以想辦法把代表同樣意思的圖跟文字的 Embedding（特徵代表向量）拉近一點點？

[00:07:47] 這怎麼做其實很簡單。假設今天有一張圖代表一個意思，然後有一段文字代表一樣的意思。圖可能用 CNN 或 Transformer，文字可能用 Transformer 或其他方法，反正送進一個神經網路後，最後吐出來的向量，我們怎麼把它拉近？如果這張照片本來就是用這一段文字去形容的，那就代表它的意思嘛，所以我們就想盡辦法把它們兩個的特徵代表向量拉得越近越好。

[00:08:42] 相反的，其他不相干的照片或者不相干的文字，我就把它推得越遠越好。就想辦法讓它看很多很多這樣的例子，讓代表同樣意義的照片跟代表同樣意義的文字，把它們的特徵代表向量拉近一點。大概就是這樣的想法。

[00:09:12] 過去大家就想辦法收集資料，因為 OpenAI 等大公司去訓練這些模型，他們當然不會公開訓練資料集。過去有公開源型的，就是有 5B（50 億）大小的數據集（LAION-5B）可以供給大家去使用。以前最有名的數據比賽 ImageNet，是要把很多照片拿去分類成 1000 種，那一個數據集其實已經是以前比較大的數據了，但也才 1400 萬張。現在突然跳到 58.5 億，所以跳得相當多。

[00:10:32] 如果大家覺得想要自己去訓練類似這種 CLIP 模型的，就可以這樣做。當然 CLIP 本來就是 OpenAI 的模型，所以它到底怎麼訓練出來的其實大家沒有那麼清楚。相對的，現在用 Open 的數據集，我們就有一個 Open CLIP 可以去做。

[00:11:00] Open CLIP 基本上就是一串文字進去的時候，經過 Transformer，它就會吐出一堆 Hidden State 的狀態（H1, H2 到 H T）。這我們已經介紹過了，就好像是每一個字對應的前後文意義到底是什麼。CLIP 一共用了 12 層的 Transformer，就像是一層一層慢慢越來越多層的抽象理解，最後的理解可能是最好的一個理解。我們就希望那個文字的理解跟圖像的理解是非常接近的。

[00:11:51] CLIP 做出來的時候，只要有一段文字進去，它就有它的特徵的抽象理解，就是它的 Embedding。它的 Embedding 也很奇特，它不是一個向量，它是一個矩陣的樣子，就是 77 乘 768 的矩陣。這是 Stable Diffusion 用的標準架構。當然你不一定要都 77 乘 768，看你開心訓練成什麼樣。

[00:12:23] Stable Diffusion 原來的圖是 512 乘 512 的，因為它有 RGB，所以就有三個 512 乘 512 的矩陣。因為 512 乘 512 對電腦來說還是太過分了一點點，所以通常我們會把 Latent 版的 Diffusion Model（LDM）縮小。就是先用一個 VAE 訓練，每一直張圖都找到一個它的縮小版。

[00:13:00] 那個縮小版不是一般的圖，因為縮小版的那一張圖你拿出來看的時候，其實你不知道它在畫什麼。AI 的圖非常的抽象，就是 AI 的世界。總而言之，一張圖它就會把它縮小成 64 乘 64 大小的圖。

[00:13:30] 縮小應該是變成 3 乘 64 乘 64（配合原來的 RGB），但他們用了 4 乘 64 乘 64。因為中間那個 Latent 長什麼樣，是我們定的。那個數字是我們定的，所以我要 4 乘 64 乘 64 或是 5 乘 64 乘 64 都可以。所以我們會先用一個 VAE 把那個 3 乘 512 乘 512 大小的圖，縮成 64 乘 64 的圖，但是它是用四個通道（Channel），也就是四張矩陣。它不代表 RGB 三原色，它是抽象的特徵理解方式。

[00:16:06] 我們沒有詳細介紹 CNN，但 CNN 其實就是先去截取圖形的特徵。每一個 CNN 的層就會有好幾個 Filter（特徵截取器）。每一個特徵截取器會掃過整張圖，記錄這邊有什麼特徵。比方說看直線特徵的，我就會記錄每一個地方直線特徵的強度，就有一個自己的「計分板」。二號 Filter 可能看橫線的特徵，所以有幾個 Filter 就有幾個計分板。

[00:17:18] 為了說明方便，假設我們現在一張圖的大小是 4 乘 4（實際上沒這麼小）。我有三個特徵截取器，我就有三個計分板，變成 4 乘 4 乘 3。經過 CNN 之後，我可以把很多特徵，每一個特徵都各自有自己的計分板。這個結果其實在某種意涵上，它還是代表原來的圖，只是經過了 CNN 去理解它。

[00:18:38] 這就是目前的 Latent Tensor。假設我們現在目前的圖大小是 4 乘 4，有三個計分板（4 乘 4 乘 3）。而我們剛剛說過經過 CLIP 之後的文字 Embedding 是固定的，就是 77 乘 768。我們可以想成好像有 77 個字，這 77 個字代表一個神祕的文字語言系統。不管我怎麼下 Prompt，它都會化成 77 個神祕的字，每一個字有 768 的特徵代表向量。

[00:19:53] 我們現在要想辦法把這個特徵代表向量混到原來的圖進去。也就是說，我們最後希望還是 4 乘 4 乘 3 大小的樣子，只是我們把意思混進去了。我們怎麼混的？上次我們有稍微說過，就是把現在的那一張圖當成 Transformer 裡面我們要用的 QKV 的 Q (Query)；那文字那邊的一行，我們把它看成 K (Key) 跟 V (Value)，然後我們就去做 Attention（注意力機制）。

[00:20:53] 做完了以後，我們會得到一個新的 Tensor，那個新的 Tensor 其實就是我們要的。我們現在稍微說明一下，不然上次說完了應該沒人懂。

[00:21:49] 首先 Q 出來的方式比較奇特一點點。現在因為有 4 乘 4，所以一共有 16 個 Pixel。把每一個點的第一個 Channel、第二個 Channel 跟第三個 Channel 合起來，這樣子就代表一個 Q。所以 Q 會有 16 個向量（16 列，每一列維度是 3）。

[00:23:19] K 跟 V 怎麼來呢？K 跟 V 都是從文字的 Embedding 來的。文字 Embedding 是 77 乘 768。現在我們要跟 Q 做 Attention。一個很重要的事情是，我們拿出一個 Q 向量之後，我們就要跟所有的 K 去做注意力機制。Google 最喜歡的就是用內積去做。所以這 77 個字裡面，每一個向量都要轉成三維的特徵代表向量，才能跟 Q 去做內積。

[00:24:21] 所以我們會乘上一個矩陣（學出來的矩陣），把 K 化成 77 個字，但是每一個字只有三維的特徵代表向量。V 也是 77 個字，三維的特徵代表向量。

[00:24:48] 接著作法是這樣：最左邊的 Q 向量集，假設我們把第二列拿出來，我要跟每一個 K 都看它們的相關強度。用內積去做，第一列跟第一列做內積得到 E1，第一列跟第二列做內積得到 E2，一路做完 77 個字。然後我們做 Softmax，就會出現 α1 到 α77，加起來等於 1。

[00:25:47] 這些數字就把 V 向量的每一列做線性組合起來。組合起來之後，當然它還是一個三維的向量。每一列 Q 都做這件事情，所以最後就會變成 16 乘以 3。你會發現最左邊跟最右邊的大小是一模一樣的。

[00:26:16] 這樣我們的文字就很自然地混進去了。過去我們會收集很多文字跟圖有對起來的相片，然後我們就告訴它說這一段文字其實畫出來的圖應該長那個樣子，所以它就會畫出那樣的圖。

[00:26:54] 重點就是因為可以把一段文字混進去，其實也會把一張圖混進去。如果你不太會形容你要做的創作，我們之後也會用到，因為你發現用一個 CLIP，你輸入文字跟輸入圖是一樣的意思。所以我也可以輸入一張圖，說我就是要這樣的東西，它就會去生出一張圖出來。這就是 Image-to-Image 或 Image Prompt 的原理。

[00:28:11] 我們上次很快講解過，U-Net 有好幾層，我們在每一層的時候，都可以做剛剛的 Transformer Attention 機制，把文字加進去。也就是說，那 77 乘 768 的 Embedding 是固定的，然後每一次都告訴它我的 Prompt 是這一個，第一層放進去，第二層又再告訴它一次，第三層又再告訴它一次。

[00:29:12] 所以會有幾個技巧：

調整權重：你有時候會希望電腦稍微有創意、天馬行空一點點，不要完全照著你的意思走，你就可以說我的 Prompt 權重不要下那麼大（例如 0.6）。

停止提示（Step termination）：不要那麼囉唆，不要從頭到尾每一次都跟它說。在某一個時間點我就把它停掉，比方說只在前一半的步數跟它講，後面就讓它自己看著辦、繼續畫下去。

[00:30:40] 再來 Diffusion Model 裡面還有一個很有趣的東西叫做 排程器 (Scheduler) 或 Sampler。

[00:30:56] 因為 Diffusion Model 是做這件事情：我們今天得到的是一堆看起來很亂的雜訊（Xt，可能是 X1000）。我們想辦法要把它還原成 X999，然後再想辦法還原成 X998，這樣一路下去。也就是說，它其實就是一個數列。

[00:31:51] 我們常常會在意一個數列到底是收斂的還是發散的。收斂的例子像 9, 8, 7... 收到 1。發散的例子有兩大類：一個是在那邊亂跳，永遠不會收斂到某一個固定的數字；也有可能是往無窮大跑。

[00:32:46] 分析的基本精神就是先隨便拆一個，然後找一個神妙的算法，想辦法找到 A2 比 A1 更接近一點我們要的答案，然後再來 A3, A4...。在 Diffusion Model 裡，這個 X1, X2... 其實就是一張圖。我們在 Denoise（去噪）的過程其實就在做這件事。我們就是想辦法把前一個 Xt，想盡辦法找到一個更好一點點、更接近正確答案的 Xt-1。

[00:34:24] 我們會希望兩件事情發生：

加速：我們希望還原的時候不要真的走一千步。我很希望像微積分一樣，可以預測如果趨近無窮遠或目標的時候，它應該會趨近什麼。所以我希望能加速，不要算那麼多步。

收斂：我們偷偷希望這個數列是收斂的。也就是說，如果這張圖走 100 步覺得不夠精細，我再走個 200 步，那應該比走 100 步還要精細，但是是原來那一張圖。

[00:35:28] 很不幸的是，不是所有的 Scheduler 都是收斂的。

[00:35:41] 我們的 Denoising Model（去噪模型）會告訴它說 Xt 這張圖進來，還要告訴它 T 這個數字（時間點），請它預測總 Noise 是多少。預測出來後我就可以減掉它還原。但是我們沒有這麼厲害可以一步就生出來（雖然可以，但圖會超可怕）。

[00:36:55] 所以我可以用 $\epsilon_t$（Epsilon t）這一步去估算從 T-1 到 T 這一步到底加多少噪。我們可以去估算 $\epsilon_{t-1}$。實際上就是想辦法從 Xt 減掉 Noise 算出 Xt-1，再把 Xt-1 跟 T-1 放進去模型，告訴我們誤差是多少，再估算 Xt-2。

[00:37:57] 加噪的時候可能是 1000 步，Denoise 的時候最好的方法當然還是 Denoise 1000 步，可是這實在太複雜了。如果我們剛剛的方法認真想了以後，我們會發現其實我可以估其中一段。當然一步一步是最準的，但是我們可以估一小段、一小段，基本上就是跳步走。跳步走之後，它就可以比較快地生出來了。這是最簡潔的講法。
生成式 AI：文字生成圖像原理與 Fooocus 實作（第二部分）

時間範圍： 00:38:53 - 01:17:37
影片來源： 【生成式 AI】11. 文字生成圖像 AI 的原理及用 Fooocus 實作

[00:38:53] 雖然在 Diffusion Model 還有很多 Denoise（去噪）的方法跟 Scheduler 可以去做，但如同我們剛剛說的，通常如果你想要讓品質變好，或是想要畫出一張固定的圖，你會固定一個 Random Seed（亂數種子）。

[00:39:30] Random Seed 固定了以後，它每次生成的開頭那些雜訊是一樣的，長得一模一樣。所以理論上它應該就是要代表同樣的圖。因為我們想要比較快得到結果，在生圖的時候，你會發現有時候生得不錯，有時候生得亂七八糟。那我們能不能先大概生出來，讓我看一下有沒有生對？對了，我才讓它慢慢畫；錯的我就不要畫那一張了。

[00:40:08] 所以我們本來想到應該有一個技術，就是我先讓它生 20 步就好了。20 步就把圖生出來（如左邊那張圖）。如果我固定 Random Seed，大家就會幻想說，那如果我跑多一點步數，應該會變成右邊那張圖（更精細的版本），只是細節變多，但要是原來那一張圖。我們夢想是這樣，因為如果是收斂的話，它就應該是這個樣子。

[00:40:46] 但很遺憾的是，不能這樣做。應該是說有些 Scheduler 不可以，像 Ancestral 系列（如 Euler a）就不行。因為它為了加速，在降噪的過程中會透過一些隨機加 Noise 的技巧來加速過程，所以它的速度會變快，可是很遺憾它不會收斂。

[00:41:14] 不會收斂會發生什麼事情？就是你畫出來 20 步，你覺得不錯、滿意了，想說那來畫 200 步試試看，結果發現畫出來的圖是不一樣的。所以這個就是我們要小心的地方。

[00:41:32] 我們來看一下其中一個不收斂的例子給大家欣賞一下。再次強調，不要幻想說我們一次就可以到位。雖然一次可以到位，但步數少時可能會畫出很可怕的圖。例如走了 5 步、10 步的時候還有點可怕，15 步的時候看起來好像有點樣子。

[00:41:59] 你可能會想說，那我以後都畫個 15 步，滿意了以後再讓它畫個 200 步，並且固定 Random Seed。所有的生圖軟體（不管是我們自己寫程式，還是像 Fooocus）都可以讓你控制 Random Seed。我們想像中間畫 200 步應該就是這一張圖，只是更精細一點。

[00:42:34] 但我們來看看是不是真的這樣。我們來看 20 步，你會發現跟剛剛好像有點不一樣。到了 25 步的時候，突然你可能本來覺得皮衣不錯，跟黃仁勳差不多，結果它換了一件衣服。到了 30 步的時候，你又發現它又換回皮衣了。

[00:42:54] 你再做下去，你可能會覺得後面會越畫越精細，應該就差不多穩定了？沒有。45 步換成牛仔外套，85 步去剪頭髮了，100 步的時候連表情都變了。所以你會知道，在有一些排程器（Scheduler）裡面，你沒有辦法用我們剛剛想像的技巧——先用個 10 步、20 步簡單畫一下，成功的再固定 Random Seed 畫精細一點。

[00:43:28] 有一些 Scheduler 不可以，有一些可以。這裡只是跟大家講說，像剛剛說的 Ancestral 系列（如 Euler a）都是有隨機性的；像 DDIM 跟 LMS 等等，基本上是收斂性的。收斂性的時候，你會發現通常需要的步數都比較多。因為 Ancestral 系列之所以有隨機性，就是希望它能夠加速。

[00:44:32] 所以你會發現，非收斂性的 Scheduler 結果通常需要的步數比較少。所以「不會收斂」不一定是一個問題，但是你可能要先找到你滿意的步數，畫出來就是那張圖就好了，不要再想那個「先粗糙再精細」的小技巧。雖然不能用那個技巧，但在很多時候它可以省比較多的時間，比較快把圖畫出來。

[00:45:25] 這些理論的部分差不多說完了。下一次我們還會介紹一些更深入的實用技巧。今天因為已經了解了，就會發現 Diffusion Model 基本把文字混進去的方法，其實就是用 CLIP 做出一組 Embedding，然後用注意力機制把它混進去。混進去之後，它就會照我們的意思把圖畫出來。

[00:46:19] 因為我們知道原理，它就是把圖的意義跟文字的意義對齊，如果很接近的話，它就會把 Embedding 做得很接近。所以一張圖進去的時候，它也會找出一組 77 乘 768 的 Embedding。所以如果你不會說（描述 Prompt），你也可以畫給它看，或是找一張那種感覺的照片給它，它就會找到對應的 Embedding 然後畫出來。

[00:46:58] 雖然今天作業沒有請大家做這件事情（Image-to-Image），但大家其實可以試試看，用圖像去當成文字輸進去，這很有趣。

[00:47:31] 順便再說一下，我們已經快要到期末了，希望大家已經開始做期末專案。大家做期末專案記得要錄一個解說影片。時間應該是 1 到 3 分鐘而已。因為我們的展覽是聯展，我們政大的做法是會選擇若干位同學一起來參加。因為是虛擬世界參展（Gather Town），你也不用站在海報旁邊，你可以直接錄個短的解說。因為參展的同學很多，解說不要太長。

[00:48:50] （休息後）大家好，那我們現在要介紹一個叫做 LoRA 的技術。在 Diffusion Model 當紅時期，LoRA 非常有名。LoRA 很重要，所以我們這個生成式 AI 課程一定要介紹。

[00:49:13] 一開始大家會用 LoRA，是因為 Diffusion Model 可以讓你微調。比方說你發現 AI 不太會畫你，你想要讓 AI 很會生成你的照片，就要拿一些資料去訓練它。但是畢竟你沒辦法提供真的非常大量訓練資料，如果今天就拿幾張照片想要訓練讓 AI 知道我長這個樣子（我可以幫自己取個代號，例如教它這個代號就是這個人），它就會畫出來了。

[00:50:24] 但是因為我們再怎麼樣也不能放很多很多照片下去，所以常會發生一些狀況。第一個狀況是，原來的模型可能我們電腦跑不太動；第二個狀況是，因為我們每一個參數都做這麼完整細緻的調整，雖然最後可能訓練成功、把我們畫得很好，可是它本來會畫的東西，可能就壞掉了（Catastrophic Forgetting，災難性遺忘）。

[00:51:07] 基於這些原因，就開始出現 LoRA。LoRA 這個名字唸起來有點可愛，大家以為是不是一個公仔的概念？不是，它是一個很數學的名字，叫做 Low-Rank Adaptation（低秩適應）。

[00:51:41] 假設我們的 Stable Diffusion 模型裡面所有的參數寫成 M（這一定可以寫成矩陣的樣子），假設裡面有兩億個參數。本來要做微調的話，我就要調兩億個參數。那我們有沒有可能不要調兩億個，但是最後真的可以影響到這兩億個參數？

[00:52:17] 這聽起來互相矛盾，但我們的目標是：假設電腦負荷不了兩億個參數的調整，我們希望能只調整比較少的參數。但我們沒辦法知道這兩億個參數哪一個比較重要，不能只拿其中 2000 個去調，因為不知道哪 2000 個重要。所以我們希望調整 2000 個參數，但能把兩億個參數全部調整到。

[00:55:38] 做法是這樣的：我們把原來的參數凍結，我們去調整要加上去的那個矩陣 Delta W。大家可能會說：「老師你騙我，M 乘 N 的矩陣只能加 M 乘 N 的矩陣，加法的時候要加一模一樣大的矩陣啊！」所以 Delta W 的數字量應該是一樣的。

[00:56:05] 但是現在我們要做一個小魔術。其實也很簡單，就是利用矩陣乘法。大家記得 M 乘 N 的矩陣，如果我想寫成兩個矩陣相乘，可以是 (M 乘 K) 乘以 (K 乘 N)。這個 K 多少都可以。乘出來就是 M 乘 N 的矩陣。

[00:56:44] 所以我可以把 K 選得很小。舉個極端的例子，K 取 1。所以 A 矩陣就是 M 乘 1，B 矩陣就是 1 乘 N。A 跟 B 合起來只有 M + N 個數字，可是乘出來的結果是 M 乘 N 個數字。M 乘 N 通常大很多。

[00:57:16] 也就是說，我們需要調整的參數就是 A 矩陣裡面的每一個元素跟 B 矩陣裡面的每一個元素。只要我們 K 取得夠好，那這裡面要調整的參數就小很多。這就是 LoRA 的概念，Low-Rank 的意思就是這個中間的 Rank (K) 比原來矩陣的 Rank 還要低。

[00:58:02] 這本來是 Microsoft 的一篇論文，一開始是用在 LLM（大型語言模型）身上的。後來在 Diffusion Model 上變得非常有名，因為大家比較感受得到圖像微調的需求。

[00:58:24] 像 Civitai 這些網站收集了很多訓練好的 LoRA。使用的時候有幾種方法，一種就是把它讀進來。讀進來基本上就是讀 A 跟 B 矩陣那些數字。如果要混到原來的模型，理論上要把 A 乘上 B 算出來以後再加進去。

[00:58:59] 有時候你會覺得這個 LoRA 我很喜歡，每一次都要用，那你也可以一開始就把它加進去。這就是為什麼有些 Checkpoint 模型是人家已經融合好 LoRA 的版本。你可以去找那些 Base 模型（如 SDXL），也可以找人家已經混好 LoRA 的模型。

[01:00:16] LoRA 真的要進去的時候，我可以強度不要那麼強，比方說權重調成 0.7 再混進去。但是直接融入原來模型比較沒有彈性。你也可以在需要的時候再呼叫它（透過 Prompt 或介面設定），告訴它我要 0.7 的權重就好。這樣的好處是有彈性，而且你可以混很多個 LoRA。

[01:01:25] 現在世代已經改變了，大家只要稍微知道 LoRA 的原理就可以了。每一個微調好的模型通常會叫一個 Checkpoint。你會發現很多神經網絡的模型現在都用這兩種方式儲存：.ckpt 或 .safetensors。.safetensors 看起來比較安全，事實上也是，所以現在大家比較推薦 .safetensors 的儲存方式。不過在我們的課程或大部分情境裡，我們現在也不太需要管它了。

[01:02:33] 理論部分差不多說完了。接著要為大家介紹的是簡易的圖像生成小工具，叫做 Fooocus。

[01:02:48] 我們去查這個 Fooocus GitHub。注意這個 Fooocus 是三個 O，它真的就是這樣拼的。本來唸起來像 Focus，但它事實上是三個 O。

[01:03:20] 關於安裝：如果你在 Windows 下安裝，前提是你的電腦要有 GPU（NVIDIA 比較保險），基本上都可以跑。如果你用 Mac 或是 Linux 也可以安裝。如果不太熟悉程式的同學不要緊張，有一天你也會看懂這些在幹嘛。

[01:04:11] 這裡只是建議如果對這個特別有興趣，尤其是有 GPU 的同學，不如就在自己電腦上安裝一下。如果你是用 M 系列晶片的 Mac 基本上都可以，只是因為它沒有為了 M 系列調整，速度會比較慢一些。

[01:04:43] 如果不想裝在自己電腦，也可以用 Colab。Colab 畢竟是提供給很多人使用的，雖然號稱給我們很好的 GPU，但事實上也沒有真的很好的 GPU。

[01:05:03] 我們準備來做實際的示範。先做一下在 Colab 安裝的示範。你就去 Google 搜尋關鍵字 "Fooocus github"（三個 O）。如果直接查 Focus 會有外面很多假的、不是官方的版本，有些還會收費。Fooocus 是一個開源軟體，免費的。

[01:06:35] 點進去標準的 GitHub 頁面。Fooocus 的目標很簡單，因為大家都覺得 Midjourney 其實還蠻好用的，只要跟它講什麼它就畫什麼。所以 Fooocus 的目標就是希望它可以變成一個幾乎像 Midjourney 一樣好用的開源軟體。它有提供一個 Colab 的連結，雖然上次更新已經是去年 8 月，但它至少可以在 Colab 上面跑。

[01:08:20] Colab 這邊不要直接按「全部執行」，這不是由 Google 寫的，會有警告。一樣，你可以把它「在雲端硬碟中儲存副本」。這大家應該已經很熟悉了。

[01:08:58] 唯一要檢查的是它有沒有開啟 GPU。像這邊已經告訴我它準備用 T4 GPU。如果不確定，可以在「編輯」->「筆記本設定」這邊看。如果選擇是 CPU 的話就沒有開 GPU。儲存後就可以開始執行了。事實上這邊可以全部執行，因為只有一行。它其實就是安裝 Fooocus 然後去執行。

[01:10:30] 安裝好之後，你會看到 Fooocus 長這個樣子，非常的簡潔。它知道我們有時候形容詞的想像力很單薄。如果你真的用 Stable Diffusion WebUI 的話，你會發現你真的英文要不錯，要很有能力去形容你的圖到底要長什麼樣子。

[01:11:17] 當然大家現在會覺得說，我們有大語言模型（LLM），只要用中文說，讓它翻成英文就好了。可是我們認真去做的時候，又會發現我們中文其實好像也沒那麼厲害去形容一張圖。之前我們助教有介紹給大家，你也可以告訴 AI 說我大概要畫這個，請 AI 幫你去說得比較仔細一點點。

[01:11:53] Fooocus 其實內建了一些簡單的 AI，所以它可以不用做什麼偉大的形容。比方說我打「Shiba Inu」（柴犬），它就會生出一張狗的照片來。你可以寫得比較好一點，唯一的限制是要用英文。可是用英文現在大家也不是問題了，因為其實大家就可以直接請大型模型幫你翻譯。

[01:12:45] 因為我們本來是有程序的，我們本來是先用 Stable Diffusion 標準的版本跟一些人家改好的 LoRA 版本。你會發現要生出一張好的圖，有時候沒有這麼容易，會有各種問題發生。比方說要生一個人的照片，有時候缺手缺腳、六根手指非常常見。這時候你都需要去下所謂的 Negative Prompt（負面提示詞）。Negative 就是說你不該畫的東西不要畫。

[01:13:42] 所以我們可以再多試這些。你下很簡單的 Prompt，比如「兩個台灣的大學生」。我們什麼形容詞都沒有，這個如果在本來的 Diffusion Model 畫的話，可能會很慘。但在 Fooocus，你會發現好像蠻簡單的。

[01:14:28] 這是最基本的用法，就是今天就下一個 Prompt。我可以下很簡單的 Prompt，因為它後面有 AI 語言模型，它會幫你把這個 Prompt 延伸，說得比較多話，去告訴我們的 Stable Diffusion 畫出比較漂亮的圖。

[01:14:53] 這裡有一些基本的更改方式。這邊有一個圖的格式（Aspect Ratios），比方說 3:5、4:7 等等。你可以在這邊設定你的圖要是寬版的還是比較長的。

[01:15:26] 下面有個 Speed 的設定，這看就知道是執行速度。

[01:15:42] 然後這邊就是 Random Seed。這邊也可以讓你去產生 Negative Prompt，但大部分情形其實你不需要做。這個 Random 勾選拿掉之後，它就會固定這個神秘的亂數種子。比方說我們再做一次一模一樣的，它應該生一樣。對，你看一模一樣的圖又重現一次。

[01:16:27] 這個 Image Number 就是顧名思義，就是你要生幾張圖。如果你覺得每次生兩張圖實在是不開心，你也可以一次給它生四張圖。我們不要太多了，免得生太久。

[01:16:53] 這個是固定那個亂數種子的技巧，所以它就真的會一直畫出一樣的圖。

[01:17:26] 好，那我們回來設兩張圖好了。我們一開始需要做的事情不要弄得太複雜。後面有一些功能，其實有一些還蠻酷炫的，我們之後會介紹。前面我們已經說過了，這大家可以試驗。然後 Preset（預設風格）大家也可以試驗，這看起來就像是要畫動畫。
生成式 AI：文字生成圖像原理與 Fooocus 實作（第三部分）

時間範圍： 01:17:37 - 02:00:00
影片來源： 【生成式 AI】11. 文字生成圖像 AI 的原理及用 Fooocus 實作

[01:17:37] 我們一開始需要做的事情不要弄得太複雜，所以有一些功能我們之後會介紹，大家先不用緊張。前面我們已經說過了，大家可以試驗 Preset（預設風格），這看起來就像是要畫動畫。我們來試試看，因為我們現在固定了亂數種子，你可以試試看卡通型的。希望它不要丟臉，因為我有時候發現 Fooocus 不太會畫其他的，但它有畫成卡通性的。

[01:18:26] 大家可以自己去試驗看看，你自己希望呈現的風格是哪一種，就可以呈現那種風格。這種風格以前可能要去呼叫不同的 AI 或是 LoRA 去控制，那 Fooocus 的好處就是它基本上幫你做得很簡單，你就可以去試驗。其他的也可以去試驗，或是我們希望真實版一點的（Realistic）。

[01:19:09] 大家可以簡單的選一選，覺得哪一種滿意就選那種。我們今天的作業就是要做這件事情，等一下我會詳細說今天作業到底要幹嘛。其實就是用 Fooocus 去畫圖，那是真正的 Model 生成的圖。

[01:19:47] 大家可以發現，固定了 Random Seed 之後，你真的可以看到不同的功能產生的效果到底有什麼不一樣。因為如果你沒有固定 Random Seed 的話，你在做其他功能的調整（比方說要產生快一點、慢一點等等）時，你就不太確定到底是你調得真的好了，還是因為它的 Random Seed 不一樣，所以畫出來效果不一樣。所以最好你在開始試驗的時候把亂數種子給固定。

[01:20:29] 這邊有很多個它已經預設好的風格，其實很多還蠻有趣的，大家可以看一看。比方說這個畫 2D 的，那就是畫插畫型的。我也試試看，因為它有時候不會成功，好緊張。你看它畫的哪有 2D？這個明明就還好。

[01:21:17] 如果我把前面的全部取消掉好了，雖然 Fooocus 官方說不用把全部取消掉。我們就全部取消掉，只留 2D 的看一看。這哪像啊？這就是因為 Fooocus 在做的時候，它的調整方向比較是為了讓照片呈現的時候沒有問題，所以它很多都是為了照片去做調整的。因此你會發現它有時候畫別的會失敗，明明告訴我們 2D 要長這樣。但有一些會成功啦，我們再隨意試驗看看。

[01:22:07] 這個做貼紙（Sticker）可以吧？不要再丟臉喔。做貼紙就真的做貼紙，好，就這樣子。所以它有時候真的會成功。大家可以多去試驗一下。它做得很好的是它都會顯示在這邊給你看。

[01:22:43] 這一個就是畫圖畫型的筆記本，它真的只有圖畫型的筆記。有了有了，它旁邊記錄了很多東西。所以它會做蠻多還蠻有趣的東西，因為已經做好了，所以你就可以很快速地去試驗看看。很多是照片型的，如我所說，很多它都是為了照片去做那個加強版。

[01:23:43] 這還不錯，它是一個免費的軟體，然後它可以讓你在那邊亂玩各種各樣的東西。那種畫水彩畫的比較簡單，我們就不要試了。這個看起來很奇特的，我們也試試看好了，你也以各種去把它混合，會產生有一些比較不一樣的結果。

[01:24:33] 寶可夢它有時候會畫成這樣，我們明明是說要做寶可夢，然後它就畫出來變成是手裡拿著寶可夢，或者裝扮成寶可夢的樣子。它有時候會跟你想像不太一樣，但是有特別還蠻有趣的效果。

[01:25:03] 大家可以盡量的去試驗看看。它最大的好處就是它真的都有顯示給你看，告訴你這個到底是在畫什麼。真的太多了，我自己看得都已經累了。大家回去之後可以慢慢去試驗看。我們再選一個比較有趣的，做成 3D 公仔型行。試試看，像嗎？沒有那麼像真的人，但對 3D 好像又太像了一點。大概就這樣了。

[01:27:00] 剛剛我們也看到 Fooocus 基本的使用，就是它基本上都不需要做太複雜的設定。剛剛給大家看到 Preset 的設定，不同的 Preset 設定了以後的確有點不一樣，大家也可以試驗看看每一種 Preset 會由什麼樣子的不一樣。也可以選不同的風格，像這個是畫柴犬的，那個插畫型我們剛剛失敗的，它這邊是有成功的。

[01:27:54] 如果有一些同學很會寫程式，或是說你可能因為例如之前我們也有學長回來分享業界工作的經驗，他們真的有在幫廣告公司做一個很快速的工具，可以用 AI 生成一個很快速投放廣告（例如在 Facebook 或其他平台）的圖。那它就很可能需要調教到把 Stable Diffusion 很容易依照廣告主的意識生成圖片，因為廣告主也可能下指令就像我們剛剛那麼簡潔，但他希望用程式就把它生成，然後又很容易改成廣告主想要呈現的樣貌。

[01:28:57] 所以它需要有程式去控制它。你有同樣的需求，或者你的專案就是想要做這一類的東西的話，你可能會要用程式去呼叫這些 Stable Diffusion 的模型。這開源型的模型，你可能想要去呼叫它。我們之前的作業有做這件事情，但是我們這一次沒有做。

[01:29:35] 我們真正的作業是這樣：我們準備要用 Fooocus 來創作。你可以先稍微熟悉一下 Fooocus，然後你已經決定了你要的風格就是這個樣子喔。那你就可以開始先用英文概念，就像我們剛剛那樣，用簡單的關鍵字英文來提示（Prompt），然後用 Fooocus 來創作。你的程度跟老師一樣也很好，完全符合我們的作業需求。

[01:30:26] 如果即使簡單的概念都沒有辦法呈現，當然可以請語言模型幫你翻譯。第一版就不要讓語言模型幫你擴充。Fooocus 其實也會用語言模型擴充，我們已經說過了，可是那個擴充我們先不要管它。

[01:30:47] 第二個呢，就是把你剛剛要畫的這個概念（所以你至少會有你要畫的這個概念），用 Gemini Pro 或是其他的大型模型，告訴它你要用 Stable Diffusion 去畫，所以你希望把它現在說的這個概念寫成更詳細的去描繪這個圖的景象，然後再翻成英文。然後你再用這一個當成你的 Prompt，去比較一下這兩個作品到底有沒有差異。

[01:31:28] 當然最好的方式是你把 Random Seed 固定。Random Seed 不固定的話，說不定是你形容的也不錯，但是因為原模型那一次它 Random Seed 選到不錯的，所以它畫得比較好。所以最好要固定。

[01:31:50] 最後就只有兩張作品。然後要說明你的 Prompt 是怎麼下的，就是你的簡潔版 Prompt 跟語言模型幫你生出來的 Prompt 是什麼樣子的。比較一下這兩個作品。當然啦，大家不要覺得說那我這樣子就是畫一次產生兩一張圖結束，通常不會很滿意。你真的要多試幾次，第一個你會找到你比較喜歡的風格，第二個你也會發現這樣的技巧真有用。

[01:32:28] 有一些時候你可能自己形容的已經不錯了，所以畫得已經不錯了；但是有些時候你會發現真的用了語言模型去幫忙你的時候，它真的畫得好很多，細節畫得很好，差異很大。那最好是選那樣的作品出來，看它這樣子實際上對你有沒有什麼樣子的幫助。

[01:32:57] 線上有沒有問題？對作業或是對什麼有沒有什麼問題？那我再提醒大家一次，我們已經快期末了，請大家可以認真的開始準備自己的期末專案。

[01:33:15] 我們期末專案會像是一個線上的研討會一樣。你會化身成一個很可愛的像素圖（Pixel Art）小人物，走在這個會場。到了某一位同學的作品前面的時候，就會看到他自己的簡介。

[01:33:45] 請大家的簡介裡面一定要說你是什麼學校、科系、名字，然後你做的是什麼。請大家盡量都是用 2 到 3 分鐘為原則把它講完。因為展覽的作品很多，特別是請大家設計好前面的 30 秒。聽說前面 30 秒抓不到你的聽眾，那就永遠抓不到他了。所以他看了前面的 30 秒之內覺得無聊，他就會把你關掉了。

[01:34:37] 當然會附你的投影片，如果有興趣的同學就會再去詳細看你的投影片或詳細的解說。你會發現有各個學校很多不同的作品，你就可以欣賞到很多同學的不同作品。

[01:35:01] 我們這一次會盡力讓大家在開始的時候就能知道資訊。因為上一次比較像是打開盲盒一樣，要走到作品前面等到它開了以後，才知這是哪一個學校的同學做的、做了什麼樣的主題。我們希望這一次會有一點改善，是事先就收集好大家參展的內容，我們會做一個簡單的索引。所以你發現這個你覺得蠻有興趣、想要看一看的，那你就可以直接去找到他的參展位置，然後直接去看他的作品。

[01:35:54] 這大概是我們期末要做的。也是歡迎大家，請大家就現在趕快開始做，會非常的有趣。你除了自己的成果可以發表出來，你也可以去看很多同學的發表。因為是用事先錄好的，所以你也不用一直站在你自己的這個簡報旁邊，你就可以到各處去看別的同學的作品。

[01:36:28] 有什麼問題，比方說你有一些想法了，你也不知道說你這個想法到底可能還是不可能，就歡迎找我啦，或是找學校老師、找助教討論。好，沒有什麼問題我們就到這邊。等一下我們休息十分鐘之後，就是我們今天的助教時間。

[01:36:55] （休息後，助教時間開始）好，接下來的話就是今天的習題課時間。我是今天的助教長安。今天我們的習題課要來介紹一下 OpenRouter 這個東西。一開始會簡單的了解一下什麼是 OpenRouter，接著會帶大家導覽一下這個網站，然後用一下它有什麼樣的功能。最後是做兩個簡單的 Colab 實作，會跟我們之前做過的這個作業有相關，把它拿出來用。

[01:37:32] 什麼是 OpenRouter 呢？大家應該是有訂房過吧？出去住旅館總會訂房，那可能會用那種什麼 Booking.com 之類的那種訂房網站。那我們知道那種訂房網站它應該不是自己有房產的，它是一個做統一做訂房動作的一個平台。OpenRouter 就是類似這樣子的。

[01:38:05] 所以像是我們今天就是會使用很多的模型、很多的 LLM。那今天這個平台呢，就幫我們去串聯這些很多很多模型。所以說今天我們使用者就只需要通過一套的 API 介面或一套 GUI 介面之類的，就可以串接然後使用非常多家的 API 模型服務，包含 OpenAI，然後其他的我們知名的 LLM 什麼都有，非常多。

[01:38:32] 所以說它像是一個中繼站的概念。它本身不會去做這些跑模型或是託管模型的服務，它只是將我們使用者這些的請求、問的問題、下的這些 Prompt，然後去送到實際上去跑那些模型的提供者。所以它只做一個傳送的服務。

[01:39:00] 使用它的好處就是我們開發者不用去個別申請每家公司的 API。像是如果今天用 Google 的，那我就要申請 Google；那如果今天要 OpenAI，那就要申請 OpenAI 的。那如果今天是使用 OpenRouter 的話，你就不用每家去申請。然後連程式碼都不用去做很大的修改，依賴套件是用原本的（如 OpenAI SDK），只需要將 Base URL 改成 OpenRouter 的網址，就可以在不動程式碼的方式下，去切換使用不同的模型。

[01:39:41] 接下來就是來導覽一下這個網站。我們就在搜尋框搜尋 OpenRouter，點進來。然後可以看到它首頁這樣子。我們可以去登錄，我想直接再建個帳號，用 Google 帳號去創建個帳號，也很快就進來了。

[01:40:36] 接下來的話可以看到這邊有一個 Models（模型）。這邊的頁面會顯示說這個網站它所支援的所有模型。接下來我們主要會使用到是免費版模型，所以可以在這邊打個 "free"，就會去篩選說這些免費模型都是我們之後可以去使用的。

[01:41:07] 它的左邊這邊的欄位可以去看說是限定說輸入是文字的、輸入是圖片的。然後可以發現免費模型的話就會比較不支援這些影音相關檔案。

[01:41:33] 這邊有個 Rankings（排名）也蠻有趣的。就這個地方可以看到一些排行榜，統計說這些模型被使用量多少。像這個是 Token 的使用量，這是一週一週在記的。可以發現上週 Token 使用量最高的模型是 Llama 3，然後第二個是 Claude。可以發現現在使用者對於這個 AI 的使用依賴性越來越高，Token 使用量呈現一個上升的趨勢。

[01:42:28] 下面有一些其他的排名，像是市場的佔有率。然後下面會有些分類，像是比較多人用 Claude 寫程式這樣。這可以切換，隨便點個像是 Marketing 好了，發現最多人是用 MiniMax 這個模型在做這件事情。

[01:43:13] 這個是語言，說現在是多少人是輸入哪一個語言然後去使用這些模型的。像英文的話最多人是還是 Llama。如果是中文（繁體中文）的話，最多的是 Gemini。這就蠻有趣的，大家可以看一下這些統計。

[01:44:13] 接下來我們發現這個帳號是我有建立過的。對，它會有一個要同意條款，同意之後就會進來。然後就到 Chat（聊天）這個地方。那 Chat 這個地方都蠻有趣的。像我們現在隨便點個 Face Coding Model，幫我寫個 Python 的小遊戲。這樣它就會跳出說你沒有 Credits（額度）。

[01:45:31] 所以我們來設定一下隱私權設定。因為我們這邊是沒有綁信用卡的，也就是不會被扣錢。我們想要用免費版模型，所以我們需要去設定一些隱私相關的條件。因為說我們現在是在使用一些免費版模型，所以那些模型方相對會希望跟我們有一些要求。像是說讓一些模型可以使用我們的輸入資料去訓練，或是公開我們的 Prompt 當訓練集之類的。如果沒有設定這些東西的話，我們就沒辦法使用免費的模型。

[01:46:25] 我們就來尋找，這邊最右邊可以點去這個 Settings，然後點到這個 Privacy 這邊。那我們就把第二個、第三個都打開。這邊寫說就可以讓免費模型去看到我們的輸入、公開我們的 Prompt。這兩個把它打開。

[01:46:53] 打開之後我們再回來，再問一次。它跟你說誒不行啊，就沒有 Credit 怎麼用呢？好，那這邊是因為我們這邊選到的這些模型都是要錢的，所以我們沒辦法使用它們。所以就把它們全都關掉。

[01:47:20] 關掉之後這邊會有一個 Add Model，那我們就點進去。點進去之後我們可以打這個魔法的密碼，就是打 "free"。所以出現一些 free 的模型可以使用。那我們這邊就隨意選幾個模型來玩。好，我們就選個四個好了，選個不同家的。

[01:48:18] 然後再問（同樣的問題）。然後這個時候就會有很神奇的畫面，就同時間會有四個語言模型幫你做你的處理你的需求。可以發現有些模型它在處理得比較久，有些是處理比較快、就已經處理完了。大家可以再自己去嘗試一些他們給的程式。

[01:49:55] 反正每個的風格都不太好。那我們再來試一個，我們請 LLM 幫我們寫詩好了，來看看他們會回答什麼酷東西。然後這個右下角是有時候他們會跳一些錯誤，像這個是說用太頻繁，然後它可能叫等一下這樣，這就還好小問題。

[01:50:23] 好像誒，它都寫得很快，馬上就出來了。然後看起來好像一首詩也沒什麼問題。好，那接下來就來看一下後面的要說明的內容。那這邊大概看完網站的，那接下來我們來寫寫程式好了。

[01:51:38] 我們後面的程式時間會做兩個 Colab 實作，然後會讓大家去玩一下說 OpenRouter 這個東西它其實是怎麼使用 API Key 的，然後把它寫進去。那我們就可以去使用不同的模型來完成我們想要做到的任務。

[01:52:21] 今天的內容跟之前那個作業一樣，做這個 AI 代理（Agent）。這程式我新增一個功能是可以去選擇模型，然後就是綁定 OpenRouter 那邊，可以去選擇這些免費模型可以用。像是這邊隨便選個中間這個，那就可以生成。

[01:53:57] 接下來我們就來看一下怎麼去做到這件事。我們在左邊的地方有加入我們的 API Key，它是一個名為 OPENROUTER_API_KEY 的 Key。那只要怎麼弄呢？我們回來 OpenRouter 的網站，在 Key 的地方點進來，然後我們可以 Create 一個 API Key。

[01:54:34] 就取名隨便取 "First Key"。這個如果有課金的話就可以輸入一個上限的金額，那我們沒有所以就沒關係。然後最後一個是有効的過期時間，就先隨意放個 30 天，然後 Create。我們就獲得一個新的 Key。然後把它複製起來之後回到我們的這邊，就可以把那個 Key 貼到這裡（程式碼中的 secrets 或是環境變數）。

[01:55:25] 那我簡單去講一下我做什麼改動。這邊改動的話，我就將下面程式碼加上使用 OpenRouter 的部分。那這個地方是比較重要的，就是這裡去列出到現在的免費模型有哪些。那找那個地方就是在 OpenRouter 網站搜尋 "free" 之後會出現這些模型，然後我們就隨便點一個去複製它的 ID。這一行就是它模型在程式裡面使用時候需要的 ID，把它複製下來，然後把它貼上來。

[01:56:22] 所以這邊是可以讓大家去更改的，就之後大家想要試哪些免費模型，就把它加上來。後面的部分就不用理它。

[01:56:47] 那這邊我們來試一個狀況。像今天我們試這個，換成第一個模型，然後我們按生成貼文。怎麼出錯了？這會報 404 Error，跟你說沒有這個、不能使用、沒有 Endpoint。這個問題就很簡單，這就代表說這個模型現在不能使用。因為 OpenRouter 它會去抓現在可以用的模型，然後這些模型可能到未來某一天就會不能使用，它可能會被拔掉或者改版本，或者是變成要付費。所以我們看到那個問題之後，我們就可以把它改掉（移除掉）。

[01:57:52] 那我們來看另外一個程式，就一樣是去改這個 AI 代理的事情。這邊也是一樣，就是我們選擇五個免費模型。然後我們在這邊可以選擇兩個模型，因為這個程式就是可以選一個負責規劃，然後另一個選擇貼文的模型。可以兩個都選不一樣，這樣覺得很有趣。

[01:59:03] 換這兩個好了，然後按生成。看一下它... 最沒錯，「今天的我與未來同樣在雨絲的氛圍裡笑得很開心」。我們來換別的模型來，生成的比較快啊，它就跑得沒那麼久。但發現這個模型雖然產生比較快，但是會躲藏一些簡體字這樣。所以大家可以從這些模型去研究一下說每個模型它都會有一些不一樣的輸出，然後可以從這些模型中選擇說比較好的。

[02:01:29] 那我們簡單來看一下說這個程式我改動的地方有哪些。一個的話就是前面這樣子。一開始所說嘛，其實不需要改動很多地方，只要改這個 Base URL。把這個網址放上來，OpenRouter 的網址（https://openrouter.ai/api/v1）。然後環境這個 API Key 也是一樣的，就改成 OpenRouter 的 Key。然後免費模型的清單就這些。

[02:02:43] 像最後 GUI 這邊呢，我們就其實是很簡單去加上兩個這個下拉式選單。所以大家可以從這個下拉式選單去選擇模型。

[02:03:27] 那最後是展示一下，這邊 Settings 裡面會有一個 Credit，然後這邊寫個 0.0 的。那這邊其實不用擔心會被實際扣錢，因為這個是用我一開始第一個在試的帳號在玩這個東西的時候發生的。後來我去查為什麼，他們是跟我說這個是一個預測，就說如果實際上使用這些 Token 是在要付費的模型上的話，可能花這些錢。但因為我沒有綁卡，所以不可能會被扣錢。

[02:04:43] 好，然後今天助教課展示部分的東西差不多就是這些。我們今天的影片跟程式會同時放在 NTU COOL 上面，然後同學待會就可以去看。

[註] 第三部分整理至約 02:08:17（助教時間結束），涵蓋了 Fooocus 風格演示、作業與期末專案說明、以及 OpenRouter 的介紹與程式串接教學。這部分已包含到影片的結尾。

這部分已經是影片的最後一段。整份逐字稿整理完畢。
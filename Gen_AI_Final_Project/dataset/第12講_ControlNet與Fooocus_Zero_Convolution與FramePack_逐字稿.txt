【生成式 AI】12. 深入圖像生成 (Part 1)

時間範圍： 00:00:00 - 00:39:41

00:00:00 - 課程開場與 ControlNet 介紹

[00:00:02] 大家好，那我們今天開始的是生成式 AI 的部分，我們會大概對圖像的生成做一次總結。所以今天我們就是要對圖像做總結了，我們會介紹一些前面還沒有介紹到的技巧。上一次之前我們其實大部分都是在跟說明圖像生成，特別是用 Diffusion Model 這種擴散模型，它的原理是什麼。主要的應用方式就是很直接的用它標準的使用方式，雖然我們有介紹說你可以選不同的 Scheduler 或是其他不同的設定，但是基本上還是比較標準的使用方式。那我們今天會介紹給大家的就是，如果你要用一些比較進階的使用方法，到底還有哪一些技巧。

[00:01:09] 所以我們這邊要介紹一個就是 ControlNet。那這個 ControlNet 呢，簡單的說，在 Stable Diffusion 出來之後有一個很重要的就是 ControlNet。因為在 Stable Diffusion 出來雖然我們可以用文字去控制它，但是其實不是很精確。比方說我就是想要某一個人做某一個動作，我很難很精確的要求我們的圖生出來就是那個動作。比方說你今天想要模擬你去幫人像外拍的時候的樣子，可能你在網頁上找到了幾個 Model 覺得說應該要這樣子的動作適合，但是你很難把自己要生成的圖調成那樣子的動作。那你就會發現即使我們英文再好，再會形容或是用 ChatGPT 幫助我們，都很難達成。

[00:02:13] 所以呢，ControlNet 基本上就是說我今天可不可以對我的圖像去做一些更深刻的一些動作控制。ControlNet 是張呂敏等人他在 2023 年提出來的，而且他在提出來的時候其實還是非常的年輕，應該還在念研究所或是甚至還在大學我忘記了。他是 2023 年提出來的，那因為 Stable Diffusion 成名的開始的時間點也差不多跟 ChatGPT 差不多，在 2022 到 2023 左右，所以這個它大概就是在那個時候就已經提出來。它是特別是可以去用我們在用開源的 Stable Diffusion 圖像生成的時候，它的一個很好的擴充架構。

[00:02:58] 今天我們其實也會用到 ControlNet，雖然我們不需要知道它到底怎麼做出來就可以用，但是大家可以從這邊稍微的知道 ControlNet 的概念。順便介紹一下，張呂敏其實就是 Fooocus，我們用的 Fooocus 的作者，他其實是同一個人。他做了很多很多有意思的東西，都可以直接讓大家去使用。

[00:03:29] 那這個 ControlNet 最基本的就是我們剛剛說到的，它很希望能對圖生生成的圖像做更多的控制。比方說剛剛說過我可不可以規定它說這個動作就是要這樣子。再來就是讓它描繪，可能我們本來畫的是一個手稿，然後讓它變成一個真實的照片的樣子；或者是說我今天就是想要生出這樣的照片，我要重新的用它的線條去描繪那個樣子等等。簡單的說，對創作者來說，它就是在圖像上面裝了一個方向盤一樣。

00:04:21 - ControlNet 原理與應用範例

[00:04:21] 所以 ControlNet 其實大概是這樣子，如果我今天有左邊的這張圖的話，那我也可以下 Prompt 去說這張圖是描繪什麼東西，然後它就會把這個裡面的情景給再描繪出來。所以這個就是描邊的圖，然後它可以產生。那 ControlNet 它做的更完整的是它也幫我們把描邊的工具也做好了，所以像這張圖事實上真實的照片是左邊這張照片。那邊緣偵測的時候，就是可以把它那個描邊描出來，所以我們就可以再去做修改。那你會發現跟原圖當然會有點不一樣，也就是說如果你覺得原圖有一些要修改的地方，你就可以去做修改，但是大致是一樣的，因為它就是描邊圖嘛，它不可能再做更大的一些變化。

[00:05:23] 所以 ControlNet 的簡單概念其實也不是真的這麼難。如果我們真的很了解前面介紹的，不管是我們把文字的意涵給混進去等等的那些技術的話，其實概念很接近那一個。它就會把原來的 Stable Diffusion 基本上把它給控制住。現在我們左邊的是原來的 Stable Diffusion 的一個區塊，這基本上是完全的給它控制住（Lock），然後我們再拉一個複製的版本在外面，讓它再去學習。那這些學習的時候，我們會把 C，其實 C 就代表 Condition，代表我們要放進去的條件。也就是說在最右邊的這一邊，我們就是想辦法讓它去學這個條件到底是什麼意思，最後可以生出我們想要生出的圖樣。

[00:06:35] 它也可以做一些我們前面的設定。比方說我們知道說在混進去的時候，它的強度要多少，它可以去做設定。有時候我們 ControlNet 跟我們下 Prompt 一樣，雖然都是我們希望是長這個樣子，但比方說我們有時候希望說它不要完全照著描邊的圖去畫，它可以稍微放寬一點點，所以我們可能會覺得說 Prompt 的強度或是 ControlNet 下的強度可以去做設定。第二個呢，它也是一樣，它是一次一次混進去，所以我們也可以在某一個地方就停止，讓後面就不要再囉嗦了。就是它畫到某一個階段的時候，那 ControlNet 效果就可以不要有。我們等一下就會用到 ControlNet 的一些技巧。

00:07:26 - Fooocus 實作教學：Advanced 設定與 Upscale

[00:07:26] 所以我們等一下要用 Fooocus 去做 ControlNet。畢竟 Fooocus 就是 ControlNet 原創作者做的，所以它當然可以讓我們用 ControlNet。那我們就直接進入實作。如果說自己已經安裝好 Fooocus 的話，或是像上次我們介紹的方法，你在 Colab 上面執行 Fooocus 都可以。那我們現在當然是偷偷摸摸的已經在一個架好的地方，我們就連上去使用它，免得佔了太多時間。

[00:09:15] 那我們會發現說這個上次我們在下面的幾乎沒有用到，除了 Advanced。那 Advanced 這邊打開來的時候就是做了一些基本的設定，我們上次基本上已經介紹了。我覺得最重要的是幾個啦：一個是在 Style 那邊你可以去選不同的風格；然後第二個是在 Settings 這邊，大部分你其實都可以不要動，除非有一些比較特別的。比方說你覺得畫出來的品質沒有這麼好，你希望品質再高一點的話，你可以選 Quality 的版本；或是你覺得實在太慢了，可不可以再快一點，你就可以選 Speed 這兩個版本等等。或是 Aspect Ratios 這邊你可以去設定長寬，寬跟高，這邊還有寫相當親切的寫寬跟高的比例。你希望看成那個長型的、或是那個像手機照相的那種形式、或是橫過來的一般照相機照的形式都可以。雖然它的設定很多，但是其實你用久了，你就會知道你都是固定用幾個。再來就是我們前面有說過的那個輸出的圖的數量（Image Number）是多少，那我們現在都設定 2，也讓那個進行的速度比較快一點點。

[00:11:03] 然後你要做仔細的控制的話，這邊是 Random Seed，看你要不要用 Random。那我們現在沒有要做上次的那個控制，所以我們就直接用 Random 就好了。好，這個說完了，這個是 Advanced。

[00:11:16] 比較重要的是我們覺得今天我們要重新開啟一個新的項目，就是有一個 Input Image 這邊，然後我們就按下去。第一個部分比較不需要解釋，它就是要幫我們放大的時候（Upscale），或者是說我同樣的 Prompt 之下，我想要再嘗試著做不同的版本出來（Variation）。這個就是這邊的功能要做的事情。我們順便說一下，如果你實在不知道這邊在幹嘛的話，Fooocus 我覺得寫的非常好，它的 Documentation 就是在解釋這個 Section。它其實就是模擬，它希望模仿讓 Fooocus 整個用起來感覺就跟 Midjourney 一樣順。Midjourney 本來就是可以讓你做一些 Variation，就是說你覺得第三張圖你覺得不錯，但是好像希望在它再根據這個再去做一些修改，那你就可以選它去做一些修改或是做放大等等的。

00:13:00 - Image Prompt：以圖生圖

[00:13:02] 重點是第二個 Image Prompt。Image Prompt 顧名思義就是 Image Prompt，廢話。因為我們前面的 CLIP，如果大家還記得 CLIP 發生的事情，就是 CLIP 告訴我們說我今天要把那個文字產生出來的意涵向量，要跟那一張圖形——如果那一個圖就是用這個文字去描繪的圖，也就是說那一張圖的意思就是一樣的意思的話——他們兩個的代表向量要非常的接近。所以這給我們的一個啟發，這個啟發就是說如果我真的想要做出一個圖形的話，我真的要去描繪一個東西的話，我也可以不用下文字，我可以給一張圖給它。

[00:14:04] 例如說，我很想要畫一個風格的圖。比方說我現在打開給大家看，假設我今天想要畫一個類似這樣的風格（展示圖片），就是覺得說我想要做一堆的小圖貼，然後希望長得是這樣子的風格。那我們來試驗畫看看說可不可以，因為我也不知道怎麼形容這個風格，你叫我去描述它，我也不會描述。那我就說好，就是要畫這種風格的。所以我就用這個 Image Prompt 當做我們的 Prompt，就是我文字不會描述，所以我用一張圖給它。所以我就把這個圖拉進去。

[00:15:28] 然後因為它是告訴我們寬跟高，所以我們要寬版的。好，5 比 3 就好。我們先來生成一個很多個像這樣子，好，就是一個可愛的女孩（A cute girl），然後 with... 我們要不要規定是亞洲的女孩？然後不同的動作（different poses）。好，就這樣吧，生一下。

[00:17:21] 我現在要生出一堆小圖貼啦，就是要有 Collection 的小圖貼。但是你就會發現它是生出了一個女孩子，應該是同一個人吧，然後可是那個生出來的很多個，第二張又沒有很多個。不管怎麼樣，你就會發現說它為什麼都沒有照著我們的圖？那原因是因為在這個 Fooocus 裡面，它的預設的設定都比較是畫這種比較寫實的相片的樣貌，所以你就會發現說它畫出來的好像怪怪的。那我們就在 Style 這邊，我們就可以把它全部去掉（取消勾選預設風格），因為它本來的設定都是比較指向那個相片的風格。那我們就希望說它可以幫我們畫出對就像這樣的圖像。

[00:18:28] 所以你就會發現說，我真的可以用這個一張圖去代表它的這個風格，然後把它畫出來。但是呢，你會發現一件事，這個跟我們原來的風格不一樣啊，我們原來風格是這樣，所以它會稍微的變化一下下。所以說你也沒辦法完全說它照著我們的風格去畫，我們其實還可以做一些設定。

[00:19:16] 我們還可以再試幾次這件事，就是你把這個 Image 放進去的這邊，你會發現有個 Advanced。這個重點來了，Advanced 勇敢的按下去，你就會發現說它可以設定我們剛剛說的這個 Weight（權重）要設多少。如果我們 Weight 設 2，還可以比 1 就強調再強調。那我們不要那麼誇張了，1 就好了。那我們 Stop At 應該也是 1 吧，對，1 沒錯。我們把剛剛的全部都告訴它，要用最大的聲音告訴它就是這種風格，不要再給我亂變。然後呢，我要一直提醒它到最後的結果。

[00:20:23] 我們來試試看說這樣畫出來有沒有比較像。好像稍微的更像一點點哦，更接近一點點。當然說不定你比較喜歡剛剛那一個。對啊，真的比較像剛剛的那一個，就是他的人物會更抽象一點點，這我們原來給他的圖是人物更抽象一點點，所以它好像更是符合了我們原來的樣貌。所以你就可以去做很多的一些這類型的設定。

[00:21:04] 所以這個是第一個 Fooocus 一個很有趣的地方，就是我們可以去做一些這個圖像的一些設定。Image Prompt 意思就很明確，就是要用圖去產生我們的 Prompt。

00:21:43 - Face Swap（換臉）與 PyraCanny/CPDS

[00:21:43] 然後第二個呢，我們現在再來看看。Image Prompt 不只是可以一張，沒錯，它可以好幾張。就是有一些圖是告訴他動作應該長什麼樣子，有一些圖應該告訴他說它的風格是什麼，等等。所以它的圖可以有好幾張。

[00:22:28] 第二件事情我們剛剛也勇敢的按下面的 Advanced 這邊，我們發現說每一張圖都可以去做一個設定。第一個設定呢，就是我們 Image 本來就是可以當 Image Prompt。那後面的其實都是那個 ControlNet。第一個呢就是設定好說我今天要做描邊，描邊的圖（PyraCanny）。第二個呢是這一個 CPDS，雖然它不是直接的做那個動作的控制，但是簡單的說它就是做動作控制，你可以用這個去做動作的控制。第三個呢是非常有趣的 Face Swap，好像換臉，它其實不是真的換臉，意思就是說我今天指定的這個人的臉的樣子，我就要把它放進去到我的這張圖裡面。

[00:23:44] 那我們先來試驗換臉這件事好了。換臉就是說我今天找到一個人的照片，然後呢，我就是希望等一下裡面那個人就是長得就我這一張照片指定的人的樣子。未來是不是就可以把我們自己的照片放下去，然後產生很多我的... 比方說我要換一件衣服啦，然後我想要控制我去做什麼樣的動作啦等等。

[00:24:51] 現在我們就做來做換臉。好，這個我們準備要犧牲掉我們的計劃助理。大家不要告訴他這個今天發生了什麼事情。然後就是我們就是拿他的照片，然後我們今天希望其實是說我們等一下形容的，不管形容什麼，然後畫出來的就是出現的照片就是它在裡面這樣子。只是做的事情可能不是像現在就拿著一包餅乾這樣子。

[00:25:55] (Prompt): using his computer in a cafe.
[00:26:11] 那換臉的本來的動作就是我們選擇是 Face Swap，它本來的意思就是我剛剛講的應該要把它的臉換過去，所以應該我們就會看到它在裡面。然後你會發現這個半點也不像，有點像了，但後面又開始不像了。好，我們可能 Stop At 太早了？我們 Stop At 有很早嗎？沒有啊，0.9 啊，已經很晚了。算了，你就會發現這個好像不太像。

[00:26:51] 我們先看一下兩個人有沒有像？就他自己生出來兩個人好像是有點像，但是跟原本的人不太像。那這個原因其實很簡單，就是因為這一個 AI 沒有學過畫這個人，所以他這個畫出來人其實沒有那麼像。現在的 AI 其實已經可以畫很像，但是我們先不要管它，就是原來的 Stable Diffusion 裡面用 Diffusion Model 我們去做這件事情的時候會發現說有一些限制。就是我們雖然已經跟他講就是換臉了，但畢竟他沒有學過這個人，所以他去掌握的能力沒有那麼好。所以他畫出來的時候，他自己以為已經畫得很像了，但是畫得不太像。

[00:27:39] 可是呢有一個好處，就是比方說我今天就拿這一個人，好，那我們就再換成這個人。然後這一個人呢，因為呢他是 AI 畫出來的，所以呢原則上，不是，事實上也是啊，因為是 AI 畫的嘛，意思就是說 AI 畫得出這個人來。所以呢，如果你今天指定的人臉是 AI 自己畫出來的，它就比較容易做的一致。

[00:28:18] 我們再來試試。Prompt: In front of a park.
[00:28:41] 目前好像你就會發現說他真的有那個兩個人的樣子是比較接近的，就是說他會比較穩定的畫出這個人的樣子，因為他是他畫出來的人。所以說呢，我們要讓角色穩定在 Stable Diffusion 裡面，要么你就自己去訓練一個 LoRA，就是大家會看到外面很多人去訓練一個 LoRA，就是想辦法讓 AI 會畫這個人。但是畢竟 LoRA 要用的那個資源還是蠻多的。如果今天你沒有堅持就是要產生這一個人，或是就是要產生你自己的照片，你只是要有一個例如長得很像你或是從你身上得到啟發讓 AI 畫出來的人的話，你只是要比較固定的角色，你就可以用類似這樣的技巧去固定這樣的角色。

00:29:54 - 多重 Image Prompt 與動作控制

[00:29:54] 那我們在這邊來介紹這個，因為我們剛剛就說過說這個可以放很多張圖當做 Image Prompt。所以我們就再來放另外一張，然後去試用一下我們以前沒有用過的功能。

[00:30:41] 好，這是一張專輯裡面的，然後我們就是為了怕侵犯著作權，所以我們今天就把臉給換過去，但是我們希望那個動作就長這樣。那我們剛剛說過這個基本上是要控制它的動作（CPDS/PyraCanny）。因為是比較長的圖，我們還是稍微把我們的版面修成比較長的版面（Vertical）。

[00:31:34] 所以我們現在同時用了兩個 Image Prompt。第一個是我們要 Face Swap，意思就是說我們要指定這個人的臉孔；那第二個呢，我們是要指定這個的動作（設定為 CPDS 或 PyraCanny）。這個看起來真的有夠怪的，好，詭異。但動作的確是這樣的動作啦。有時候還是需要描述一下，不然因為 AI 有時候看不懂那個圖裡面是什麼東西。但是你會發現動作就是這個動作，然後人就是我們指定的人。

[00:32:16] 然後你也會發現如果我們所有的 Prompt 都不指定的話，會有點奇怪，會跟原來的風格完全不一樣，因為我們只指定的動作，然後我們只有說這個人是要長這個樣子。

00:33:06 - 描邊（Canny）功能實作

[00:33:06] 那唯一的我們還沒有介紹的就是描邊（PyraCanny）。不過描邊其實大家可以想像。那我現在又要這一個新的挑戰。這張圖我們完全是 AI 生成的，這個應用數學系，這個是我們真正的在我們政大應用數學系的系館前面照的相片。然後請 AI 生成，但人物是假的。不管了，反正我們今天就是想辦法要生成一個這樣的圖片照片。

[00:33:38] 所以我們再一次就在 Image Prompt 的中間呢，把這一張照片拿進去。我們設定一下 Aspect Ratio 5:3。反正我們就畫一張這樣的照片，然後我們就要照著畫出來。那所以說呢，我們就打開這個 Advanced 這邊，然後呢，我們現在選擇就是描邊（PyraCanny）。就是我要照這張圖描邊出來，然後看我們有沒有很堅持它就是要長這個樣子。

[00:34:53] 雖然 Stable Diffusion 基本上只能接受英文，但是現在我們知道我們可以用大型模型，可以讓大型模型去把我們的句子翻成英文，或者是讓它修飾，讓 Stable Diffusion 更清楚要畫什麼。
Prompt: A robot using a laptop.

[00:35:49] 在開始的時候，因為我們會注意到我們那個 Stop At 大概在 0.5 的時候就結束了。所以在開始的時候，他好像比較像他要產生我們本來指定他的應用數學系，但後來就看到大家好像很熟悉的在用 AI 生圖的時候常常會出現的奇怪的不知道哪一個世界的文字。所以因為它是描邊，所以會發現說他真的畫得有點像。

[00:36:28] 那我們當然也可以想說，那他剛剛沒有畫得這麼好的原因，是不是因為呢，我們讓他就是跟他說只有再做到一半的時候，我們就跟他說後面就不要管那個原來的描邊的控制，我們就把它放棄，就沒有再告訴他就是要照著描了，所以它最後可能會有點跑掉。那我們是不是可以來一個 Stop At = 1，就是完全全的告訴他說，從頭到尾都一直很囉嗦的告訴他就是要這樣畫。我們再試一次。

[00:37:08] 哦，這個比較嚴格的，它連衣服上的圖樣都是一樣的。那你可能會覺得奇怪，那為什麼顏色不一樣？顏色不一樣的理由很簡單，就是因為我們只有描邊給他。

[00:37:39] 第一個他以為前面又抱著一個袋子，就是苦命的學生，這一個書包還不夠，前面還要再抱著一個袋子。那那個字稍微像一點點，但是也看不懂。因為那個 AI 沒有真的去了解到底是什麼東西，所以他生出來的沒有辦法生的這麼好。然後第二個就看起來有一點點樣子的字，然後但當然還是亂碼的字。然後因為我們指定描邊是從頭到尾都很囉嗦的告訴他就是要這樣描，所以它看到的樣子，它就會依它看到的。它這次看得比較對，就是一個機器人的圖樣的造型，然後它就把它畫進去放在那個地方。

[00:38:29] 那你也會發現如果是描邊的話，你就可以跟它再一次哦，即使是描邊，你會發現它描出來的人還是長得不太一樣。因為我們雖然剛剛那個圖也是 AI 畫的，但不是 Stable Diffusion 畫出來的，所以 Stable Diffusion 其實沒有這麼會畫那一個人。所以他畫出來的人其實雖然已經給他描，他還是沒有辦法畫的這麼像。那可是呢，因為這個是就是 Stable Diffusion 畫出來的這個人物呢，如果你覺得說這樣子畫的還不錯，你想要用這個固定這樣的角色的話，你就可以讓想辦法讓它固定這樣的角色。

[00:39:07] 好，這就是我們今天本前段要為大家介紹的，就是我們之前還沒有介紹完的那個在 AI 裡面那個比較重要的一些應用。好，那不知道目前為止有沒有什麼問題？其實還有一個我更常用的應用。大家現場有沒有同學有問題或是線上同學有沒有問題？可以嗎？OK 沒有。好，那我們現在來介紹一個我常常做的一個應用方式。

【生成式 AI】12. 深入圖像生成 (Part 2)

時間範圍： 00:39:47 - 01:17:37

00:39:47 - Describe（圖生文）與 Inpaint（局部重繪）

[00:39:47] 好，那我們現在來介紹一個我常常做的一個應用方式。就是我們剛剛已經做完了，我很懶惰的全部洗掉，就是重新把它整個頁面再讀進來。那我們剛剛看到 Input Image 這邊還有好幾個啦，其實後面的比較... 它的 Describe 就只是說，我今天如果我真的不是很會形容這張圖，我又很想他形容這張圖的話，我就可以把一張照片丟進去，然後讓他形容給我。那老實說這些，因為我們剛剛前面說過，它其實你也可以用那一張圖就直接把那張圖的意思直接放進去就好了，也可以。

[00:40:34] 那為什麼我們一定要看到它的形容的樣子呢？那是因為形容的樣子我們比較知道說，他到底有沒有誤會我們的重點。他說不定誤會了，所以我們可以看。第二個是我們還可以去修改。所以這個是那個去形容它裡面的圖的樣貌的時候，我們不如就試驗一次。我們就找一張圖來示範一下，我決定要再度的犧牲我們的專任經理。好，所以我們來看一下，就是今天可以告訴他這個是 Photo 型的還是什麼什麼，然後也要告訴他說這個風格，要記得風格要去形容，所以我們就來開始看一下形容。

[00:41:27] 他其實形容的很簡單啊，就是 "is holding something in her hand"。這個畫出來應該很可怕吧。好，就這樣子。所以你就會發現它的形容其實是很粗略的，它形容其實很粗略，所以這通常還需要再去做一些修改。也就是說他在從這張圖裡面，他只看得到這一個，他從這張圖大概他就看到這個。所以在 Image Prompt 的中間，他大概就只有看到有一個女性，然後他是拿著一個東西在他的手上，大概這樣子。所以如果我們直接用這個當做 Image Prompt 的話，它大概就會出這樣的圖像，不是像這樣照片的圖像，是像剛剛上面這個文字描繪的圖像。好，我們先不要管了，因為這不是我們要做的事情。

[00:42:22] 那我們要介紹的是另外一個很有趣的叫做 Inpaint 跟 Outpaint。不如我們就再來一次，我決定我們這個實在太犧牲了，犧牲了好幾次。Inpaint 跟 Outpaint 呢，簡單的說，就是如果今天你生出來的這張照片實在太小了，就可能但是你想要做你的投影片的背景，比方說這樣，但是你生出來的是方格型的照片或是方格型的圖像。可能用的一個 AI 軟體，它真的讓你非常的滿意，可惜它就是太小了。其他的你全部都很滿意，那這樣子的話，你希望能構在往左邊、往上或是往下，然後去把它擴大，這種叫做 Outpaint，就是往外畫。

[00:43:17] 那 Inpaint 是什麼呢？我們現在來直接用實例去說明。Inpaint 的時候呢，你也會發現說我可以把一些東西給擦掉。比方說你今天就不想要讓它拿著，這個是一個餅乾，就把它畫掉，就畫大一點。好了，我們決定要讓它拿一... 你也不用畫得太仔細，就是該塗掉的地方塗掉，可以讓它重新去創作地方就重新創作一下。好，就這樣。好，就是我現在把它該塗掉的地方塗掉。其實前面的就不用說了，因為它不會重新畫，Inpaint 就是只有那個塗掉的地方，它會重新畫。所以很多人在這個如果 AI 生成的有局部你覺得不太滿意的地方，你就把它重新畫一次。

[00:44:25] 好，就這樣子，就是拿這一本書啊。那我來試驗看看，最常用到就比方說手指畫錯了，畫出了很多根手指的時候，你就讓它下修一條。這個書也太袖珍了，結果手指反而變怪了，因為他不太會接。好，但是至少他有照著我們的意思，那所以你可以讓他去拿別的東西，拿一個杯子或者拿什麼東西這樣。袖珍版的書，好，因為我們可能讓它發揮的範圍太小了，如果發揮的範圍再大一點點，它可能會畫得比較好一點點。好，所以這個就是我們可以用的小技巧，一個是 Inpaint。

00:45:19 - Outpaint（擴圖）實作

[00:45:19] 然後其實我更常用的小技巧是 Outpaint。好，我們來示範一下為什麼 Outpaint 是一個更有趣的一個小技巧。我們等一下我們會解釋。那我們先進到我們可愛的... 我們就想要用 Gemini 來生成一張圖，建立圖像。要生成什麼圖啊？一個可愛的機器人在用筆電。我發現我的生命的詞彙跟形容的事情都很少，真的太悲傷了。好就這樣子。好，就一個可愛的機器人在用筆電。那你可去形容它說他用什麼風格去畫。要什麼風格？3D 的卡通風格。就 3D 卡通風格啊。好，我們來試畫看看。

[00:46:45] 好，就這樣子，然後它就畫出一個這樣的... 好，就這張機器人。好，我們就要拿這張機器人，可是我覺得說這張機器人這個太小了，我比方說我剛剛說的，我想要做一個投影片的畫面，然後我希望它寬一點。那這樣的時候呢，就非常的適合拿我們找我們的 Fooocus 來協助。然後就 Input Image 這邊，然後我們剛剛看到的 Inpaint 或是 Outpaint。那我們剛剛做了，那其實 Outpaint 是更方便使用，就是這張圖太小了，想要擴充一下。然後它就會跟你講說你要從上下左右擴充，哪一個方向去擴充。

[00:47:44] 那我們現在只要往右邊擴充，那我就這樣，然後就讓他生成。我的 Prompt 也可以什麼都沒有，反正他就是現在就有一點點像我們可愛的 ChatGPT，他就要接下去說了。他只是說的不是話，他是要說出一些圖，不是文字，是說的東西其實是一堆一個圖像，他就要很合理的要延續。我有點忘記原來的長什麼樣，長這樣，他為什麼一定要有一個人出現在這邊？

[00:48:41] 好，那就這一張好了。那比方說你覺得說這個擴充好，擴充還不夠多，我們就可以再來，就把你剛剛選擇覺得畫得比較好的，就再拉一次。然後呢，一樣往右邊，然後它就會繼續擴充。所以這個就是你可以用其他的 AI 出來的一張圖，然後你覺得不錯，只是那個格式不符合你的使用，那我們就可以用這種小技巧，讓它變成伸出來一張那個比較大的圖。這個他原來是在畫一位老... 其實本來是沒有要他畫人啊，那當然我們可以用那個再用那個 Inpaint 把它給修掉，就把那個人物修掉也可以這樣子。

[00:49:31] 但是我們現在就知道說，哦，我們可以做這樣的事情，然後就會發現說他可以很自然的去擴充。這這個風格真的很不合啊，這右邊幹嘛那個寫實的人物出來，但是這個都是可以修的。或是說一開始的時候，我們就告訴他沒有其他人，只有這個可愛的機器人在這個房間裡面。好，那我們先休息 10 分鐘，等一下我們再繼續。

00:49:58 - 課程回顧與大型語言模型生圖導論

[00:49:58] 好，那我們上一節呢，就為大家介紹在 Stable Diffusion 裡面，特別是用 Fooocus 的話，你要怎麼樣用一些比較進階的功能，讓你對整個圖會有更多的控制，或是有一些你想要修改的部分。不管是變在那個範圍再更寬一點點，或是說你內部有一個小細節，你想要再把它改一下的話，那你都可以去做一些修改。

[00:50:50] 那個 Input Image 這邊呢，那這邊就有介紹 Input Image，那你就可以用這個直接去形容。Input Image 裡面的它其實除了 Image Prompt 之外其實有那個幾項的功能，我們剛剛其實有大致的介紹過了，所以大家也可以去參考。特別是在 Image Prompt 這邊呢，就是我們可以直接用一張圖去代表我們要描繪的東西。那除了說我們要他的風格之外呢，其實我們也可以像剛上節有說過，我們也可以就照著這個邊來去做描繪，或是照著它的動作，或是照著這個人物去畫。那我們也有講過說它的一些限制，比方說人物其實不會畫的真的很像，但是他畫出來的那個人，他要繼續的把這個同樣的人再畫出來，他是比較做得到的。

[00:52:04] 好，那我們現在呢要介紹一個很有趣的一個主題，就是我們前面這個用 AI 文字生圖的部分都是用這個 Diffusion Model。那事實上有一度 Diffusion Model 也被認為應該就是文字生圖的最厲害的一個模型了，那就是完全全的打趴我們再更之前的 GAN (Generative Adversarial Network)。然後可是呢，現在又有一個新的挑戰者，就是我們的老朋友大型模型 (Large Language Model)。

[00:52:33] 我相信大家都已經發現，就是我們現在的這個像是 ChatGPT 等的大型模型，幾乎不能說幾乎，各個也都會畫圖。其實不只 ChatGPT，剛剛我們已經示範過，我們是用 Gemini，就是一個語言模型，所以它也會畫圖的。所以我們剛剛示範已經會畫圖了。那有一陣子有吉卜力 (Ghibli) 之亂，就是大家都很想要說，我今天這個一個人的照片呢，我就把它換成吉卜力風格再畫出來。好就這樣子。

00:53:08 - 多模態模型應用：以圖生圖與風格轉換

[00:53:08] 或是說呢，你會發現說這個描述可以更精確一點點。比方說呢，今天呢，我想要做一個... 我就給他看說某一台 Nikon 的相機，它就長這個樣子，你就找到 Nikon 的相片，說哦，我想要把它做成一個小模型的樣子，然後好像有一個人把它放在他的手心上面，就這樣子。然後所以讓人家會覺得說，哦，是不是真的有做出來，說不定真的有人做出來，就是做出一個這個 Nikon F4 的小模型。

[00:53:44] 所以你就可以把它放上去，所以就是再一次這個圖像也可以變成一個 Prompt 的一部分。那你就可以告訴他說我就是要做這個，或是我就是要畫這個人。然後現在的語言模型很多可以畫得很好。然後他可以甚至可以設定好說，我今天我們有一個可愛的機器人，好，這個左邊的這個機器人是我們自己生出來的一個圖像，就是我們自己畫出來的那個機器人的樣子，呆萌型機器人樣子。然後呢，那我們想要把它讓它當主角，變成我們的漫畫中間的角色。

[00:54:28] 那其實在這邊其實是解釋那個怎麼樣子可以是自動的去評價這個 Character 這件事情（註：此處口語較模糊，指利用 LLM 保持角色一致性）。然後我們就想要畫一個這樣的漫畫出來，然後 ChatGPT 就會幫我們把這個漫畫出來。好，而且就是用這個角色，所以非常非常的方便。那你甚至覺得說這個角色可不可以把它做成那個 3D 感一點的角色出來呢，它也會幫我們做出來。就是跟他說我們的吉祥物可不可以把它變成這個呆萌型機器人，可不可以把它 3D 卡通化，然後它就幫我們 3D 卡通化了。就是這樣子。

[00:55:18] 而且呢，而且現在其實它那個不只是 ChatGPT 會生圖，這個在外面你可以找到很多很有趣的範例，就是畫出一些很有趣的東西。比方說呢，這個是人家寫的，就是我今天... 他本來當然不是要產生我們的這一個待部的咖啡店啊，這個咖啡館。然後那它是產生另外一個杯子型的咖啡館。那你就可以去很詳細的去形容這個杯子型的杯子造型的咖啡館到底要有什麼樣子的情景，那這張圖裡面要有什麼，那包括有很多可愛的小人在旁邊散步啊等等的，就是一個迷人的城市的一角這樣子之類之類的，你就很仔細去形容它。

[00:56:11] 那中間呢，我們因為我們希望把我們自己的這個大一步的機器人做成我們的 Logo 放在這個咖啡館上面，所以你就跟他講說這個咖啡館的這個會還會有一個 Logo，就長這樣子的 Logo，然後你就告訴他。反正你就告訴他，很仔細告訴他。那你也會發現說你今天其實這一段也不用自己去打，自己去生，甚至不用到外面去 Copy，你就跟 AI 討論一下說你要怎麼樣去描繪你要畫的東西。

[00:56:43] 那我們就可以看一下下他們就各種不同 AI 畫出來的樣子。好，那最左邊就是 GPT 原版的原來畫出來的樣子，就是 ChatGPT 畫出來樣子。這其實本來這個 Prompt 是抄來的時候，它其實就是從 ChatGPT，它就是用 ChatGPT 做的，所以最接近本來它畫的一樣感覺。那你就會發現這個 Gemini 本來畫出來是完全是不太一樣的風格，不太一樣的風格。所以你可以看一下下，所以如果你真的就是要左邊那種風格的話，你跟 Gemini 你還要再去溝通一下。

[00:57:19] 好，然後那 Grok 呢，就是另外一個那個就是 Tesla 的老闆，馬斯克他們的 AI 公司 xAI 他們的模型，它其實那個模型也做得相當的好，那它繪畫出另外一個不同的風格。同樣的 Prompt 下去的話，那右邊呢是另外一個我們比較少去使用的，其實也是一家新星的 AI 他們畫出來，其實他們的風格有也有自己的一些樣貌。

00:57:52 - 大型語言模型生圖原理：Tokenizer 與 VQVAE

[00:57:52] 所以說現在的語言模型幾乎也都可以畫出圖像來了。然後在這個畫出圖形之前呢，我們會發現語言模型好像已經可以看著開始看得懂文字了，就是我們所謂的多模態。不一定要用文字打進去，那也可以用一張圖進去。可是我們回想看，我們在 Stable Diffusion 的時候，其實就有那麼一點點這樣的概念，因為呢我們會用 CLIP 這個模型呢，把這個文字跟那個照片如果是同樣意涵的話，會把他們的 Embedding 拉得很近。也就是說呢，我可以從那個圖像去了解這個意思，也可以從文字去描述這個意思，如果是同樣的意思的話，它其實用圖像或是用文字都可以。

[00:58:40] 所以就是說圖像跟文字可能沒有我們想像中間的差距的這麼大。那所以說呢，語言模型就想到了，那可不可以想辦法用語言模型，第一個是看懂圖裡面發生什麼事情，第二件事情是我可不可以用語言模型去生出圖像來。好，所以所以呢，這個就是我們現在要解釋它的原理到底是什麼。

[00:59:10] 好，所以呢，這個就是原本的這個大型模型的原理，它就預測下一個字的模型，我們已經很熟悉了。所以就是說呢，我們就把這個文字... 那我們之前沒有一個地方沒有非常清楚的講，我們每一次都說它是預測下一個字的模型，其實更精確的說，更精確的說其實是預測下一個 Token 的模型。在中文的世界裡面，大部分一個字就是一個 Token，就是每一個字都有它代表的一個號碼。但是呢，在英文的世界當然不是這樣子。

[00:59:46] 然後那中文的世界其實也不完全是一個字就是一個 Token。那比方說呢，我們看下面這邊，就「炎龍老師很...」什麼。就是我們要我們洗的這個頭（起頭），我們希望這個我們的大型語言模型要把他往後接，就接字的接龍的機器人，他往後接。那我們在這邊寫出來的，這個是真正的 ChatGPT 的某一個版本，現在我有點忘記是哪一個版本了，他們的編號就是「炎」這個編號其實 9340，「龍」這個編號是 156565。好，我們就會發現這各「老師」它是變成一個 Token。也就是說呢，它有時候會把常用的詞它就會變成一個 Token。就原則上一個字就是一個 Token，但是有些常用的詞，它也會把它變成一個 Token，就這樣子。

[01:00:43] 所以「炎龍老師很...」什麼的時候，他就會生出下一個說他可以放「幽」這個字，很有趣，他所以他有時候「幽」跟「幽默」，我們感覺好像它也可以變成一個 Token，但它沒有，它就是「幽」還是一個 Token，「默」也是一個 Token 這樣子。所以要產生「幽默」這個詞的話就變成這樣子。好，所以呢，它其實是一個預測下一個 Token 的機器，這是最精確的說法。

[01:01:18] 然後另外一件事情呢，每一個語言模型它的 Token 叫做 Tokenizer 的，就是設定 Token 的方式也是不一樣的。那 OpenAI 裡面有一個很有趣的網頁可以看到說不同歷代的 ChatGPT 它的 Token 的給法，它的方法。也就是它同樣的一句話，它是哪一些字它會把它當成不同的 Token，或是哪一些詞它會把它當成一樣的 Token，這些事情你會發現歷代來說是不一樣的。也就是有一些時候那個「老師」可能也是分成兩個 Token，有時候他「老師」是分成一個 Token。所以這個是每一個模型、語言模型它都有自己的 Tokenizer，也就在他自己分 Token 的方法不太一樣。

[01:02:16] 剛才說到的 Tokenizer 是每一個語言模型它都有自己的 Tokenizer，它會把所有的文字呢，它都會變成一堆的編號。好，那並不是一對一的對應。也就是像比方說「我想學習生成式 AI」，那你就會發現它出來的 Token 的數量跟原來的句子的長度是不一致的。那原因就是我們前面講的，那有一些它會把它放在一起。那怎麼放在一起其實就在這裡。就大部分的地方其實都是分開的，你也會覺得有點莫名其妙，那「學習」為什麼不可以把它變成一個 Token？這個是他們在訓練的過程中間，他們發現說這樣子分成這樣的 Token 的時候，他的訓練比較方便，它就變成這個樣子了。所以在所有的語言模型，它會自己想辦法去訓練好根據他的需求去訓練他自己的 Tokenizer。

[01:03:39] 然後呢，在英文的時候呢，更有趣了一點點。英文的時候我們可能也會想到一個字，一個單字啊，英文的單字就是一個 Token，當然也是可以。但是會有一點點複雜，比方說有時候是加 s 跟沒有加 s 的，如果你要把他變成不同的 Token 的話，那個有時候這個真的有一點點複雜。那所以英文的常常切的方法是叫做一個叫做 BPE (Byte Pair Encoding) 的方法。我不會詳細的解釋給大家聽了，但是舉一個例子就是 unbelievable 的話，它會這個它就切成 un 一個 Token, believ 切成一個 Token, able 也切成一個 Token。就這樣子，所以這個 unbelievable 它其實有三個 Token 在裡面。所以英文的切法大概是這個樣子。

01:04:39 - 將圖像視為語言：VQVAE 與 Visual Tokens

[01:04:39] 好了，我們現在要開始介紹，那不管是今天我們希望這個語言模型，就是大型語言模型，它要看得懂圖，或是他要能夠生成圖，最自然最簡單的想法就是不如就是把一個語言變成一種文字。這樣可以嗎？就把它把一個語言變成一個文字啊。所以呢今天呢放入一張圖進去之後，這個其實就好像是說就像我們的 Image Prompt 一樣，就是說這部分我不講，我就是用這張圖代表我要輸入的意思。

[01:05:17] 所以就是這張圖就代表我輸入的 Prompt，然後我後面還可以再去接我一般的文字的 Prompt。這就好像只是說他今天說了像是英文跟中文這樣子，前面是說英文，後面是說中文，你也可以混著兩個語言一起說下去。那我今天也可以做翻譯，比方說我今天想要這張圖到底在說什麼，就把這張圖的意思把它用中文把它描述出來，意思是就好像把圖世界的語言文字把它描述成這個中文出現。那也可以是反過來，我們今天有一堆中文打進去了以後，下這個 Prompt 就說請把它翻成圖，也就是說翻成圖的世界的語言，就是這樣。

[01:06:03] 所以想清楚了以後就發現說這個其實很自然，很自然就是一個語言模型，大型模型，它就可以看懂圖，然後他也可以生成圖。所以可以看懂圖的就很接近他可以生圖了。好，但是呢，大家如果認真的想的話，又會有一個新的疑問。就是因為圖像的變化其實比較多，就一張照片我們這樣切成幾塊好了，比方說他們都有一個標準的格式啊，就是比方說 512 乘 512 的那個照片。你就會發現說為什麼 ChatGPT 它很喜歡生的就是標準型，就是一張方格型的，因為它訓練可能就是這樣訓練的。

[01:07:19] 那我們就會發現呢，這個圖的世界好像比文字更複雜一點點，不，應該是複雜很多啦。因為這每一張圖即使他畫出來的東西是他畫同一個人，他是同一個人的照片好了，那你就會發現這個在圖像的世界是不一樣的，是完全全不一樣的。就是它的那個每一個點的像素啊，那個長什麼樣子，其實是完全不一樣。所以在這樣的情景之下，我們要把它這個圖像做成一個 Token 放進去的時候，感覺好像很複雜。這第一個。

[01:07:56] 第二個呢，更複雜的是我要去做預測的時候，我們記得說語言之所以會預測就是他告訴我們說下一個要給「87」這個字比較高分，或是要「北」比較高分，就這樣子。這語言就是這樣做的。那圖像有這麼多變化，我到底要跟他怎麼講才講得通呢？所以這個就是我們後面想要跟大家說明的事情，它就是用一個很有趣的一個模型叫做 VQVAE。

[01:08:28] 就是我們會先訓練這個圖像的 Tokenizer。就我們剛剛說過文字要有一個 Tokenizer 要把一串的文字告訴它編號。文字的比較容易想像，英文雖然剛剛看起來還複雜一點點，規則比較多一點點，但是也很容易想像，反正符合這個規則，我就可把它切成不同的文字。那圖像怎麼辦呢？圖像呢，我們會訓練一個 VAE，是一個很特別的 VAE。所以呢，也就是說我們回想我們過去的 VAE，就是我今天一張圖，一張小塊圖，我們故意把它切成很小，就是等一下想要跟大家解釋的地方，切成很小塊。然後我們就經過 Encoder 然後進去了以後，然後就 Decode 出來，就是基本上就是我們想要做這件事情。

[01:09:44] 反正就是一個圖進去了以後，然後經過了 VAE 然後 Encoder 之後變出一大一個 Latent Vector 出來。那這個 Latent 會符合某個常態分佈，因為廢話，這是我們前面有說過，這是我們強迫它要符合某個常態分佈的，然後再經過 Decoder 就要還原這張圖。這個是 VAE。那 VQ (Vector Quantized) 就是 VAE 就是要做這件事情，但是它不一樣的事情是什麼呢？就是有趣的地方是它的 Latent 只有有限個選擇。

[01:10:26] 過去的 VAE 就是每一個 Latent Vector 就是不一樣，就是沒有是什麼有限的，它是無限個，因為每一個位置的數值它都可以變有點變化。但是呢，這個 VQVAE 呢，最後出現的它的只會找到 K 個，那 K 是我們自己選的要多少個。也就是說我們這個圖像的世界，這個文字到底有多少個是我們自己定的，比方說 2000 個、2 萬個之類的。就是我們自己定的，就是這個 K 到底是多少，就是要符合這樣的向量。也就是我們圖像世界的語言文字到底有幾個不同的 Token，這個我們自己定就好了。

[01:11:37] 所以說呢，一張圖呢，進去了以後，它就會好像是文字一樣，它就變成一串的數字。就比方說呢，這一張圖我們就是這樣切，當然通常可能切得更細，但是不管我們就這樣切。然後他就會說在圖像的世界裡面，這一張圖叫做 37 逗點 5 逗點 76 逗點 12 逗點 4 逗點 11... 就這樣。所以它是用這個文字，它的文字的世界就是換成一串的數字。跟我們原來的那個文字是一模一樣的，只是現在就是換成圖像它就會換成一串數字。

[01:12:24] 然後根據這個 Decoder 回去了以後，它就會變出那樣子。然後雖然它的文字的限制是有限個的，就是文字是有限，就是每一個圖它每一塊，每一個區塊... 像比方說左上角就不信你有這麼容易在全世界的圖裡面找到跟左上角一模一樣的那一小塊。那當然很難。但是呢，因為它是全部的意思進去了以後，就是這個完整的這一句話，就這一串數字進去的才是這個完整的去描述這個意思。那描述這個意思，它就因為我們訓練了一個 VAE，所以它可以很順利的還原來的圖應該長這個樣子。所以雖然它的是有限個，但是因為它的排列組合，就像我們去用文字，雖然這個文字的數量是有限的，但是可以描述的概念其實是無限多個。所以大概是這樣的概念。

01:13:56 - VQGAN 與總結

[01:13:56] 那其實會有各種技巧，然後你就會發現這個可愛的 GAN (Generative Adversarial Network) 又回來了。這 GAN 呢，就是因為剛剛說到了，它只是有限個方式去描繪這張圖，那所以說呢，他有時候直接用 VAE 訓練，也許沒有辦法訓練好。所以 VQGAN 呢，就是想辦法說我今天訓練出來了以後，我要確定它的品質真的有達到這個水準。也就是說他去產生這張圖，就好像是說有一點點是他看到這串字，這串圖像世界的語言文字，然後要還原這一張這個世界的時候呢，他可以很精確的把它還原回來，那就是用 GAN 去做這件事情。所以這個 G 終於很開心的再度的回歸了。

[01:14:52] 那不管怎麼樣呢，它就是最簡單的想法就是我們的圖像的生成呢，再用大型模型去做圖像的生成，簡單的說只是把那個圖像看成一種語言啊。那變成語言的方式就是我們剛剛變的，就是我們會去做一個 Tokenizer，這個 Tokenizer 把任何的圖它都會把它變成一串的數字。那相反過來呢，就是我們要生圖的時候呢，就是想辦法把那個我們前面那個文字的敘述或是加上圖的一些敘述全部合起來，然後變成一個新的去預測下面的那個敘述，就把它翻譯成我們的圖像語言的世界的語言文字。然後這樣生出來了以後，那我們就可以用 VQGAN 或是 VQVAE，然後想辦法把這個圖再把它生出來。

[01:16:20] 好，那個就是我們要介紹的所有圖像生成的概念。再一次，圖像生成在新世代裡面，在最古老之前，就是 GAN 大家都覺得他是應該是一個會一統江湖的一個最重要的生成 AI 的模型，但是沒有想到在那個圖像部分就被我們的 Diffusion Model 打趴了。Diffusion Model 就發現說生出的真的好很多，然後又可以做更好的控制。直到現在呢，就會發現說大家就想到說，那為什麼大型語言模型我們不能用它來生圖呢？因為他一開始的時候其實是大型模型讓他看得懂裡面的意思，那看得懂裡面的意思，我們就突然發現說，這樣子既然我們都可以把圖看成一種語言文字，那何不就是我們也可以把它生出來的東西就是圖，也就是說讓大型模型會說圖的世界的語言。那他會說的話，那他就可以把它圖出來了。好，所以簡單的想法就是這樣子。

01:17:37 - 期末專案說明

[01:17:37] 好，那照說我們這一周的作業應該要讓大家做一個總結，圖像生成的總結、文字生圖的總結。但是因為我們已經快到期末了，所以我們這一周的作業稍微的特別的一點點。這一周的作業呢，其實就是請做那個你的期末的專案的提案。
【生成式 AI】12. 深入圖像生成 (Part 3)

時間範圍： 01:17:48 - 02:06:20

01:17:48 - 期末專案範例一：MBTI 諮詢師

[01:17:48] 我們這一周的作業呢，其實就是請做你的期末專案的提案。像是我們政大呢，就會用這個提案呢，因為這個截止日期還是兩週了，那我們政大就會用這個提案呢，就是我們到時候會希望選出一些同學去參加真正的期末分享。我再說一次，是所有的同學都要做期末專案，不是你沒有去參加期末分享就可以謝謝再聯絡。我們就既然沒有被選上，就好像投研討會，如果投研討會... 有點像投研討會，但跟投研討會不一樣的是投研討會如果你沒上的話，你可以不用完成那個論文。但是很遺憾的是我們這個作業呢，不管你有沒有被選到期末分享，然後你都需要完成你的期末專案。所以你就開始把你過去已經在思考的期末專案，這一次很清楚的把它呈現出來。

[01:18:56] 那有一些學校會像我們政大一樣，會挑出一些同學去參加期末專案（分享）。那有一些學校，我們知道過去也有一些學校，那老師就覺得這個大家都投了，他覺得這個也應該要鼓勵大家一起參加，所以有一些學校可能會讓所有的同學只要你有投，你就可以參加這個期末的分享都有。所以是看各個學校的老師決定怎麼樣去去。那不管怎麼樣，反正總而言之每一個人都會做期末專案啊，不然不如你就現在先把那個期末專案想得清楚楚的。所以我們的作業其實就是請大家很清楚的把自己的期末專案說出來，用各種的方式讓老師更清楚說你的期末專案到底是想要做什麼樣的東西。

[01:19:49] 那我們舉幾個例子。那希望大家的期末專案是完全不是從這個例子來的，不過也不要... 因為這個例子我自己都覺得有點無聊，但是因為大家可能比較難想像說我可能可以做什麼，所以我們就舉幾個例子給（大家參考）。第一個例子呢，你可能想要設計一個疑難雜症的諮詢師。就是你今天去問 AI 問題，那 AI 就會回答你。你可以根據任何的那個，因為需要稍微的讓 AI 了解你一點點嘛，所以說那可能是要問... 所以像這個我們設計的這個 AI 就是說你要設計你的 MBTI 要把它輸進去，例如說你輸入了 INFP，然後那你自己的問題就是你可能會問他說：AI 說不知道未來可以做什麼樣的事情，覺得很迷茫。那 AI 就是會根據你的 MBTI 的類型，然後去回應你說根據這個問題，你應該要怎麼樣去看這個問題，它會給你一些建議。那右邊的建議都是假的，所以大家可以不要（當真），因為我沒有真的做這個模型，這個只是說大家可以做類似這樣的東西。

[01:21:04] 那會用到什麼樣的我們學習過的技術呢？其實會用到一些我們學習過的技術。就是在做的過程中間，你會發現說這個雖然感覺好像是蠻簡單的 AI 的模型，你也可以直接下 Prompt 之後說這個 MBTI 輸進去的時候，如果是這種形式的 MBTI，你要根據這個使用者，去回答他的問題。你也可以用一個 System Prompt 就把這個對話機器設計好。但是你也可能會發現這個效果好像沒有你想像中間的那麼好，因為這個 AI 搞不好會在那邊胡說八道一陣子，因為他看到這個 MBTI 之後，他不一定會正確的解釋。

[01:21:53] 那所以說要怎麼辦呢？所以呢最好是輸入的... 因為有 MBTI，那你就可以去做一個 RAG (Retrieval-Augmented Generation)，我們之前學習過的就是做 RAG。那你就可以去找一個你覺得你最相信解釋的最好、把 MBTI 解釋的最好像，也有可能有各種情境的應用的解釋，然後你就把它放進去，當成 RAG 的資料放進去。所以每一次他在搜尋的時候，很有趣哦，搜尋的時候，你當然可以把這個 MBTI 相關的特性特質把它搜尋出來。但是你就會想到說這個如果你只是要做這件事情，你何必要做 RAG 呢？MBTI 就 16 種啊，那你每一種就一個檔案啊，你只要說我找我的 MBTI 是比方說剛剛的 INFP，我就把 INFP 那一項全部把它找出來，那一段文字找出來，然後就貼到 Prompt 就好了，結束。

[01:22:46] 所以意思就是說，如果我只是想要把那個裡面的所有的特質輸進去的話，那我可能可以在這一段，我可以不用任何的 AI，我直接做關鍵字搜尋就好了，就把那個相關的找出來。但是呢，也有可能你要給他的資料真的太長了，即使是一個人他也說的非常的長，這第一個。第二個呢，你有可能是想要... 因為這個你輸入的資料可能是這個人，這個某一個很有名的人，或是你覺得解釋 MBTI 解釋的很好的人，他在各方面不管從工作啦、感情啦或什麼，他都解釋的很好。那他的解釋的過程當中，你就想要把它相關的那個部分再把它抽出來就好了，你不用把整本抽出來。那這個時候就真的有可能會做 RAG。所以都有可能，而且可能在這個時候把問題一起進去做個（搜尋）。所以兩個都有可能，就是用關鍵字搜尋或是 RAG 都有可能。

[01:23:56] 那不管怎麼樣搜尋出來了以後，就把這個人的特質或是說應該要去解釋的一些重點的方向把它找出來。那找出來了以後說根據這一個我們搜尋出來的資料，然後去回應那個使用者的問題，然後要什麼樣的口氣，你還要去（設定），然後最後再大型模型回復，就是這樣子。然後你就可以用這個很順暢的做成一個很完整的一個程式出來。那這個當然可以做成期末的專案。

[01:24:26] 那我們剛剛已經說過了，就是即使是同一個問題，你也可以很混水摸魚的隨便下一個 System Prompt 結束。但是你在執行的過程，直接做的過程中間，你就會發現應該會有很多要考量的地方。所以你可以大概就是... 你做的期末專案並不一定是問題本身的看起來表面上看起來簡單或者難。因為你直接去做的話，你就會發現說你越做會越發現說很多地方可能要考量很多。就像我們剛剛說的可能不是一個 System Prompt 就可以解決的。有時候 System Prompt 就可以解決的，有時候不是這麼適合做期末專案啊，除非你那個 System Prompt 真的是落落長，這非常的長，巨細靡遺的考量了所有的偉大的狀況。那樣的 System Prompt 當然也可以做期末專案，因為你真的花了很多的時間你才寫出這種偉大的 System Prompt。好，這大部分的情景可能只下一個簡單的 System Prompt，那個應該不太適合當期末的專案。

01:25:43 - 期末專案範例二：新聞摘要系統

[01:25:43] 我們第二個例子呢，這個例子很有趣。這個例子只是說我今天把那個今天的新聞抓下來，那我可能只專注在某一類型的新聞，比方說 AI 的消息，AI 相關的消息。當然你不一定要 AI，你喜歡什麼都可以，或是你希望還要分類也都可以。好，我們先把它簡化一點點，我們只想抓 AI 的消息，然後就把新聞抓下來。然後抓下來了以後呢，就是要把它每一則消息去做摘要，因為苦命的我們可能這個沒有那麼多時間去看，所以他要去幫我們摘要。

[01:26:28] 那更重要的事情是他可能要先幫我排序，因為我會覺得比較有趣的，他應該放在前面。那我覺得跟我無關的或是我沒有想要看的那種消息，那我可能放在後面一點點。那你剛剛就會發現這裡有很多有趣的事情。第一個你到底要怎麼樣去做排序？然後第二個呢，每一個人其實排序的方式是不一樣，因為每個人看的重點不一樣。搞不好你覺得說這個你要覺得是比較有趣的應用的，你要把它放在前面；那有些人不是，有些人只是說他如果是用 GPT，我就是 GPT 粉絲，所以只要是用 GPT 我就把它放在前面，用 Gemini 放後面一點，之類之類的。不管怎麼樣，這個重點就是他要自動的上網，把今天的新聞抓下來，然後要找出 AI 相關的，然後要根據我們的方式去做排序。

[01:27:23] 那我順便說一下下，這個其實我自己也覺得乍看之下也是蠻無聊的，實際上感覺好像也蠻無聊的。但大家可能想像不到，就是有一些公司有一些非常高階，而且是高階的人物，整天做的事情也沒有... 也不說整天啊，就是進公司的第一件做的事情呢，就是坐下來看報紙，或是現在可能不一定看報紙，可能是看各個重要的媒體的消息。然後呢，就把那個重點的新聞給抓出來，就是今天到底是哪一件事情是比較重要的抓出來，然後摘要給老闆看。這個在很多公司這件事情是重要的消息、重要的事情，就是我要去看各個消息，然後跟我們有關的，然後我要把它摘要出來這樣子。而且是有時候是真的由一個專門的人，甚至是一個很高階的人去做這樣的事，因為高階的人才知道這個件事情到底是重要還是不重要。所以說呢，雖然看起來好像是很簡單的小練習，但是它在很多地方其實真的是需要這樣的應用。

[01:28:31] 好了，那我們就要做這件事情。假設要做這件事情，所以你要描述的時候呢，你在今天的這個作業，你就要描述你到底要做什麼啦。如果可以圖像的去做出來說你就是要成一個像這個 APP 的樣子，然後你最後最終希望它呈現的樣子是什麼。你不一定要真的做得出來，比方說你本來最終希望它就是做成一個 APP，你不一定要真的可以現在就可以做成一個 APP 這樣子。當然你會寫程式，你真的做得出來當然非常好。但你不一定就是真的要做成一個 APP 這樣子，但是你只要概念上描述說你就是要成一個 APP 這樣子。

[01:29:12] 那我們剛剛的事件呢，就是說我今天想要做的就是把那個 AI 相關的新聞把它找出來，並且根據那個有趣的程度排序，然後我們要做用大型模型去做每一篇文章的摘要。那摘要我們當然要限制一下字數，然後需要說的重點我們要設計給它這樣。所以第一件事情呢，我們就要去做網頁爬蟲 (Web Crawler)，把新聞爬下來。那當然很多同學就會可能會有困擾，就會說我不會爬怎麼辦。

[01:29:43] 好消息就是說你今天的專案如果比方說要做這件事情，然後偏偏你不會網路爬蟲，然後但是你知道要去哪裡爬，就是哪裡要找這樣的新聞，你也可以在目前先用人工去做這件事情。總而言之，你的期末專案，只要完全全確定它整個真的可以 Run，真的會用到生成式 AI，然後如果未來有人幫你寫或是你在跟 AI 好好的溝通，他會幫你寫出來這個程式就可以了。所以舉例來說，就是網頁爬蟲，你真的不會爬，你暫時可以先把它自己貼上去，就是這些新聞啊自己貼上去，這就這些新聞。

[01:30:35] 然後第二件事情就開始選出 AI 相關的新聞。那這個選出 AI 相關的新聞，這邊你去的時候就會發現有很多要考量的事情。當然我們開始的時候可能會想到這個很簡單啊，就是請這個語言模型告訴他 System Prompt 告訴他說，這一個一則新聞進來了以後，你要幫我分辨說這是 AI 的新聞還是不是 AI 的新聞。那如果是 AI 的新聞就答是，那不是 AI 的新聞就答不是，就是這樣。好，然後你直接去做的時候可能會發現種種的問題，有一些問題今天助教會帶著大家，就是你就是簡單的分類問題，他有可能分錯了。這第一個。

[01:31:23] 第二個呢，你可能也不是這麼簡單的，就是它就只是 AI 相關的新聞。你可能想要說的事情是說 AI 應用相關的新聞，而且他要一定要講到是某一個應用，它要怎麼樣去使用。那比方說他打造了一個客服的機器人，好，這是 AI 的應用，沒錯。或是他拿這個 AI 去預測股價，好，沒錯，那是 AI 應用的新聞，你才要。那些寫其他風花雪月的新聞，你其實不要，你沒有要理他的，就這樣子。那或是相反過來，它是 AI 的影響的新聞，你才要。那些寫什麼 AI 應用，他們又做了好棒的應用，你其實不太想理他，就這樣的。所以這都有可能，所以你可以讓 AI 去想辦法抓出來你真的想要看的消息新聞。

[01:32:13] 那不管怎麼樣，反正寫 AI 相關的新聞。但也有一種可能有部分跟我們剛剛一樣，就是有一些地方用 AI，有些地方不用 AI。像我一直很抱怨，所有的 AI 塞新聞的（系統），現在其實很多地方他都有幫我們塞啊，就用 AI 去塞。但是其實我真的很想要的一個功能就是出現那個關鍵字的，我其實不想要。比方說在 AI 的世界裡面，你發現那個是蔡炎龍說的，你很討厭，你就是不想看到蔡炎龍說。蔡炎龍說什麼，你都覺得這個根本不重要，好像你就不想看到是有蔡炎龍相關的消息。那你可能就要用關鍵字就把它剔除，就是這樣。但是很遺憾的就現在的幾乎所有的新聞的篩選的都沒有這這麼簡單的功能，都有高級的 AI 的功能，可是沒有這麼簡單的功能。所以說這也是可以考慮的東西。也就是說雖然到目前為止要選出 AI 相關的新聞，老師說看起來好像很簡單，你就下一個 Prompt 告訴他要選 AI 相關的新聞，但是有需要考量的事情可能很多啊，可能沒有沒有想像中（那麼少）。

[01:33:25] 然後總而言之，最後就真的找出來一堆你覺得應該是相關的，或是你的 AI 系統真的覺得這是相關的，你應該會有興趣要看的新聞。但是你在想像你自己是一個大老闆，你沒有那時間去看這麼多的消息，所以你要開始教 AI 怎麼樣去評分，就是要去把它去做一個評分排序。那這個評分就跟所有每一個人評分的標準就不一樣了。那比方說呢，剛剛我們也許是把所有 AI 相關的新聞都抓過來了，那可能你比較重視的是 AI 的應用的新聞，所以如果是要有應用的話，你就會要把他放比較前面。然後他如果是出自某樣的媒體，他如果用的語調是它比較是科普型，就是大家一般人能懂，你要把它放前面一點點；或者說你是相反的，如果他沒有講的很清楚的講出裡面的理論的，你要把它放後面一點點，等等都有可能。

[01:34:21] 所以每個人評分方式是不一樣的，你可以想辦法請大型模型給他幾個評分項目，叫他幫你評分。那他就會幫你評分一個評分出來這樣子。好，那你就可以依照這個評分，你自己規定的評分方式，然後大型模型去幫忙評分，然後打出來的成績，然後你就依那個成績去做排序。那排序完了以後呢，就是你要的新聞，而且是依照重要的順序，你會有興趣的順序排列的。那你再把它這個點進去的時候可以看到裡面的摘要，這樣子大概這樣。

[01:34:54] 那這個整個界面有一些地方我們剛剛說，例如爬蟲、例如最後呈現的方式，你可能想要呈現的方式美輪美奐的，就是一個一個新聞的標題，然後下面可能有一小段的這個摘要的敘述。那點進去就才可以看到一個比較完整的摘要，然後再點進去再來如果要看全文的話也可以去看全文。你可能設計的是這樣子，但是你在做的時候你發現依你現在能力你還沒有辦法做出來，沒有問題。因為因為所有的大型模型需要參與的部分真的可以做出來的地方，你都確實證明他真的做得出來，也就是說你這個系統真的做出來，唯一的是可能要有一個比較會寫程式的協助，你把這個真的完全全實現成你想要的樣子，這樣是可以的。

[01:35:46] 就是目前期末專案有一些地方要假裝是電腦做，事實上是我們做的是可以的。這個是我要再強調的事情。那我們再舉一個例子就更清楚的知道什麼叫做假裝。

01:36:00 - 期末專案範例三：電商文案與貼圖生成

[01:36:00] 就是呢，我們就是比方說這個其實很多的廣告公司，特別是那個電商或是個人的特別是個人的電商，他們其實很想做這件事情。就是今天有一個產品，他就想要很快速的能夠產生一些可以幫他宣傳的文案配上（圖片）。所以他的目標可能有兩個：他第一個要產生一個很適合他的文案，那這一家店既然這就同一家電商，那他會當然會希望他的文案是風格是很一致的，所以你就希望訓練出它一個很一致風格的文案；第二個呢，它的圖也可能風格要很一致的，然後但是它有需要呈現的一些東西。

[01:36:54] 那我們這邊舉的例子是說，我今天就是要產生那個 AI 產生圖貼，但是那個使用者可能形容詞跟老師一樣是沒有那麼多的，所以他只是知道說他要生產那個動物的圖貼，他其他話都不會說了。然後就這樣子，然後你就要幫他想辦法把這個 Prompt 去把它擴充。那擴充完了以後，你可能想到的是用 Stable Diffusion 去做，那意思就是說它可能是用 Fooocus 裡面用到的這個一樣的技術去做這件事情生圖。但是你不太會自動的去呼叫 Fooocus，你不會，那所以說呢，你期末專案可不可以做這件事情？當然可以。

[01:37:43] 就像剛剛的樣子，就是今天一個主題進來，這個主題通常是很簡潔的，就像我剛剛那樣子而已。然後你就要想辦法用英文產生一個... 你可以透過大型模型去產生一個很好的英文的 Prompt，也就是說他把細節說的更仔細一點點。那中間的那些詞彙有一些部分是真的要你自己說的，不然大型模型不會知道你真的要說成什麼樣。比方說剛剛要生成那個很多的小圖貼，那大型模型怎麼知道你要生成小圖貼？所以你一定要跟他講，你一定要跟他講。那其他的風格就是他要什麼向量形、要畫的怎麼樣怎樣怎樣，那可能要你試驗過了很多次的之後，你才會發現說這樣的風格是你最滿意最喜歡的。然後不要忘記，因為剛剛我們說我們想要選擇的是 Stable Diffusion 嘛，所以在這個時候呢，我們就必須要產生英文的（Prompt），所以最後他真的要自己產生英文的。

[01:38:43] 但在理論上呢，那當然剛剛我們也說大型模型也可以生圖啦。所以說呢，你其實也可以用... 你可以不一定要用 Stable Diffusion，也就不一定要用 Fooocus，你也可以用大型模型去產生圖。但是你的重點就是要用（LLM）去產生一串那個很好的 Prompt。那個 Prompt 有可能要需要翻成英文，如果是給 Stable Diffusion；那或是不需要翻，因為要給大型模型。但生出來圖就是你可以直接在做某一個應用的時候拿去使用的，大概是這樣子。

[01:39:18] 然後雖然這一邊我們都有一些技術可以直接程式從頭做到尾，但是我說了，如果我們在我們期末的專案，你可以讓大家知道說你要做這件事情，而且技術你也的確可以做到。唯一做不到的地方是你自己直接比方說用那個 API 的呼叫去呼叫大型語言模型去幫你生出圖出來，反正你可能做不到，雖然不是很難的，因為大家可以勇敢的嘗試一下下，特別是大型模型的話不是很難。那或是你就是要用 Stable Diffusion，但是你不知道怎麼樣去呼叫 Stable Diffusion 讓它可以自動幫你生圖。那可以，你就用先用手動的方式去把那個 Prompt 貼上去，然後讓它去生圖。然後生完的圖，你就假裝它是自動生出來，雖然是你手動生出來，期末狀態我們是接受這樣子的。

[01:40:05] 所以呢，我們在今天的作業就是希望大家可以很清楚的描述你自己的期末的專案。那不一定要所有的部分你程式都寫得出來，但是你需要確定的是這個所有的重點部分，特別是要用生成式 AI 的部分，不管是文字生成還是圖像生成的部分，真的你只要是這樣子下了之後，它就真的做得出來，你要做到這樣的程度。然後所以中間當然你要去描述你這的流程啊，使用的技術啊，或是你 Prompt 是怎麼下的。好，那想得越清楚越好，越讓老師或是助教知道說你的概念大概怎麼樣。

[01:40:45] 好，那這個作業一樣是兩週。那再一次，這個是大家很重要的一次作業，因為你期末就是要往這個方向做了。那很多的老師可能就會依這個來選擇，就是哪一些同學可以覺得這個真的做得很有趣，真的是很好的應用，那可以來參加期末的這個聯合的分享會。好，我們先休息一下，等一下我們就是我們今天的助教時間。

01:41:14 - 助教時間：陳同學閃電秀（人形機器人）

[01:41:14] 好，那各位同學，我們就開始今天的助教課。那我們先歡迎中央大學的陳浩宇同學來跟我們做今天閃電秀的分享。

[01:41:25] （陳同學）大家好，我是陳浩宇，然後今天要分享的是「它與 AI 的關係」。好，首先我們先... 好，OK 這樣有嗎？好。首先我們先介紹什麼是人形機器人。簡單來說就是長得像人類，然後行為上也模仿人類的一個機器人。然後他們這樣做成這樣做目的是因為就是因為我們像社會的建設基本上都是以人類為基礎，然後如果它做成人形的話，基本上就可以用最快的速度，然後也最大程度去融入到我們人類的社會之中。

[01:42:17] 然後在人型機器人中有兩項產品非常受（矚目），就是波士頓動力 (Boston Dynamics) 的跟特斯拉 (Tesla) 的。然而這兩個產品他們在開發的初期時，他們所選擇的機器人大腦的軟體是（不同）方向的。首先是波士頓動力的，它所使用的... 就是類似數學的物理模型，然後它就是透過精確的計算，然後在機器人做出每個動作前，先透過它的一些影像的輸入，然後去做精確的計算，然後再做輸出一些機器人它可能馬達上動力的一些角度跟動力。然後它的特點就是它的動作非常的精確，然後可預測。所以你可能印象中你有看過一些影片，就是機器人有做一些很酷炫的動作，像是跑酷或是後空翻之類的，就是 Atlas 它有這些能力都做到。

[01:43:23] 再來是特斯拉的 Optimus，它所使用的是端到端 (End-to-End) 的神經網路作為它的大腦。然後以這個大腦作為大腦，它有個優勢，就是它是... 它就是先透過影像的輸入，然後跟動作的輸出去學習它這兩個輸入跟輸出之間兩者之間的映射關係。然後到類似的情況，它其實都可以做出很不錯的一個輸出的應對。而不像如果是用數學模型的話，它可能遇到類似的狀況，但是它可能有點些微的差異的話，它可能就會做不出反應來。

[01:44:06] 好，然後我們來說一下就是使用 AI 做（機器人大腦），當然它的分別的優勢跟劣勢在哪裡。首先優勢的部分就是它泛化能力強，然後學習成本低跟一體（硬體？）成本低。就像是剛剛說到就是遇到類似的情況，我們以拿起個杯子為例，假如 Optimus 它拿一個杯子的話，它可能學習拿杯子，它是學習拿杯子的動作。然後它可能遇到不同形狀的杯子或是大小不同、重量不同的杯子，它都可以可能做出還不錯應對的表現。而不是可能像是它是透過就是比較可能可以說就是比較寫死代碼，然後就是去運對的，所以如果遇到一些比較沒有遇到過的問題，它可能就是無法做出應對的動作。

[01:45:00] 然後它第三點像硬體成本比較低的部分就是，像是它在開發初期時它沒有選擇就是使用 AI 的部分，它就是會用比較像是光達或是一些深度的攝像機，然後去當做它的輸入，然後去建造一個 3D 的一個模型。然後再用這個模型搭配它的那個計算，然後才可以做出它的輸出。像 Optimus 的話，它就只需要就是一些單純的一些影像的輸入，然後就可以直接轉化透過計算，然後轉換成它要的輸出。而劣勢的部分就是有它是黑盒子的部分，就是安全性的問題。然後還有非常依賴數據，跟它的動作的行為上精確度力度都比較不如那個物理的數學好。

[01:46:06] 那在這幾年我們觀察那個 Optimus 跟這些機器人的成長，我們可以看到端到端神經網路，它其實是一個非常值得挑戰的一個道路。然後它也在通用性上面算是非常的完整，就是讓就是很有可能可以達到就是最後硬體上的那個通用型人工智慧 (AGI)。甚至在今年他有就是逐漸去加入端到端神經網路在他的大腦之中，然後讓他... 他的學習能力也許就是... 在我看來其實我們離總就是每戶人家都有一台人行機器人的生活其實也不遠了。非常期待就是未來人行機器人的發展。好，以上就是我今天的分享，謝謝。

[01:46:58] （助教）我們謝謝陳同學精彩的分享。那如果對陳同學的這個有額外想跟他討論，也可以到這個直播的聊天室跟他做進一步的討論。這樣陳同學，謝謝。

01:47:17 - 助教時間：LoRA 微調模型原理

[01:47:17] 好，那各位同學好，我是劉子俊助教，然後我負責今天的助教課。那先插播一個訊息，就是這週的作業就是期末... 期末的那個發表成果，期末專案的發想會為期兩週嘛。然後那個作業做完以後，老師以及助教們會挑出從政大同學裡面挑出 12 位來參加最後的期末的展演的發表會。所以兩週後作業截止以後，各位同學就可以開始準備錄製自己專案的這個介紹的影片，然後被選中了，然後我們也會在截止之後公佈那 12 位同學的名單，然後那些同學會參加那個最後期末成果發表的展演這樣子。

[01:48:06] 好，那我們開始今天的助教課。今天助教課想跟大家介紹這個 LoRA (Low-Rank Adaptation) 這個微調模型、Fine-tune 模型的這個方法。那今天一樣我會先從上週作業開始講起，然後講 LoRA 介紹，以及最後會給大家 LoRA 的實作的範例。

[01:48:26] （作業講解略... 01:48:26 - 01:49:57 為上週作業回顧）

[01:49:57] 那我想先跟各位介紹 LoRA 這個微調模型的方法。LoRA 呢，就是全名叫做 Low-Rank Adaptation，就是老師在上週有提到過的。那它最主要的功能就是說... 最主要的特色應該說特色，是說它不重新訓練整個大型模型的全部的參數。像是比較開源的模型像是 Llama 3 70B，B 是 Billion 嘛，就是 10 億，所以這個模型其實總共的參數有 700 億個。那如果你想要直接 Fine-tune 這個模型，那你的硬體的要求就會相對很高。所以 LoRA 這個方法，這個微調的方法也是說不重新訓練全部的參數，讓模型呢也能夠快速的學習新能力或新風格的一種 Fine-tune 一種微調的方法。

[01:50:44] 那核心的概念很簡單，就是老師上週講解這兩張圖，其實相當的淺顯易懂也是相當的重要，其實它核心的概念就是這個。也就是說它凍結原本的參數，只（訓練）一個新的這個 Delta W 的參數。然後把要調整的這個參數原本是一個 m 乘 n 的矩陣變成兩個矩陣，m 乘 k 跟 k 乘 n 嘛。然後當你的 k 取得很小的時候，你這個矩陣的計算量它就變成兩個小矩陣，不好意思，它就變成兩個小矩陣，這樣就可以大幅度的減少訓練模型的這個參數，也就可以大幅度的減少你硬體的需求，而且也可以加快很多你 Fine-tune 模型的這個時間這樣子。

[01:51:33] 好，所以它的核心概念就是說它不動原本的大模型，而只是在裡面插入一些輕量的小模組，然後這樣就可以有效的讓模型學到新的知識或風格。好，這就是 LoRA 這個微調在做的事情。好，那它的應用其實就跟一般的應用非常的像，讓模型特定（學會）畫風啦，然後可以用於這個商品生成擴充訓練資料等。然後在語言模型面的應用的話，它也可以針對各個專業領域做優化微調。

01:52:09 - LoRA 實作：微調 Bloom-560m 模型

[01:52:09] 好，那我們這次實作的範例的話，我們會用 Bloom-560m 這個模型來做。560m million，所以代表說它這個模型很小，只有 5.6 億個參數。然後呢，我們使用了這個資料集，我想說先給大家看一下這個資料集長什麼樣。我們最後希望的效果是說微調過後的模型能夠更聽懂我們給他的人設，就比如別說我們希望他是一位激勵我們的心靈導師，他能夠更好的扮演這個角色，然後來激勵我們，給我們更好更詳細的回覆。

[01:52:41] 那這是它的訓練資料，這是我們這次使用的訓練資料。你看它有很多個角色，像是這個你可以扮演一個 Linux 的這個 Terminal，然後你看看它就會後面寫說這個是個什麼東西，然後會有很多的描述，會有很多的 Prompts 這樣子。然後還有 Travel Guide 就是導遊。好，這是我們所使用的訓練的資料集。好，然後我們會用這個資料集在這個小的 LLM 上做微調，然後看看它前後回覆的差異是多少。然後參考連接的話是參考這個部分這個提供的範例。

[01:53:23] 然後我們先從... 我先給大家看過整個的流程好了。首先我們會載入這個模型，然後呢，在 Fine-tune 之前其實有很多動作可以做。Fine-tune 其實算是最後一步可以嘗試，因為畢竟 Fine-tune 需要的硬體要求較高，而且時間也會較長，較為麻煩。那效果也不一定會比老師上課介紹過的方法還要好。所以各位可以先從想怎麼讓 LLM 回答的更聰明，可以先從比如說 Few-shot example, RAG 等等的很多較為簡單、較為簡易的方法來開始嘗試，說不定就可以達到很好的效果，不一定要用 Fine-tune。

[01:54:02] 所以我們一開始會想先以這個最簡單的模型來做基底，來看它的輸出到底長什麼樣子。接著呢，我會給它幾個範例，當做 Few-shot example 來看說給他幾個範例以後，模型到底有沒有回答的比較好，這樣子來做一個比較。最後面我們如果發現還是沒有比較好的話，我們才會進入到載入這個資料集的部分，載入資料集以後再用 LoRA 進微調這個原本的這個模型，OK。然後最後面再去看到底微調過後的模型有沒有回答的更貼近我們的想像這樣子。

[01:54:44] 那我們來看一下這個程式碼的部分。今天助教課的 PDF 我已經有上傳到 NTU COOL 上面，各位同學可以自行去下載，然後打開這樣子。好，然後一開始就是先安裝需要的套件。然後這一格的話是在引入，就是直接載入這個 560M 5.6 億個參數這個小模型。好，最後面這裡再來這邊就是設定說你最後要哪些的輸出，輸出的樣子要長什麼樣子。

[01:55:16] OK，然後呢，我給各位看一下一個最原始模型，什麼都還沒做的時候，它的推斷長什麼樣子。我放大一點好，這是一開始模型什麼都還沒調整的時候。然後我給他的 Prompt 是說：I want you to act as a motivational coach（我希望你可以當做我一個激勵我的心靈教練）。然後呢後面是接他的回答，那他的回答其實就是有簡單一句話，就是 "don't be afraid to be challenged"（不要害怕被挑戰）這樣子，就其實相當的簡單，而且並不是這麼的清楚。好，這是最一開始最一開始的回答。

[01:56:01] 然後呢，接著我就想試著加入 Few-shot example，就是給他一點點的範例，給他一個問句跟一個 Response 來當做範例，然後看他能不能回答得更好。比如說我給他的一個導遊啦，然後導遊會講這些話等等，他想要去京都怎麼樣怎麼樣怎樣怎樣，好，他推薦他去京都。OK，然後這裡，然後還給他第二個範例，是他是一個專業的寫論文的作者這樣子。好，然後最後面我一樣問他同樣的問題，I want you to act as a motivational coach。

[01:56:42] 然後就我給各位看一下最後的輸出，就是 "to be more effective at teaching students how they can improve their academic performance"。就是可以發現其實稍微的那麼一點文不對題，效果也並不是真的很好，很符合說我們理想中那個它是心靈教練在激勵我們的這個樣子。因此我們就開始進行我們的微調。

[01:57:05] 好，首先我們要載入這個資料集，這邊就在載入資料集。然後因為這是個小小的範例，所以我們就選資料集前 50 筆來當做我們微調的訓練集就夠了，這樣子。好，50 筆以後，然後這是其中一個預處理完後的這個資料。

[01:57:32] 然後呢，我們主要的是這個，主要的是第七個，這個 LoRA 的配置。它有一些比較重要的參數來跟大家做介紹。像第一個 r 等於 4，就是 r 等於 4 也就是說它會拆成兩個模型嘛，B 跟 A 模型。那 r 等於 4 就代表說它可以拆成剛是 M 乘 N 拆成 M 乘 k, k 乘 N 這兩個矩陣，那 r 等於 4 就是 k 等於 4 的那個意思。就是 r 越大，雖然說它的表示能力越強，可以學到越複雜的更新，但是說它的這個參數量也會大量的增加。就有點跟我們一開始想要大幅降低它的參數量有點背道而馳，所以這邊大家通常會設 4、8 或 16 這樣子。

[01:58:21] 好，那 lora_alpha 的話，我把這個公式寫在這邊。W' 就是我們剛剛所說的看到那個 W，應該是新的那個 W，我們會把原本的 W 凍結嘛，然後後面才是這個 Delta W 這樣子。然後它最後的參數會是這樣子設定，會是 Alpha 除以 r 再乘以把 Delta W 拆成的 B 乘 A 這樣子。好，那最後面它乘的這個 L Alpha A 就是在設定這個 Alpha 的部分。好，也就是說它其實是把 Alpha 除以 r。Alpha 除以 r 這邊是... 哦這邊打錯了，不好意思，這邊應該是一才對。代表說它會被縮小放大 / 一倍，再加回到原本的 W 上面。

[01:59:09] 好，然後呢 target_modules 這邊代表說它是套用在 Self-Attention 上面的 query_key_value 這個 module 裡面。OK，然後這裡是代表說它是防止 Overfitting 所以要加一個 lora_dropout 0.05。好，然後這個 task_type 的話，這邊是說它是這邊的任務是像 GPT 還有 Bloom 那樣自回歸的生成任務，是要用語言模型的方式來做微調。它還有其他的任務，我等下可以（補充）。

[01:59:39] 然接著就是這邊的部分，主要是給大家印出來給大家看說它實際上訓練到會訓練到參數是多少。可以看到這邊訓練的這邊總共的參數是差不多是 5 億 5.6 億個左右，差不多 5.6 億個左右。但它其實真正訓練了只有 39 萬個參數，它只要訓練這麼多就好。也就是所有參數只要訓練 0.07% 的參數就好。所以就是大幅的降低了我們微調模型的這個難度以及時間這樣子。

[02:00:12] 好，然後這邊就是設模型儲存、模型輸出以後的這個需要儲存的目錄。好，OK，然後最後就是進入我們訓練。那在訓練的部分，各位同學應該會看到這個警示，然後你們需要輸入一個 WandB (Weights & Biases) API key。你其實只要到這個 WandB... sorry 我用之前開好的 WandB 這邊。這邊這個是目前用下來都是免費的，然後你只要申請你的帳號，在你的頭像這邊點一個 API key，然後複製完以後再貼回到原本的這邊就可以了。嗯好，那因為這個跑一次要大概 15 分鐘的時間，所以就各位同學可以回去試試看。

[02:01:00] 好，跑完了以後呢，就會把模型儲存在這個資料夾裡面。各位在這邊打開執行完以後會看到一個資料夾叫做 bloom-560m-fine-tuned 這樣子。好，然後之後你載入這個模型以後，我們再做一次一樣的問題，就是希望它扮演一個 Motivational 的這個教練。然後這邊是以下是他的回覆： "OK I will provide you information and the topic of which is being discussed you should be able explain how you can help people overcome obstacle in their lives." OK 就是我可以就是幫助別人跨越他們這個生活中的障礙這樣子。OK 像是他這邊有舉很多很詳細的例子嘛，improving relations 應該是增加各之間的友誼啊、關係啊這様，through health habits and lifestyle 點點點點。所以就是它變得更加的詳細，而且更加的懂我們在做什麼，想要的回答是什麼。這樣子是微調以後的效果。

[02:02:11] 嗯，好，不好意思，有點講太快了，已經講完了。那我給各位同學看一個我另外一個微調的這個做的事情。那因為這個 BERT 模型比較舊一點，就不當做這次的範例，但還是給大家看一下那時候在做的任務。我這次這裡的任務是希望調 BERT 的模型，讓它能夠去辨別一個電影評論裡面到底這個評論是正向還是負向，是比較偏向正面，對這個電影評價是正面還是負面這樣子。然後用的是 IMDB 這個電影評論的情緒分類。它裡面它裡面大概有 25000 筆的訓練資料，然後是英文的影評，全部都是英文的影評這樣子。然後會有標註 Label 1 是 Positive, Label 0 是代表說這邊評論是偏負面的叫 Negative 這樣。

[02:02:57] 好，那這邊的話一開始的話我是先把這個訓練集的部分 25000 筆隨機切一筆當做我們的驗證集，OK 五分之一筆就是 5000 筆當做我們模型的驗證集來看我們模型的在訓練的過程中有沒有壞掉或者偏掉這樣子。好，然後這個是其中一個這個 Training 這個 data 的範例，然後這就是它的 Input 嘛。然後 Label 1 就代表說它應該是偏 Positive 就偏正面的影評這樣子。

[02:03:30] 好，接下來我們就載入這個原本的這個 BERT 模型，OK。好，BERT 模型載入以後呢，我們就可以把我們這個 Text 轉成模型可以輸入可以訓練的這個形式。做一點處理以後呢，我們一樣有用 LoRA 來做微調這個 BERT 模型。然後用到的參數跟剛剛其實是一樣的，OK 其實是差不多的。只是這邊剛剛有個 Task Type 是那邊是語言模型的這個生成回答嘛，那這邊的話就是我要它的工作其實是分類，就是分辨說這篇評論到底是正面還是負面的。所以這是一個序列分類 (Sequence Classification) 的任務，也就是說你 Input 一段文字以後，你必須 Output 一個 Positive 或 Negative 的這個標籤。好，所以這邊的任務要改成這個。

[02:04:23] 然後這邊 Alpha 剛剛都有介紹過了。好，然後這邊看到我們總共訓練的參數，像它這邊的參數其實有一億個參數左右，這原本的模型。那其實我們只要訓練 9 萬個，也就是 0.27%，0.27% 的這個模型參數量，其實就可以做好我們微調的工作。

[02:04:46] 好，然後這是來做一個簡單的測試。我們輸入了兩個句子，第一個就是它這個電影真的太糟糕了，我絕對不會看。然後第二個就是說，哦，這個電影還不錯，我很 Enjoy 這個每一分鐘這樣子。然後一開始的模型兩個都覺得這是 Negative 的這個評論，Negative 的影評。那我們來看經過 LoRA 訓練以後會不會變得比較好。那這邊就是在設定那個訓練的參數，訓練參數，OK。好，然後這就是訓練模型的部分。

[02:05:22] 然後這邊我們訓練完以後，一樣會重新跑一次這個測試的這個句子一跟句子二。OK 那它就有成功判別出句子二是（Label 1）表正向的。那我們把這個模型最後套用到這個驗證集以後，我們來看它的正確率。剛剛驗證集隨機切一嘛，所以是 5000 筆裡面它正確率是 0.9 左右，所以是相當高的這個正確率。那我給各位同學看，如果沒有經過 Fine-tune 的模型，它的正確率會是多少？沒有經過 Fine-tune 的模型的畫，直接來做這個預測的話是 0.46 左右。所以經過這個簡單的 Fine-tune，我這個跑的時間也沒有很久，我們就可以大幅的提升這個預測影評的這個準確率。它可以很清楚的辨別說這評論到底是正面還是偏向負面的評論這樣子。
【生成式 AI】12. 深入圖像生成 (Part 4)

時間範圍： 02:06:29 - 02:12:12

02:06:29 - 助教時間 Q&A：LoRA 相關問題

[02:06:29] 好，那各位同學有什麼... 有什麼問題嗎？各位同學，線上同學也可以，有問題嗎？就是關於 LoRA 這個微調的部分。

[02:06:50] 好，那... 那這個連接（程式碼範例連結）的部分就是在這個 PDF 裡面都可以找得到。那好，如果有問題的同學歡迎在禮拜一下午兩點到四點的時候，我會在線上（Office Hour），那也是我的 Office Hour，也可以找我來討論。

[02:07:16] （助教確認線上狀況）好，全部... 好。那我再... 我再重新說明一次這個期末... 這個期末成果發表的這一個部分好了。

02:07:22 - 期末專案時程與發表會說明

[02:07:22] 呃，在今... 在今天開始這個作業，也就是兩週後截止的那個作業的時候，最後你要發想你的期末的題目。然後作業截止以後，助教以及老師們會挑出——從政大同學裡面挑出 12 位左右的同學當做我們這個優良的範例，然後來參加我們最後期末成果的展演。

[02:07:46] 那影片的部分是每一位同學都要做。每一位同學都要去錄製的，對。每位同學都要完成自己的期末專案，然後都要錄。只是挑出的那 12 位同學是我們覺得比較有趣、比較好的作品，然後參加期末的這個發表會這樣子。好，那各位同學還有問題嗎？沒有問題的話，我們今天就助教課... 還有嗎？有人有問題嗎？

[02:08:26] 哦，好，那因為有點（延遲）... 我們稍微等一下看。

02:08:44 - Q&A：LoRA 訓練時間

[02:08:44] 這個 LoRA 的話，它如果沒有用 GPU 加速的話，直接用 CPU 跑的話，大概是 15 分鐘左右。所以各位同學如果回家試的話，可能要稍微等他一下這樣子。好。

02:09:29 - Q&A：期末專案主題與規範

[02:09:29] （回答學生提問）那那個期末的主題應該... 應該沒有特別限定吧。只要跟生成式 AI 有關的都可以。老師剛剛提供的只是範例，然後還是希望同學們可以想出所謂自己的（創意）。

[02:10:06] （回答學生提問）期末專案你說規範嗎？是柏志同學... 陳柏志同學，請問你問的是這個期末專案的這個規範嗎？還是說內容的這個...

[02:10:26] 呃，下週截止這個期末專案，它是只要完成題目的發想就好。就是還不用完全的做完這樣子，只要發想就可以了。對，需要完整的發想，你大概想怎麼做，想要做哪些內容這樣。

[02:11:09] 內容的部分，期末專案內容的部分應該是跟上課內容有關，生成式 AI 有關應該就都可以的... 應用有關應該都可以的，應該都是可以的這樣子。

[02:11:27] 再強調一次，專案不是下週要做完，專案是下週這個截止這個題目的發想而已。對，你還有到期末都還有時間（可以做）。

[02:11:49] （回答學生提問）課程資訊... 我覺得只有下（週截止提案）... 我沒有應對這個... （助教確認